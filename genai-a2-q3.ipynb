{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.11"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11388535,"sourceType":"datasetVersion","datasetId":7131661}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2025-04-13T14:09:07.163856Z","iopub.execute_input":"2025-04-13T14:09:07.164551Z","iopub.status.idle":"2025-04-13T14:09:07.175477Z","shell.execute_reply.started":"2025-04-13T14:09:07.164526Z","shell.execute_reply":"2025-04-13T14:09:07.174876Z"},"scrolled":true,"trusted":true},"outputs":[{"name":"stdout","text":"/kaggle/input/language-learning/Dataset/english-corpus.txt\n/kaggle/input/language-learning/Dataset/urdu-corpus.txt\n","output_type":"stream"}],"execution_count":139},{"cell_type":"code","source":"import collections\nimport logging\nimport os\nimport pathlib\nimport re\nimport string\nimport sys\nimport time\nimport seaborn as sns\nimport pandas as pd\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimport tensorflow_datasets as tfds\nimport tensorflow_text as text\nimport tensorflow as tf","metadata":{"execution":{"iopub.status.busy":"2025-04-13T14:09:07.176961Z","iopub.execute_input":"2025-04-13T14:09:07.177358Z","iopub.status.idle":"2025-04-13T14:09:07.183698Z","shell.execute_reply.started":"2025-04-13T14:09:07.177340Z","shell.execute_reply":"2025-04-13T14:09:07.183023Z"},"trusted":true},"outputs":[],"execution_count":140},{"cell_type":"code","source":"# Adjust this if the folder name is different in your \"Add Data\"\nurdu = []\nwith open('/kaggle/input/language-learning/Dataset/urdu-corpus.txt', 'r', encoding=\"utf8\") as f:\n    urdu = f.readlines()\n\nenglish = []\nwith open('/kaggle/input/language-learning/Dataset/english-corpus.txt', 'r', encoding=\"utf8\") as f:\n    english = f.readlines()\n\nimport pandas as pd\n\n# Strip whitespace/newlines and zip into a DataFrame\ndata = pd.DataFrame(list(zip([line.strip() for line in english],\n                             [line.strip() for line in urdu])), \n                    columns=['english', 'urdu'])\n","metadata":{"execution":{"iopub.status.busy":"2025-04-13T14:09:07.184288Z","iopub.execute_input":"2025-04-13T14:09:07.184537Z","iopub.status.idle":"2025-04-13T14:09:07.231190Z","shell.execute_reply.started":"2025-04-13T14:09:07.184516Z","shell.execute_reply":"2025-04-13T14:09:07.230481Z"},"trusted":true},"outputs":[],"execution_count":141},{"cell_type":"markdown","source":"**PreProcesing**","metadata":{}},{"cell_type":"code","source":"import re\n\ndef expand_contractions(text):\n    contractions = {\n        \"won't\": \"will not\",\n        \"can't\": \"can not\",\n        \"won\\’t\": \"will not\",\n        \"can\\’t\": \"can not\",\n        \"n't\": \" not\",\n        \"'re\": \" are\",\n        \"'s\": \" is\",\n        \"'d\": \" would\",\n        \"'ll\": \" will\",\n        \"'t\": \" not\",\n        \"'ve\": \" have\",\n        \"'m\": \" am\",\n        \"\\’re\": \" are\",\n        \"\\’s\": \" is\",\n        \"\\’d\": \" would\",\n        \"\\’ll\": \" will\",\n        \"\\’t\": \" not\",\n        \"\\’ve\": \" have\",\n        \"\\’m\": \" am\"\n    }\n    for contraction, full_form in contractions.items():\n        text = text.replace(contraction, full_form)\n    return text\n\ndef preprocess_english(text):\n    text = text.lower()\n    text = expand_contractions(text)\n    text = re.sub(r'[$-)\\\"’°;\\'€%:(/]', '', text)\n    text = re.sub(r'\\s+', ' ', text)\n    return text.strip()\n\ndef preprocess_other_language(text):\n    text = text.lower()\n    text = expand_contractions(text)\n    text = re.sub(r'[$)\\\"’°!;\\'€%:(/]', '', text)\n    text = re.sub(r'\\n', '', text)\n    text = re.sub(r'-', ' ', text)\n    text = re.sub(r'\\s+', ' ', text)\n    return text.strip()\n\ndata['english'] = data['english'].apply(preprocess_english)\ndata['urdu'] = data['urdu'].apply(preprocess_other_language)\n\n# Simple data augmentation: swap source and target\naug_data = data.copy()\naug_data.columns = ['urdu', 'english']  # Reverse roles\naug_data = aug_data.sample(frac=0.2, random_state=42)\n\n# Combine with original\ndata = pd.concat([data, aug_data]).reset_index(drop=True)\nprint(\"✅ Dataset size after augmentation:\", len(data))\n\ndata = data[data['urdu'].str.split().apply(len) < 40]\ndata = data[data['english'].str.split().apply(len) < 40]\n\ndata = data.drop(columns=['urdu_len', 'english_len'], errors='ignore')\n\ndata.head()\n","metadata":{"execution":{"iopub.status.busy":"2025-04-13T14:09:07.232043Z","iopub.execute_input":"2025-04-13T14:09:07.232328Z","iopub.status.idle":"2025-04-13T14:09:07.605837Z","shell.execute_reply.started":"2025-04-13T14:09:07.232309Z","shell.execute_reply":"2025-04-13T14:09:07.605225Z"},"trusted":true},"outputs":[{"name":"stdout","text":"✅ Dataset size after augmentation: 29430\n","output_type":"stream"},{"execution_count":142,"output_type":"execute_result","data":{"text/plain":"                english                       urdu\n0   is zain your nephew      زین تمہارا بھتیجا ہے۔\n1  i wish youd trust me  کاش تم مجھ پر بھروسہ کرتے\n2      did he touch you      کیا اس نے آپ کو چھوا؟\n3      its part of life         اس کی زندگی کا حصہ\n4        zain isnt ugly        زین بدصورت نہیں ہے۔","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>english</th>\n      <th>urdu</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>is zain your nephew</td>\n      <td>زین تمہارا بھتیجا ہے۔</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>i wish youd trust me</td>\n      <td>کاش تم مجھ پر بھروسہ کرتے</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>did he touch you</td>\n      <td>کیا اس نے آپ کو چھوا؟</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>its part of life</td>\n      <td>اس کی زندگی کا حصہ</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>zain isnt ugly</td>\n      <td>زین بدصورت نہیں ہے۔</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":142},{"cell_type":"code","source":"\nfrom tensorflow.keras.preprocessing.text import Tokenizer\n\n# Urdu tokenizer using Keras Tokenizer (not BERT)\nurdu_tokenizer = Tokenizer(num_words=8000, oov_token=\"<OOV>\")\nurdu_tokenizer.fit_on_texts(data['urdu'])\ndata['urdu_tokens'] = urdu_tokenizer.texts_to_sequences(data['urdu'])\n","metadata":{"execution":{"iopub.status.busy":"2025-04-13T14:09:07.607409Z","iopub.execute_input":"2025-04-13T14:09:07.607649Z","iopub.status.idle":"2025-04-13T14:09:08.068484Z","shell.execute_reply.started":"2025-04-13T14:09:07.607634Z","shell.execute_reply":"2025-04-13T14:09:08.067744Z"},"trusted":true},"outputs":[],"execution_count":143},{"cell_type":"code","source":"train_examples =  tf.data.Dataset.from_tensor_slices((data['english'].values, data['urdu'].values))","metadata":{"execution":{"iopub.status.busy":"2025-04-13T14:09:08.069254Z","iopub.execute_input":"2025-04-13T14:09:08.069539Z","iopub.status.idle":"2025-04-13T14:09:08.086849Z","shell.execute_reply.started":"2025-04-13T14:09:08.069517Z","shell.execute_reply":"2025-04-13T14:09:08.086267Z"},"trusted":true},"outputs":[],"execution_count":144},{"cell_type":"code","source":"BATCH_SIZE = 32\nMAX_LENGTH = 50\ndef make_batches(ds):\n  return (\n      ds\n      .cache()\n      .batch(BATCH_SIZE)\n      .prefetch(tf.data.AUTOTUNE))\n\n\ntrain_batches = make_batches(train_examples)\n\nfor i in train_examples:\n  print(i)\n  break","metadata":{"execution":{"iopub.status.busy":"2025-04-13T14:09:08.087597Z","iopub.execute_input":"2025-04-13T14:09:08.087837Z","iopub.status.idle":"2025-04-13T14:09:08.113485Z","shell.execute_reply.started":"2025-04-13T14:09:08.087815Z","shell.execute_reply":"2025-04-13T14:09:08.112770Z"},"trusted":true},"outputs":[{"name":"stdout","text":"(<tf.Tensor: shape=(), dtype=string, numpy=b'is zain your nephew'>, <tf.Tensor: shape=(), dtype=string, numpy=b'\\xd8\\xb2\\xdb\\x8c\\xd9\\x86 \\xd8\\xaa\\xd9\\x85\\xdb\\x81\\xd8\\xa7\\xd8\\xb1\\xd8\\xa7 \\xd8\\xa8\\xda\\xbe\\xd8\\xaa\\xdb\\x8c\\xd8\\xac\\xd8\\xa7 \\xdb\\x81\\xdb\\x92\\xdb\\x94'>)\n","output_type":"stream"}],"execution_count":145},{"cell_type":"code","source":"# Learning rate schedule + optimizer\nclass CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n    def __init__(self, d_model, warmup_steps=4000):\n        super().__init__()\n        self.d_model = tf.cast(d_model, tf.float32)\n        self.warmup_steps = warmup_steps\n\n    def __call__(self, step):\n        arg1 = tf.math.rsqrt(step)\n        arg2 = step * (self.warmup_steps ** -1.5)\n        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n\nlearning_rate = CustomSchedule(d_model=512)\n\noptimizer = tf.keras.optimizers.Adam(\n    learning_rate=learning_rate,\n    beta_1=0.9,\n    beta_2=0.98,\n    epsilon=1e-9\n)\n","metadata":{"execution":{"iopub.status.busy":"2025-04-13T14:09:08.114237Z","iopub.execute_input":"2025-04-13T14:09:08.114454Z","iopub.status.idle":"2025-04-13T14:09:08.121486Z","shell.execute_reply.started":"2025-04-13T14:09:08.114439Z","shell.execute_reply":"2025-04-13T14:09:08.120938Z"},"trusted":true},"outputs":[],"execution_count":146},{"cell_type":"code","source":"\ndef get_angles(pos, i, d_model):\n    angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\n    return pos * angle_rates\n\ndef positional_encoding(position, d_model):\n    angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n                            np.arange(d_model)[np.newaxis, :],\n                            d_model)\n    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n\n    pos_encoding = angle_rads[np.newaxis, ...]\n\n    return tf.cast(pos_encoding, dtype=tf.float32)\n\nn, d = MAX_LENGTH, 512\npos_encoding = positional_encoding(n, d)\nprint(pos_encoding.shape)\npos_encoding = pos_encoding[0]\n\npos_encoding = tf.reshape(pos_encoding, (n, d // 2, 2))\npos_encoding = tf.transpose(pos_encoding, (2, 1, 0))\npos_encoding = tf.reshape(pos_encoding, (d, n))\n\nplt.pcolormesh(pos_encoding, cmap='RdBu')\nplt.ylabel('Depth')\nplt.xlabel('Position')\nplt.colorbar()\nplt.show()\n\ndef create_padding_mask(seq):\n    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n    return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)\n\ndef create_look_ahead_mask(size):\n    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n    return mask  # (seq_len, seq_len)\n\ndef scaled_dot_product_attention(q, k, v, mask):\n    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n    \n    # Compute the dot product of the query and key\n    scaled_attention_logits = tf.matmul(q, k, transpose_b=True)  # [batch_size, num_heads, target_seq_len, target_seq_len]\n    \n    # Apply the mask if provided\n    if mask is not None:\n        scaled_attention_logits += (mask * -1e9)  # Apply mask to logits, ensuring the masked positions are set to a very negative value\n\n    # Compute the attention weights using softmax\n    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n    \n    # Compute the output as a weighted sum of the value (v) tensor\n    output = tf.matmul(attention_weights, v)  # [batch_size, num_heads, target_seq_len, d_model]\n    \n    return output, attention_weights\n\n\n\nclass MultiHeadAttention(tf.keras.layers.Layer):\n  def __init__(self, d_model, num_heads):\n    super(MultiHeadAttention, self).__init__()\n    self.num_heads = num_heads\n    self.d_model = d_model\n\n    assert d_model % self.num_heads == 0\n\n    self.depth = d_model // self.num_heads\n\n    self.wq = tf.keras.layers.Dense(d_model)\n    self.wk = tf.keras.layers.Dense(d_model)\n    self.wv = tf.keras.layers.Dense(d_model)\n\n    self.dense = tf.keras.layers.Dense(d_model)\n\n  def split_heads(self, x, batch_size):\n    \"\"\"Split the last dimension into (num_heads, depth).\n    Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n    \"\"\"\n    x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n    return tf.transpose(x, perm=[0, 2, 1, 3])\n\n  def call(self, v, k, q, mask):\n    batch_size = tf.shape(q)[0]\n\n    q = self.wq(q)  # (batch_size, seq_len, d_model)\n    k = self.wk(k)  # (batch_size, seq_len, d_model)\n    v = self.wv(v)  # (batch_size, seq_len, d_model)\n\n    q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n    k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n    v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n\n    # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n    # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n    scaled_attention, attention_weights = scaled_dot_product_attention(\n        q, k, v, mask)\n\n    scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n\n    concat_attention = tf.reshape(scaled_attention,\n                                  (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n\n    output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n\n    return output, attention_weights","metadata":{"execution":{"iopub.status.busy":"2025-04-13T14:09:08.122318Z","iopub.execute_input":"2025-04-13T14:09:08.122619Z","iopub.status.idle":"2025-04-13T14:09:08.318167Z","shell.execute_reply.started":"2025-04-13T14:09:08.122596Z","shell.execute_reply":"2025-04-13T14:09:08.317496Z"},"trusted":true},"outputs":[{"name":"stdout","text":"(1, 50, 512)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 2 Axes>","image/png":"iVBORw0KGgoAAAANSUhEUgAAAjcAAAG2CAYAAACDLKdOAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuNSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/xnp5ZAAAACXBIWXMAAA9hAAAPYQGoP6dpAAC0/ElEQVR4nO2dd3hUdfb/35MOhCQEUiE06TUaJAQsIJFQVFBWwUUpKqwIrhgLsF8EFBEpKuoiuIiUVRbLCgooiqGtEFoAAYEICISShJreM/f3B7+MRnLPCfOZmzuTnNfzzKPMmc+dOzcz977vqRZN0zQIgiAIgiBUE9zM3gFBEARBEARHIuJGEARBEIRqhYgbQRAEQRCqFSJuBEEQBEGoVoi4EQRBEAShWiHiRhAEQRCEaoWIG0EQBEEQqhUibgRBEARBqFaIuBEEQRAEoVoh4kYQBEEQhGqFiBtBEARBEAAA27Ztw/3334/w8HBYLBasWbOGXbNlyxbcdttt8Pb2RosWLbBs2bIbXrNgwQI0bdoUPj4+iI6Oxu7dux2/839AxI0gCIIgCACA3NxcdO7cGQsWLKjU60+dOoUBAwagV69eOHDgACZMmICnnnoK33//ve01n332GeLj4zFt2jTs27cPnTt3RlxcHC5evGjUx4BFBmcKgiAIgvBnLBYLVq9ejUGDBum+ZuLEiVi/fj0OHz5se27o0KHIyMjAhg0bAADR0dG4/fbb8c9//hMAYLVaERERgWeffRaTJk0yZN89DNmqi2G1WnHhwgXUrVsXFovF7N0RBEEQnBRN05CdnY3w8HC4uRkX/CgoKEBRUZFDtqVp2g3XNm9vb3h7eytvOzExEbGxseWei4uLw4QJEwAARUVFSEpKwuTJk212Nzc3xMbGIjExUfn99RBxA+DChQuIiIgwezcEQRAEF+Hs2bNo1KiRIdsuKChArbqBQEm+Q7bn6+uLnJyccs9NmzYN06dPV952WloaQkJCyj0XEhKCrKws5Ofn49q1aygtLa3wNceOHVN+fz1E3ACoW7cuAMC93SOwuHuavDeCIAiCs6KVFqP0yOe264YRFBUVASX58Gj3CKB6TSotRs6Rz3H27Fn4+fnZnnaE18aZEXED2Nx1FndPWNy9TN4bQRAEwdmpihQGi6eP8jVJc3MHAPj5+ZUTN44iNDQU6enp5Z5LT0+Hn58fatWqBXd3d7i7u1f4mtDQUIfvTxlSLSUIgiAITojFzd0hDyOJiYlBQkJCuec2btyImJgYAICXlxeioqLKvcZqtSIhIcH2GiMQz40gCIIgOCEOESfaza3PycnBiRMnbP8+deoUDhw4gMDAQDRu3BiTJ0/G+fPnsWLFCgDA008/jX/+8594+eWX8cQTT2DTpk34/PPPsX79ets24uPjMWLECHTp0gVdu3bF/PnzkZubi1GjRql9NgJTPTfTp0+HxWIp92jTpo3NXlBQgHHjxqF+/frw9fXF4MGDb3BtpaSkYMCAAahduzaCg4Px0ksvoaSkpKo/iiAIgiC4PHv37sWtt96KW2+9FcB1YXLrrbdi6tSpAIDU1FSkpKTYXt+sWTOsX78eGzduROfOnfHWW2/ho48+QlxcnO01Q4YMwbx58zB16lRERkbiwIED2LBhww1Jxo7EdM9N+/bt8eOPP9r+7eHx+y49//zzWL9+Pb744gv4+/tj/PjxeOihh7B9+3YAQGlpKQYMGIDQ0FDs2LEDqampGD58ODw9PfHGG29U+WcRBEEQBEdhsTjAc2O9ufU9e/YE1f6uou7DPXv2xP79+8ntjh8/HuPHj7+pfVHBdHHj4eFRYVJRZmYmlixZgpUrV+Kee+4BACxduhRt27bFzp070a1bN/zwww84cuQIfvzxR4SEhCAyMhIzZszAxIkTMX36dHh5SXKwIAiC4JpY3N1gcVcNS9XM1FrTP/Xx48cRHh6O5s2bY9iwYTZ3V1JSEoqLi8s1B2rTpg0aN25sa/yTmJiIjh07lnNtxcXFISsrC7/88ovuexYWFiIrK6vcQxAEQRCE6oGp4iY6OhrLli3Dhg0bsHDhQpw6dQp33nknsrOzkZaWBi8vLwQEBJRbExISgrS0NAD6zYPKbHrMmjUL/v7+toc08BMEQRCcDTc3d4c8aiKmhqX69etn+/9OnTohOjoaTZo0weeff45atWoZ9r6TJ09GfHy87d9ZWVkicARBEASnwiHVUjVU3JgelvojAQEBaNWqFU6cOIHQ0FAUFRUhIyOj3Gv+2PhHr3lQmU0Pb29vW0MjoxobCYIgCIJgDk4lbnJycnDy5EmEhYUhKioKnp6e5Rr/JCcnIyUlxdb4JyYmBocOHSo3Nn3jxo3w8/NDu3btqnz/BUEQBMFRuEITP2fF1LDUiy++iPvvvx9NmjTBhQsXMG3aNLi7u+PRRx+Fv78/nnzyScTHxyMwMBB+fn549tlnERMTg27dugEA+vTpg3bt2uHxxx/HnDlzkJaWhilTpmDcuHHVfm6GIAiCUL2xuLnBojp53MDJ5c6MqeLm3LlzePTRR3HlyhUEBQXhjjvuwM6dOxEUFAQAeOedd+Dm5obBgwejsLAQcXFx+OCDD2zr3d3dsW7dOowdOxYxMTGoU6cORowYgddee82sjyQIgiAIgslYNKpbTw0hKysL/v7+8Og4TAZnCoIgCLpopUUoOfQpMjMzDcvXLLsm+feaCIuHWhRCKylE5ubZhu6vM2J6Ez9BEARBEG7kelhKtVpKwlKCIAiCIDgJDhm/YKmZCcU1U9IJgiAIglBtEc+NIAiCIDgj7u7Ks6W0mxycWV0QcSMIgiAITogj+tTU1D43EpYSBEEQBKFaIZ4bQRAEQXBCxHNjPyJuBEEQBMEJcchU7xoqbiQsJQiCIAhCtUI8N4IgCILghDiiiZ/ybCoXRcSNIAiCIDghknNjPzVT0gmCIAiCUG0Rz40gCIIgOCHiubEfETeCIAiC4ISIuLEfETeCIAiC4IQ4YnCmRQZnCoIgCIIguD7iuREEQRAEJ8TigMGZqutdFRE3giAIguCESJ8b+6mZn1oQBEEQhGqLeG4EQRAEwQmRain7EXEjCIIgCE6IiBv7kbCUIAiCIAjVCvHcCIIgCIIT4uZmgZubRXEjiutdFBE3giAIguCEWNwssCiKE9X1roqEpQRBEARBqFaI50YQBEEQnBCLxQKLRdFzo7jeVRHPjSAIgiA4IZb/n3Oj8rAnLLVgwQI0bdoUPj4+iI6Oxu7du3Vf27NnT5sI++NjwIABtteMHDnyBnvfvn3tOiaVRTw3giAIguCEWCwOyLm5Sc/NZ599hvj4eCxatAjR0dGYP38+4uLikJycjODg4Bte/9VXX6GoqMj27ytXrqBz5854+OGHy72ub9++WLp0qe3f3t7eN/lJbg7x3AiCIAiCAAB4++23MXr0aIwaNQrt2rXDokWLULt2bXz88ccVvj4wMBChoaG2x8aNG1G7du0bxI23t3e519WrV8/QzyHiRhAEQRCckLJqKdUHAGRlZZV7FBYW3vB+RUVFSEpKQmxsrO05Nzc3xMbGIjExsVL7vGTJEgwdOhR16tQp9/yWLVsQHByM1q1bY+zYsbhy5YrCkeERcSMIgiAIToibxeKQBwBERETA39/f9pg1a9YN73f58mWUlpYiJCSk3PMhISFIS0tj93f37t04fPgwnnrqqXLP9+3bFytWrEBCQgJmz56NrVu3ol+/figtLVU4OjSScyMIgiAI1ZyzZ8/Cz8/P9m8jcl6WLFmCjh07omvXruWeHzp0qO3/O3bsiE6dOuGWW27Bli1b0Lt3b4fvByCeG0EQBEFwShwZlvLz8yv3qEjcNGjQAO7u7khPTy/3fHp6OkJDQ8l9zc3NxapVq/Dkk0+yn6t58+Zo0KABTpw4cRNH4+YQcSMIgiAITogjxU1l8PLyQlRUFBISEmzPWa1WJCQkICYmhlz7xRdfoLCwEI899hj7PufOncOVK1cQFhZW6X27WUTcCIIgCIIAAIiPj8fixYuxfPlyHD16FGPHjkVubi5GjRoFABg+fDgmT558w7olS5Zg0KBBqF+/frnnc3Jy8NJLL2Hnzp04ffo0EhISMHDgQLRo0QJxcXGGfQ7JuREEQRAEJ8QRgzO1m1w/ZMgQXLp0CVOnTkVaWhoiIyOxYcMGW5JxSkoK3NzK+0WSk5Px008/4Ycffrhhe+7u7jh48CCWL1+OjIwMhIeHo0+fPpgxY4ahvW4smqZphm3dRcjKyoK/vz88Og6Dxd3L7N0RBEEQnBSttAglhz5FZmZmuQRdR1J2TeoQ/wXcvWsrbau0MA+H337Y0P11RiQsJQiCIAhCtULCUoIgCILghMjgTPsRcSMIgiAIToibGxyQc+OgnXExRNwIgiAIghNys6XcetuoidRQTScIgiAIQnVFPDeCIAiC4IRYLA7w3EjOjSAIgiAIzsIfB1/ai1ZDxY2EpQRBEARBqFaI50YQBEEQnBEHJBSjhiYUi7gRBEEQBCdEqqXsR8JSgiAIgiBUK8RzIwiCIAhOiCMGZ6qud1VE3AiCIAg1Doubu30LNTvX2YGMX7AfETeCIAiCy2G3OBFqBCJuBEEQBIcj4kMdi9v1h+o2aiIibgRBEISbRsSL8UjOjf2IuBEEQaiGiPhwfaQU3H5E3AiCIDghIk4EwX5E3AiCIJiAiBeBQ6ql7EfEjSAIgh2IOHFuDPv7VGEpuOTc2I+IG0EQaiwiUFwX+dsJFCJuBEFwWeQCZx5y7I3HYnFAQrGEpQRBEJwLuYCahxx783F3s8BdUdxoEpYSBEGoWuQCah41/di7wvgFwX6cpnfhm2++CYvFggkTJtieKygowLhx41C/fn34+vpi8ODBSE9PL7cuJSUFAwYMQO3atREcHIyXXnoJJSUlVbz3glAzsbi5Kz0EfVSPrasf+5r++YHrycDuig9JKDaRPXv24MMPP0SnTp3KPf/8889j/fr1+OKLL+Dv74/x48fjoYcewvbt2wEApaWlGDBgAEJDQ7Fjxw6kpqZi+PDh8PT0xBtvvGHGRxGEaoWrXARckep+bKv756sKHBGWstZQcWO65yYnJwfDhg3D4sWLUa9ePdvzmZmZWLJkCd5++23cc889iIqKwtKlS7Fjxw7s3LkTAPDDDz/gyJEj+OSTTxAZGYl+/fphxowZWLBgAYqKisz6SILgMlSHu1uzqAmeAwpX/3x2/+0szv/ZBCcQN+PGjcOAAQMQGxtb7vmkpCQUFxeXe75NmzZo3LgxEhMTAQCJiYno2LEjQkJCbK+Ji4tDVlYWfvnlF933LCwsRFZWVrmHIFRHXP0CZDTVWZy4eljH1fffEaiGpBzh+XFVTA1LrVq1Cvv27cOePXtusKWlpcHLywsBAQHlng8JCUFaWprtNX8UNmX2Mpses2bNwquvvqq494JgPq5ykjaL6nx8nP2zOfv+cVjcdfZfqzqfgISl7Mc0cXP27Fk899xz2LhxI3x8fKr0vSdPnoz4+Hjbv7OyshAREVGl+yAIgOtfAIzGlY+PK+87YP7+64qLGoSHG+ChXAruoJ1xMUwTN0lJSbh48SJuu+0223OlpaXYtm0b/vnPf+L7779HUVERMjIyynlv0tPTERoaCgAIDQ3F7t27y223rJqq7DUV4e3tDW9vbwd+GkEQ7MHsC6gKzr7vTr9/JosXNzuPj+bkx1W4jmnipnfv3jh06FC550aNGoU2bdpg4sSJiIiIgKenJxISEjB48GAAQHJyMlJSUhATEwMAiImJwcyZM3Hx4kUEBwcDADZu3Ag/Pz+0a9euaj+QIFSAs19gjMbVP78z77/Z++aq4sSVkLCU/ZgmburWrYsOHTqUe65OnTqoX7++7fknn3wS8fHxCAwMhJ+fH5599lnExMSgW7duAIA+ffqgXbt2ePzxxzFnzhykpaVhypQpGDdunHhmhCrD7IuMkTj7Z3Pm/TN736q7+DDt+Fbh+7o5QNyUirhxPt555x24ublh8ODBKCwsRFxcHD744AOb3d3dHevWrcPYsWMRExODOnXqYMSIEXjttddM3GuhumH2RcpInP2zOfP+mb1vRosXsz0jRh9fu7fvxN9J4XecStxs2bKl3L99fHywYMECLFiwQHdNkyZN8O233xq8Z0J1xuyLlArOvu81ef/Ec2Lu9i1uBmXSGrXdCnC3uMFd8f3cLTUzo7hmfmqhRlEd+l04K85+bI3cP4u7O/lQxc3NXenB7r/BfWKM374b+TBy/6oKs/rcLFiwAE2bNoWPjw+io6NvKNz5I8uWLbs+vfwPjz9XQGuahqlTpyIsLAy1atVCbGwsjh8/ftP7dTM4ledGEITyOINAMBOzP7+R3hfxrKjdW5v2+arQc2MGn332GeLj47Fo0SJER0dj/vz5iIuLQ3Jysq1w58/4+fkhOTnZ9m+LpbygmjNnDt577z0sX74czZo1wyuvvIK4uDgcOXLEsFYwIm4El8fsC6AKZu+7s18Ald/ficWJsx97s8WH6et1vjtWWJW2ezM4olrqZte//fbbGD16NEaNGgUAWLRoEdavX4+PP/4YkyZNqnCNxWLRbb+iaRrmz5+PKVOmYODAgQCAFStWICQkBGvWrMHQoUNvav8qi4gbwekx+wKpgtn77uwXUOX3VxQvRnpPzL4489sX8WLP9s0IS6luo7IUFRUhKSkJkydPtj3n5uaG2NhY29ijisjJyUGTJk1gtVpx22234Y033kD79u0BAKdOnUJaWlq5UUr+/v6Ijo5GYmKiiBtBcFbMvMCbLS6MRsQLtV7EiZHv7wzixpH8eYZiRc1sL1++jNLS0grHGh07dqzC7bZu3Roff/wxOnXqhMzMTMybNw/du3fHL7/8gkaNGtlGIVW0TWpMkioibgTTcfaTRXUWL4Zv32RxYmg1lJOLE2cXH2bvH/fd0hU3WqnS+94M7hYL3C2Knpv/v/7PI4amTZuG6dOnK20buN5Mt6yxLgB0794dbdu2xYcffogZM2Yob99eRNwINR4RLwrbd3HxorK+untOzPKMVBZ7xUllcQbPjSOa+Ln9//Vnz56Fn5+f7fmKGt02aNAA7u7utjFGZfxx7BGHp6cnbr31Vpw4cQLA76OQ0tPTERYWVm6bkZGRN/VZbgYRN4LhmO2ZcWXxIuLE6PX2CxSzPSfVXXwYfvzs9dzYvUc3jyNzbvz8/MqJm4rw8vJCVFQUEhISMGjQIACA1WpFQkICxo8fX6n3Ky0txaFDh9C/f38AQLNmzRAaGoqEhASbmMnKysKuXbswduxY+z5UJRBxIwiKmC3eKES8GOddqe7ixei/vel2O4+v2c0ZjSY+Ph4jRoxAly5d0LVrV8yfPx+5ubm26qnhw4ejYcOGmDVrFgDgtddeQ7du3dCiRQtkZGRg7ty5OHPmDJ566ikA1yupJkyYgNdffx0tW7a0lYKHh4fbBJQRiLgRlDH74u7KoR2zkyqru3gxcv9cPSfFaM+K6Z4XxeOra7eWkOsciYebBR5VPFtqyJAhuHTpEqZOnYq0tDRERkZiw4YNtoTglJQUuP3hd3ft2jWMHj0aaWlpqFevHqKiorBjx45yw6tffvll5ObmYsyYMcjIyMAdd9yBDRs2GNbjBgAsmqZphm3dRcjKyoK/vz88Og6Dxd3L7N1xOVxZXBi9/eouXlxZnFTKrnD8jfacmO354DBafLh50OdqN+aibuHsOom81qI8/PbRY8jMzGTDPPZSdk16/vNd8K7tq7StwrwcvPNItKH764yI50ZgMdsz4+woJaWKeFHcvrl39yprnV28mOYZqaRdVby4u3PfTZ31pdW7Q3F1QcSNYDrO7pkxUrw4e1iouosTFQFiurhwcvHh7sF8d5gSZ9PEC2N3q8LLphkdiqsLIm4El/fMmJ73YWCCoYgX88QLt97ZxQsXtjHaM8KJF1b8MNt348QRs33u87m5V2y3WKtyKrgDxI1inxxXRcSNYDguL54UxIvZwxFVq4XU3991xQtnV943TzXx4eyeFVZcMNtnPTOsOCLNCmGpmikWXA0RN4Iyrh5WMjLvxdlzWmqyOAHUvBtme07cmIuz0eLDKM9Ipdcz768altJbX+JWTK5zJI5s4lfTEHFTA3B2z4nZYSUOo70vFCJejBUQKp9f9b1VPSeqYR3li7+HwdvnxJuyuCHN+se3KsNSknNjNyJuhGqP0U231EIXIl7I9QaLF5XQkLsHffo0XFwoixNGPDCeFaPFldH7x72/h856d6tcNl0B+SsJLGZ7VpQTK01MijVbvKjmdZgtTpT3X7EXChX6MTpsonpxd/b9M0p8lFHLi/5ueDPrvXTsRZ5VF5YSz439iLipBjh72InD7P1XFxj6J0ln97yIeLH/Au7snhOjwzpcWIo7tl6M+NATF5W1c+Kllhd9+dMVN1ZPcp0jcXdTFyfMn6HaIuJGML3cl92+omfGSPHCrTe7YsZdVXwYLF647Rudl6IiAFRzTjw8uWPv2uKjtuJ6Vnww+6/+/hWvL/AqJdc5EvHc2I+IG8F0zA8rGRc6Mr6ct3qLF+O9D/Z7T4zedw9PtX3n3t/bcPFC27mwUS1G/PGeGcX317F7aXLZdAXkrySwmB024jC7FF2lXNjsUmgRL/Z7fjjPi+p7q26fu3gb7VlxVnFShg8jvnx09s+jSDoUuwIiblwAsxN6Vbfv7GElI70Xqp4XNqeE23fFXixcRZDxoRU17wbvHWH+PsT+eXgZ6zlRFSeq4oPfvsHigzt+jDjhxJOeeCmjts7xyZU+Ny6BiBvB5T0zhlckKXhfDBduBosXtlGci4sXldAPt23DxYliWMdw8cK9v6Jnx5vLuWG2z4kbvfe3MN8ZwTkQcSOwmO+ZURMvxns39O2qnhfVsJBqaEN9vbFJtarb55JmqQs4Jw58feiqGlXx4etjsPhRFB/cenb7BosXxvEDLx2PR3ZVVktZLMqzoWS2lOCyuL7nxeCEYRPzXkzPaTHZs2K2eKmlKAAogcF5Vny9abvR4qWOs4sXxe17MTlLrJ0J1+ittzL77UjcLBZ2DEZltlETEXHjBJieE2N6kzxzG90ZmfeiWuqtLE64cmTmAsk2iuOSZhXFixcjEFTzVupy3hVifV1m37j35sQLJ46MFh+8XVWcMH97VfFikLgpkbCUSyDiRlDG8IodxdCN0Um77t617F7L5oQwF0jVnBKjt68qTuqy3gs1gcBtnxIgnDhR9Zyoho04e11vY8WHj8HihBN3biWFpN3C2fPzKn7f3GxynSNxB8AchkptoyYi4kZwfs+MwfOPjJ0/ZHTYSK2JnbOLFy5vxUjxwtmN9oz4ciEz5bCQmmeDy2lhxQlXCg8raXcrzCHtKC6g1xfnk3ZLScXr2fd1IG5uFuVqJ6mWEpwWoy/eqhg+YsDJRwhQFUeqYSN1caLo2eEukNVYvABAXcIzxHlW6iqKE1/GK8VX+6h5RjjPCzubiQtpMuLDUlyx5+R3O+N5Yda7FdHiRtOz5+WS6wTnQMSNYHo1E+eZUQ0ruXvph40AOqwEAB5e9AWYEgCcOPDkQgPMxZ0LK3Hb92a2z138A2rTx15VnPjXVhM/nEDgvCNUXo2qZ4UTH3yfF2PFCSs+ChnxwYgHN068cOIjnw4PWZnwUXFuFr0+r+L1RXn0fjkSqZayHxE3NQBOPLDrTfbMGJ+TQ/8MVLwvXKM3NqFXMSFXVbxwCbdGixc/5v057wjrXWH235+wq4sXNfHhw3lmtBLSbimgwyuWIs5zwoR1GHGCQtoDYrVTfFR6PWMvza34+JTk0x4jRyLVUvYj4qYKcPrBlAZ3+OXEhQfnWVH2vHiTdk/OO6LgfeE8L9x7e9diyolZzwotDvxr0X8bdr3B4oQTT5T4AHjPTR1GfFJ5IbUYYcmJF87OiQc3nYuvbX0RLR5Y8ZGvKg4Y8ZGTQdo1ZvvFOfTnK8ri7LR4K8qu2J5dWESucyRuFvWE4hqaciPixhkwOifG6C65HKZ7dpTLpe33viiHjQzOaVHNWTFbvPgz+8flpdRi7faLG0/FhFgL49lwK+LEDS1eOHHB2jnxkn2NtOt5RspgxYmO+CijmBUvnPip+PgVFFXd+AXBfkTcVANUw07s9k3uUKwcVlLsxcKKHzLnhkm6NFq8cOKA9ew4t3jhBAYvXuwXN55W+g6e85y4MeKFEzcooMVFKSMuWHHCiRtm+8WM+ODES2EGI35YccK8fy4tUgqzKg4/5ZTQ4T5HItVS9iPixgUw2nPi5kFfwFRzXswOK6mGfryZC7wX1QuFEQ9c2CewDv3Z6/vSxz6QsVPVQABQj/nsRouTOpz4YBvJMcKWyStxy9EPjaiKF2vWVdJeqigurNn0+qJMWtwUZtB2Tjzw62nPkp64KKOAsRfbKV7KKMqpeH2etZRc50gk58Z+RNxUA8wezshVMyknBKuGlRTLnVW8L2aHjTjx4sfmrNDHxmjxwokTbvvuXOiH8X5QoR8Ll5PCiIvSzCv0ekbclDLbL7zGiRf62PB2ZvuMeGDtmdx62nNWnEvbCwpoD0xOScVhxfwqFDeC/Yi4cQKMb2KnVqrNJfyqdPAFAE9vej2XlOul6HnxqcMIjDr0/tX31feuBLJraXsDYtuAumclkAs7MTlBvoqek9pMxY9bIX0BtTChFTdGvCCHFhiUgLBy4oSxF17NJO0FnHjgxAcjbgoyFD0n1+hScT3PR2W3n19Eiwg98VFZe36pxtgrXl+g0dt1JO4OSChWXe+qiLipARg+WFK1Dw1X7syNCFAeIcD0K1HIK1EtheZyWjg751nhxIuq50RVvLjl0wKA87wgl0lqvXaJthMChfO8sOLiCu35YT0jjLjhxEc+Y+c9L7RnJJ/xnPDixD7x8budXl9gpdcXWSteX1iF4kbCUvYj4qYKqO4dgvmwlVpYiRveqNooj+9ia3/oiBcvarORWPHCbJ9LqK3DhpWMFS+c3ZrN5K0w4sWaRQuUkgz97RdcYTwviuKl4Bqds6MuXmjxwYmbnEL7wjq/29XES4GO+Kjsej3xUoaeNirS6HWCcyDixkGoCBj18QJqYScuZ4ZLCPb0qU3buYRfH6YcmgsrcUm5fnRoJ9jPh7bXpdcHEdtvwHTw5e1MEz3Gc+PHlLFz6z2YcmO3DNozwoqTzMukvfjaRdJeytiLMzJIe95Fev8pgcKKG0XxobqeC+tkFnNhH/oizokXez0jjrIzjhu7KULViRt3NwvcFaudVNe7KiJuXACjPSvsbCU2bMUMb1Qopa6MnSun5sJKyuXS3vp2tpqIsbMJuYo5L+5cC3zFsBAnXjhxwtmLrtCeG168cN4X/c+Xd5k+dqx4yWA8K5m0ZyWLESeZxao5KcZ6TjjxwYkXo9FzGLtX4W5JWMp+RNxUA8web2D08Eejw0rc+joK84n4Drn0Z1OtJvIspUMLqmEhLctY8VJ4kd5+PiNO8i/R4ib/Mi3O8q7oJ92qelbymLCPqjjh7LmKYRsup6WUCd8Y5Vkpg0uk5WYu6U1Fd9MASMGU0yPipgowvhpKrdpJuZpJMWzEVStxYaUwf3r/wwLosFOYPxOWYnrNhBAVUfWYSi4ubMTZ3XLpnBH3fPrijkxaXJRcTiPtpVdSSTsnTjjPCidOci8y4oTzrlymK4YogZLJlBKrihNOfKjmlJgV1imDEx964qKMWsxNlQ+z3pe5cdCz52ulAK2pHYZUS9mPiBsnQDmnxmBxxCYEM0mlqtVMbFhJdbI0413hQkdUxZFq2IhNyGXsyM0gzWzCbYaxYSFV8ZKbTifd5jHihfO+XCvSFzDGixdjc1KMhhMnnJ0TH5x48WfOK/7Mb08vWT/PWnXixuKAsJRFwlKCWRheas0mBNOeCy4h2IdJiuU8N5xnpVE9OmG5cX3aHsr0igljEobrM56pACIh2s+d9l+75dDiwj2HCftcuUDaSy6dJ+1F6bTnJec8vX95abTnSFWc5F6k7TmMOOEEyFWFXiqqYR2zPSe8Z0TNc1KHsXPiIpC7qfGnf7e169PnPd+QOvT6YN8Kn88pKgaWHyHXOgpJKLYfETdVgOmDMblSbTZnRq0Umxv+qJoTw3leuIRhrlya865Q3hlLfga5VrWaqPQKHTYqvpxO2nMZcWK0Z0VVvHDihBM3WQpJtaqeFVWMDusYLU64Nga1G9DipA4jTuqGVSxObOtD69P2hg0qfN47n86Vqg4sWLAAc+fORVpaGjp37oz3338fXbt2rfC1ixcvxooVK3D48GEAQFRUFN54441yrx85ciSWL19ebl1cXBw2bNhg2GcQceMgjCwFN9qzww2OZKuZmLwQLmykWq3EjghgK5Loi0Adxk5VHHHihW8yp5aQm8+FhRh73kW6V4vZ4uVqkVrohwsdUQLFaPFidFiHEy+BzO+aEy++9WiPbJ1gRpyEc+IkgH7/hkH0esbuEdK4wuetuXSo05G4AVB1vNB/xRv57LPPEB8fj0WLFiE6Ohrz589HXFwckpOTERwcfMPrt2zZgkcffRTdu3eHj48PZs+ejT59+uCXX35Bw4YNba/r27cvli5davu3tzfteVNFxE0VwOfEMHkXzFRsTrxwCcHc4Egu7BTAhHVCmYTfhoGMvS59kmzMbL9BbW4EAeN5KqQFinumvvfEeimFXFucfpa0F5yl7dkptGcm5zzt+clOpfvYZF9g7FfohF1VcZJVotaC38jQj9GeEz9GnHDigvWcBNK/K1/G88F5RnwbVez5sK2PuPFC+UdqNwon7R5hzUi7W3DF4qSM0rr0+5fUqXj/S7OyADxNrnUU7hYLW9VVmW3cDG+//TZGjx6NUaNGAQAWLVqE9evX4+OPP8akSZNueP2nn35a7t8fffQR/vvf/yIhIQHDhw+3Pe/t7Y3Q0FA7PoF9iLhxApQ9N4z4Ue0QzHlmVBN+2bARY6/D7F8tJqzmZaX7ibDeF2I+EZewW3yJFidcQm4eEzbKYTwnbELuVfoutTqLF4AWMJw44TwnquKlATf0lPF8cOLEr5E/vf3GIfT2GbtnWFPa3ugW0l7qR18oixn7Nfpnj6sZFb8gJ5tZ6KRkZZX3wnp7e9/gPSkqKkJSUhImT55se87NzQ2xsbFITEys1Pvk5eWhuLgYgYGB5Z7fsmULgoODUa9ePdxzzz14/fXXUb8+HRpUQcRNFaA6tZst5eYShrmEYGa4I+eZ4RJ+IwJpO+eZCWcTfplya9ChD/cMOqkW6adJc9H5k7q23NNnyLXZKXRYKfMUbc86R+e8cJ6XS3n0cENOvFxju9yaK05Uk2b9PPR/m5z4CGL6MwU0oH8XfhF1aXsjP9reNIy2N6Ptng1pccGKD3/a81JYhw4LpeXT363UHFpknD6eQdpPXqaF/fH0in87RXn0b8qROLKJX0RERLnnp02bhunTp5d77vLlyygtLUVISHlhGhISgmPHjlXq/SZOnIjw8HDExsbanuvbty8eeughNGvWDCdPnsQ//vEP9OvXD4mJiXBnIhv2IuLGQVACxegme8ql2kxiIOd54RJ+VYc7svONuHLqbDpvxC0vg7RzIwCKr+iHfvIv0tvOTWVKqZlSZs5+NV9NvHAJt84uXoz0noRy3/tQOqckoAntGfFrEkjbGfHi24wOy3hGtCLtCGlKmov9G5L2TCudK5fOdGA+w3RwPnmVESdptAg5nk7fGGTo9EgqLaDf15G4u11/qG4DAM6ePQs/v98FsRE5L2+++SZWrVqFLVu2wOcPVbhDhw61/X/Hjh3RqVMn3HLLLdiyZQt69+7t8P0ARNw4Bap9aoweb8BVM3EJv37c4EnFaiVuhAAXVmInQzN2quKICyvlXqT3nUvIzcymKzf4sFD1Fi/1mO82510JIr7bfo1oz0pAU1q8+Delwzb+LWjx4NOE8aw0psVLab0I0s55Vi7m0U0Mz2bSwvvkNfq7r+c5KeNoKn3Tcolp4JjJ2HMyKv7tWYvodc6Kn59fOXFTEQ0aNIC7uzvS08uHy9PT09l8mXnz5uHNN9/Ejz/+iE6dOpGvbd68ORo0aIATJ06IuHFllMcbsB2E1RKCQxQ7+IYxYaVGTIfh+rWZ4Y9MWMnjKp20W3r+V9JedJp2t2b8Sif1ZpzQ7zVz7bcMcm3mGfoEfYHxvFwq5DwvamEjo8UJV07MiRPOe1KfqdjhvCcBzerp2uq1osVB3RZNSbtnk7akHaG0eClmxMmZHPq7c46ZCn78DC3qD5+nv7vHOPHBiJdsJt8r5zIzmuMa3SahKIe+8SjOr3j/tNKqy7lxs6jPhrqZaisvLy9ERUUhISEBgwYNAgBYrVYkJCRg/PjxuuvmzJmDmTNn4vvvv0eXLl3Y9zl37hyuXLmCsDDa+6iCiJtKohI6YquhFMUPl/DL2VU9M1ypdR3mAsWGlXLokySV0AvwnhfVcmnK+8J5ZqgOuADfp0W1y62ze1Y48RLE9EKp1yyAtrekvRMBLfUFRJ0WLcm1Xs3bk/aSwCakPcebDkul6SS8lnHiKv3dS75Ei4uDZzNI++lUJqxziX7/7Kv0++ddoRtQFjI9oIpyGY9tkX0l3VUrbtSrpW5WHMXHx2PEiBHo0qULunbtivnz5yM3N9dWPTV8+HA0bNgQs2bNAgDMnj0bU6dOxcqVK9G0aVOkpV0Xlb6+vvD19UVOTg5effVVDB48GKGhoTh58iRefvlltGjRAnFxcUqfjULETRXAhpWYhGAvRjx4M+KESwhW98zQ9vpMqbVPLi0+3C6fJu1Fv/1C2rOOHSftV4/SSb9Xj9PihvLOnGUSdtMLOXFDe1448cLBlTNzLew5cdKQSfYOZnqd1G+p7zkBgHqt6Tu/wDa0gPBp1YG0uzfW964U129Orj2TRwvPU0xOyRGmku7gWfrincyIk6tMpVzWZXr7RosPa4maiOBuCn38aWHrU6/isKG1uABXDtm9WzeFGVPBhwwZgkuXLmHq1KlIS0tDZGQkNmzYYEsyTklJgdsfbtgXLlyIoqIi/OUvfym3nbKEZXd3dxw8eBDLly9HRkYGwsPD0adPH8yYMcPQXjcibhwElTej3qSP88wYO3tJtQlebSah2cLMR+Ia1VEJvQCQm8qMCGBO8lw59WVCoHDiRDWnhYMTL6qeFaPFS/22dN5JYAe614l3Czr2b4mgQ0P5AfpJueeZsE8yk9NxiAnb7DtDi+rzF+jfzTUu7HORrhLkwjqF2fT+lRSoVRVx50WvOnRI0cuX/m7VYfrg+DWoeH1pYR7oM4rrM378eN0w1JYtW8r9+/Tp0+S2atWqhe+//95Be1Z5RNxUAcrVUGzCsFrYyY/rEMy1SefCSgX0HRpbrcSMGOBGCORdpN8/hwkdZWdySbv6AkalvT/Ah4048cL1YuHDQkzCLRMWUhUv9TvReSferW4l7VqjdqQ9pw6d1HuWqOj55SJ98f75HP2948TLRSanJeMS08OIaSBZcI32DBntWfHwofvseNelvzu16tEJrnWDGa9eGJ0Q3kyn1L44PweVK4pWx5HVUjUNETcOgsqb4ZrsedRifuRcHxfFsFMoE3YKrkOLo3rudGjF48o50l504iBp58JKVw6fJu2XjtCenVSmV0wKE1q6TIgbzjPDweW0cL1Wwhlh2pjpDt2gNZ330aBjI9IeFEnnpXi3q3heTRnWhrQ4uepJXwB/y6CF6X6mj9Cu3/RL9Y+nZJBrLzPiJCuN/l3kMUNRjcopKYMTH7Xr031s6gTRnhH/ELqDcYOGtPjo3JT+bnZpQn83Ihlx09iv4vN2VlYWvqyaBsWmhKWqCyJuqgDVPjVcqbbRYSfOM2NhPDPc8McSrtSaCytx5dTpaiMCVL0vFHzOC33sGzDipqEvLazrNQ8g7fXb0RewBpxnRVG8XPGgL1AnrtB5K/tS6e/mjuP0d/O3Mxm6tqtMQm12Op3Lla8oXjQr/b3lzjvedWlxoJdzUoYf02G4figtHm5pGkDau7ekxc9tYXRZcwumUq5eEX1esRyrOLGmNMc1S8FrGqaKm4ULF2LhwoW2mF379u0xdepU9OvXDwBQUFCAF154AatWrUJhYSHi4uLwwQcflOuemJKSgrFjx2Lz5s3w9fXFiBEjMGvWLHh4VO1Ho04kbIdiRllzOTVc2IkdTKnYRM8tm+sjozbckQsrcTkzXEUSJ25UQ0cURue8cOKlQTv6AsaJFx+DxctRJm8l6Tz93djOiJezhHgBgMvn9T03OWmnyLX5TNhHNSeFK0Tw8afFAedZqdeQDvsEMx2SYxhx0pXxrHRg8rUaetO/W8/UA6S94BA9TuDy3ooLFbILq65aymK5/lDdRk3EVHHTqFEjvPnmm2jZsiU0TcPy5csxcOBA7N+/H+3bt8fzzz+P9evX44svvoC/vz/Gjx+Phx56CNu3bwcAlJaWYsCAAQgNDcWOHTuQmpqK4cOHw9PTE2+88YZD91Ulb4Y7CXnXosWJD+N54cJO3HgDbnwBV82knafDRnnJR0j75YP64wsA4OIh+v1TztN30KcVRwxwSb2U9yWIEZYRzLFvwrjOQyPp4X+h0W1Iu19UNGm3tLidtF/2od//CFMOnHiGrrjZlkz/7S+cziDt187T+Vo56bRAKSC8jlzOCXfO4Kp1fJkOwf7hdL5SaJMA0t6NER89mtNzfzqF0OIjFExn8FNJpD33a1p8nN5Fn1dS99EJ02eZ786p3IrPGwWaWqj5ZnCDBW5QDEsprndVTBU3999/f7l/z5w5EwsXLsTOnTvRqFEjLFmyBCtXrsQ999wDAFi6dCnatm2LnTt3olu3bvjhhx9w5MgR/PjjjwgJCUFkZCRmzJiBiRMnYvr06fDyol3yjoTqZaMalqqtGnZiPTP0l98th7475sYTsF16ueGN3IgBRpwYXZFEeV+4sFE41+CQS8jt0JS01+3QmbS7NaPt12rRnp1kVrzQf/vNR+nvThpzAbrKTE3PvcRMXc9khDsR+uFKjWsxYZ264S1Ie/2GtPhofgsdVoplvHIxEQGkvUU9+rtZ62IyaS86uI20pyXuJ+2pe0+T9guH6O/OCaaajWvDoHfeKELViRvBfpwm56a0tBRffPEFcnNzERMTg6SkJBQXF5cbvtWmTRs0btwYiYmJ6NatGxITE9GxY8dyYaq4uDiMHTsWv/zyC269teJKisLCQhQW/p5o+OdpqfZAem6YDsNeXDkt00cmnLFzCcG1izJIe+k5usNv9tGjpP3SwdO0nUn4/ZUZMXChwL6TVBlc3guXtHsLMXi0KdNErlE3usts+J10NZBPF7p1eUE4XQp9+DKds7L1Z/ru9/tDtGfk/G+0uLly5jRpz2GGlhZm07O5ODyZcuI6Qfp/n8DGdMgulMkpuacDHfa5m/GcdAymB2/6Z9Ae0aKkb0h7+nbas3J++2+k/UwyndNyghl8SSXqA/xNCTcUNYTxqt6lM9g0z1qKpbSuchgSlrIf08XNoUOHEBMTg4KCAvj6+mL16tVo164dDhw4AC8vLwQEBJR7fUhIiK0DYlpaWoXTS8tsesyaNQuvvvqqQz8HmXPDdRjmZjsxP0JfxdlMbrl0WKeEzZnJIO1cQu81HfdvGUYm9AJ8uXQD5viGEuMrWM9Le7oRnFebKNJeHNKatJ9iutjuPp9B2jnPChcWMlu8cEmzXOinfhN9e7NWtPiI68iIF6bap5UfU2hwig7bZO9IIO3ntv5M23fSCc+c1+4sMzpE9aaD6159C3NT17wN7Rlr2KPi32Z2YRHwbtUUg18fv6C+jZqI6eKmdevWOHDgADIzM/Hll19ixIgR2Lp1q6HvOXnyZMTHx9v+nZWVdcM4+D+jlHPDVkPR9gDl8QeMuGEGS7LVTFyfmStq1Uqq84+4kyQ3GVolabd+e7pDbp0OkaSdS8g9m0/v+94LGaR9yzFavKSwnhe6l4rR4oXLW6kbRntXgprRpezt2uhvv18Huo/KHY1pr1AjjT621l1bSHva/7aT9pSt9AU45TD9uz6aTQvjS0xYh/tdcpWATZmZeC2ZUvGIHvQ5vVGv20h7rei+FT6flZ0DvLuKXCuYj+nixsvLCy1aXI89R0VFYc+ePXj33XcxZMgQFBUVISMjo5z35o/TSUNDQ7F79+5y2yubZkpNMPX29nZ422eqlw032LIu06cmPIAOa4XVZdqMMwnBRSfpXuJcwi8XVjqVRufUqN7hcb1gOPdzZ2ZwaMQd9AWwaZy+d6Xunf3ItbmN6BPsngt0Rc26X+hy4+1MWOnCcTqhN/MsHXLkclY4uEZsfg3pydbhLemk2h6daQFyX3s6L+X2cP1eL3XO7SPXZn21nrQf+ZHOOTn7E90H52dmfMOFAvp3pSo+OjOFDG1a0Z6pxj3pHkhh9/Yk7W6d6ZDsaY32mn5yihbWX/9Q8W+jmDlfORIJS9mP6eLmz1itVhQWFiIqKgqenp5ISEjA4MGDAQDJyclISUlBTEwMACAmJgYzZ87ExYsXERx8vWpj48aN8PPzQ7t29B2voyE9N0zYg+1Tw9hrMZ4H1YRgdnAkkxDMDX9UnY/Exda5Lrtsr5e2tLip1UY/r6U4lG7vz4WNuITc3cdocZGeQp/As1NPkHajPSv+xOwmAGjYiu6z05MRLwOZvJbIIPrGwuPoJl3bpYQN5NrfNtDNKU8cpH93nOeE83hyHssIpkqTE/1N7qY9I0370W0Cat/5AGm/Wp+u9PsphT6vfX2QruLcf5DOF0s/XvF6azEtKh2JVEvZj6niZvLkyejXrx8aN26M7OxsrFy5Elu2bMH3338Pf39/PPnkk4iPj0dgYCD8/Pzw7LPPIiYmBt26dQMA9OnTB+3atcPjjz+OOXPmIC0tDVOmTMG4ceMc7pnhK570vSdch+FQf9ozw5ZyMx+19BidEHzlEH2BS2dKsU9eU7uD5BIDuTvItoznq3Vnuly5xf209yQwbiBpz26sfxLflkInq6/aS1fzHDxI90pJSz5M2nPSTpN2rpyZy1kJaEIPnmzUlr4A9rudFo4PMuKktTctrK27vyTt5+b+SNpP/qD/2zjIeNU4jyT3vQ/yps85sUwfmBaMx7H5fXQbgDq9HiLtaf605+UrorszAKz8jg5p/vYLPY/o8vEDpJ3r8Mw1QdTrwKyV0AUODsUBnpsaqm3MFTcXL17E8OHDkZqaCn9/f3Tq1Anff/897r33XgDAO++8Azc3NwwePLhcE78y3N3dsW7dOowdOxYxMTGoU6cORowYgddee63KPwudUGys54ab3WRVLNXOZxqpcZ4ZlT4xQCXmHzF3mEFMSWzd9vQFujSc9gKevKYvEH46Secj/XqSvgBwCbl5l+kTOCdeuGohLizUsDV9AeXEy2Amb6WVRwZpL/lpDWk/842+5wUATn5PV/wkEd99rkqP+143ZXLpbmVySlr0o/82jQb2p3egy32keQ99WsB/ttDH7qd9dMjz/BG6lDyTqdLkxktw3+16jDCPaN+0wudLCnKxd+s8cq1gPqaKmyVLlpB2Hx8fLFiwAAsWLNB9TZMmTfDtt986etdugJr6DdAVUVw1FNenhg07FdDVTkVMQnAeMwCQr2ai74C4qBPXpZe7g2XDSkxFkmcruiLpvJUuud19Xj/naNev9LG/eIYWnlxCLtfllmsg6cck3Ia3aUraY6PonBfO89LKnRZ3xdtoz8tvX20m7ccTTpP2fUzeyqVC/e82Fw5t5Ut7FDt1oEN2LQfRHsUGDzxC2i+HRJL2747Tx/7fiXQ+10kmrHP1BJ2TxE0d56pMA5rS4iS8LR3Wuvt22qv4WFTFwjwnOwv3zCCXOgyplrIfp8u5cVWoXjZch+GGTJ+aoNr0xR3JdFiJ7QDMJARz/Sg4zw2X8Ev1iQGADlH03X2boT1Ie52+w0j7cQ/6Av3JLjp09N0O/YtAykE6WTv7Av234TwvXClzWDu6T04Pps/O6G50tVdnX/ruuWTzYtL+y8qNpP3QVjp08TMzsZ1LRueEc/9Q/YTi1vfTnpOmj9xP2rXutDjZnEIL149+Ok3af95Lh3UuHdtL2jnxwQnneoz46NAnlrQ/encz0j6oNS0Ow6/SIdtLX/2LtB+dWfHxySulPXaOxAL1qFIN1TYibioLOx+KsNdiwkpcHxsfN9r1UXKFPglxCcHZzAUiV7GPDJczE8GIv/rMSYxK6AWA/HpNSfs+ptnYdmYEwMXT+sc/9yItjDjxwlUTBTZnLiCd6JDc0NtoYdfZn/HKbf6MtB//jO61YrR44aaidw2nQz/tHonU3/bQR8m1V5vSonv1Ifqm4t9M2Oe3fXSp97XT9MWdC+tQDQwBILQ97fG8M4aeXcUJ58hajEf6+/mkfd+/aa/evkQ6bHY4q+LvXlEVjl8Q7EfEjYNwJwZ1sjk1jLhxK1RrssdVM1Gud4CvZlLNmWGHO3ai29S7Nae9Eyev0RfI/zF5MamnGHFIVBxxYSMPH33PAMBXEzVtRydLD+1CX6C6h9MhN+u2T0n7ic9oz8vBTadJOxcW4r57XMVPTAu6HLj9Y3RSbYOHR+nafqtDJ9Qu30ELt7Vb6blW5w7SpeLZqbTXj/OsNGhFzw1rfhsd1nmiNx3SfIhpklf753Wk/fQnn5P2w6vpNgU7r9LijfM463n1CjUAVTQ7081igZtiRrHqeldFxE0l4aqlqF42XDVUKNOnxuMqfQeXzlQ7cX1oVKs6uLvjTkwzs1aDu5B23/tGkPafS2jPzlub6ZLQxJ/o3IKLR+hmacW5+gnd3N1v49voi+tg5gLyBJPzEnrmJ9J++uWPSfuBLyqejFwGdwHhPCtcSLJ7JO156vhkL9LuM/Bp0r75Ev3dffs7/e/OkcT/kmsv/7qHtHOeE79GdNir86ChpP2JOHr9ox1oYVxr72rSfvwdOql2zQb6vMUJ21KNPu9wVZIPx9JhrfZj6IRqt3ufqvD5rKwszA2nE+UdhQUO6HPjkD1xPUTcVBK2FJzoZcN5briEYWsGlxBMexa4hGBufAHnmeEmX3MjCLhqpVx/2r29+xBdLs1VJGWdo6s2KPEC0FUZgc3pkFmXSDqfiKsmCsukK0ou/PcL0n7sK/rud881NfHCVQRx4iVyXBxp9xjwDGlfe4a+gM7fQIu34zv1e9VwYR/unBHcjg5bdbqT9tpNvJcWL919aY9v1rJppH37v+hO8duYhOQ0ppqMG59wR3P6vNH5qTtIe8Cj40n7zrwA0j7n04r/vsX5tCdccA5E3FQSauo3QA+/bFiP9twE1ab/DEW/0Sfgi8zFnUsI5lz/nGemQ1t6xk7rR+m7a9z5V9K8iml29q+19AX61C7a88L1w9Drd1FGs2j9i9SY++kL1MhIOqdGW/sead/57lrSvmUfnY/Fee24v/2AW+n97xJPJ9W63f930r6cyUv519zdpP23XfT8JW5qOPW3bxv3ILn2KeZv/wQjbPHtP0nz/hETSfscg//293SiPT9RE+hSdPeBz5P2Tw7Tf/uJzO/+t+ErSLu9f/uq7HPj9v8fqtuoiYi4qSTcXZgHMTmay6lxL6AbvWWnM56bK/TdtWpCMOeZadCGFjfc8McTWfRJdmsyLW4up9DiriCTPklyuQkBTTuS9m636l8A+7ek8w7c9zMt+lfRCbl7GeHHXcC4aqE7WtJN/G4d14e0c+Jl6UH6u/3hN0dI+6mddNiNq/jhZk+1uStG1zbxAbr/Ub8GtNfo8vwXSHvS+9tIewLTwoELJ3cLpL/3dzxKf++bP/8iad/vRXuWXllBD+48sJm2X2FKzbnfdZPutPD+S/+Kh9IW5uXgnSrqc2OxWGBRjEuprndV7BY3VqsVJ06cwMWLF2G1lr943nXXXco75myohKW4aigL06cm/1IGaeeqnTjPDNevg61mak/Htq0RdNhp3xla3CUfpxN+uRECXEUSl9vQpC0dOnk4Ul/cNMqhkz7PfE7nbXDVRL/m0H97rodQj4Z+pP3WsfeQdq8HJ5D2ZYznZdEaTrzQoRFutlVg886kvUMvWnjPeqC9ri2qiA5nHp8yh7Rv/YL+7FxOSiBxQwUA97ejbzq6TmRyTh56mbS/tZueffXxGlqcpeyhq5lK8mnxVr8F3Qeo6710ocEb99HitFX6zgqfz8rJxTvkSsEZsEvc7Ny5E3/9619x5swZaH9K+rJYLCgtpatvXBFO3NTy1U+MbMwMmLOk0zNoLh+mL3Bc2Im7g+MS81r0p+9uGzz0OGnfdIUWR+9/R3/+k4n03TkXVuIucN360Um9c4kLHAA0PvKNrm33a3Qvje+ZclQub+H2evR3654n6IqYiBemkvavLtEt/me9SYd9TvyP9jxxs6u4vJSHn6aTal/vSwvXgG10I9GdA/W9K5N/oYUVl490VwO6Um3aGwNIe/2xdM7MkmN0Z/FnP6d7MJ36Jy1uuJuGsFvpPjZ/nzyatP+jF33ThC9nk+btr44l7YufziDteveEVVkKLk387McucfP000+jS5cuWL9+PcLCwmqE24vrUOxF3EXVYvq8lDIdhHPT6ZNUFnMS5RKCGzKzrzjPTBEzHHLHdvoOL+00M/7hGh124tqsh7SkS3aHMp1Km5XQ4un0l1/r2pKS6LAIJ164aqLofrTwbDL2WdK+MYs+dnNXHyDtJ7dvIe2ceAnpQHt5H3qI9qy8ei/9+d2/or0nW6bSFUHfndX3KnLNKR9sS1fx9ZhF3xRcueMJ0j7yGzrnZPPaXfT2mbAOV+nXgZna/faQSNJ+6xV6/w78hf7ufruZrnLkxmNwYbneY7tX+Hx2YRE+nPcJudZRyFRw+7FL3Bw/fhxffvklWrSg+49UJ7hW4MF++t6PBox4yP+VThhOZaoSuOnAXGJg63voZlqBD9Kl2F8czyDtn31HV/Sk/Uy7p7nYeau76byPN4fTF8jYUjo8kPQEPats7Rb9kyz3t7m/ER0W6jWPbhR3+V76AjDwS/rufMcauoMw10slqE030j7qucdI+5v9aOGZ9d5LpH3NU/TgS65UnROP457UD200f20uufbjM/TvbuwKWlykvEon3HJDTdvF3kvav3mL7twdmUqHBHdOmELav/y/VNK+kvEoD2hN56u9vJr2LJ25jfbq/e1Tuo/QhPUV93CShGLXwC5xEx0djRMnTtQocUNN/QYAXx/90EttxnNzjSnl5i6QHFzJZWAb+g6tOKTixLoyfkqi+8hcPUvfYXH9PtiwEpHQCwB3NaEFRPo8usvugV2054ZqgtiZCUlGPqWfsAoAbg9MIO1zvqWP/d6NSaSdEy/csX9gMB3Sm9WXFi/X5tEX8HVvMV1mmbwU7vjf/9ydpL3BxPm6theZoZufr6TFAec5YY/9kLtJ+/z76SZ8mUxC89I5dEjRzGMPABOZ4//Fs3QDSq4Pkd7xtxbTn1twDiotbg4e/D0v4tlnn8ULL7yAtLQ0dOzYEZ6e5S/snTrRvT1cES7nJoDolMpVQ+Wl0QmzquImnOniGtiZPgmeZqqZ9h+nk0a54Y+cZya0NRNWupVuZMdVJB37nBYAem3Yy6AqjmJ6016xkCfpaqLFP9MhuXXf0aEJ7gLKVQvdM7Bi13wZbzKTqbkL6NfzaPHycyZ9IeFCC/e/SifNFj8+nbQ/RFT0JH5Fz27icsG4ap1Xn6LzpYb60dvf+yBdiv1fpns0l6v3eA/6pqjboumkfb0bncs28f9ocXXqJ3pgMufZ6jFiJGlf9njFCcvZ2VnouHYSudZRSLWU/VRa3ERGRsJisZRLIH7iid9jwmW26ppQzF2AGxInWfdrdM7JxUN0XkZ6oVozLC4h2L0XHft/m7lDOraNTiotYprgNb+LPsnPG0Wf5LudpS8yCSPpvIuvz9D7xyVcD52iHxbT/vYmufbepbSw2vvVUtJeWkwndd42mO4h9MnfaM9L0Dd00uaXEfR3Z881WpzEBtMJy++sfo60J9+uPx4BAHov2EHaj8XS4svHXz800nMoLZw++Svtebn8Kt09edUddNjlWabMfzDTgXjmnkWk/UvQ4mP4woqrico4+zjtOeE8U089RYebp02kQ5a/jKMbPP7n5VWk/QOdw18oCcUuQaXFzalT9ByU6g6XUFzXS/9Qalm0ZyPvMh2W4Uq5mzJTw7mE4NQS2n188Fdm/5k7VB9/OrGydXv6JHxHI3r+0tl39RN6AWAXM1uLK4XvcQfdar3+4/rel1eY0Q6HN9NJlVxCbstedCO5tx+jy2EbJtLi6ZvJX5F2Trz0CqIrggYsoS9Ae9vReRNj5tHlxie3rCHt3Oyuvz2tL2Cmd6bDzfseoqud/vMjfU71Z2ayPf9MV9LecB7dxO7BFXTOybZVC0g7d9MS9TCd0/PZ07Sw9vn4/0j7B0M2kHauTQKX8N3rs4pvTLJycvFuzEByrWA+lRY3TZr87l7ftm0bunfvDo8/DYssKSnBjh07yr22usAlFPsS1VKlV5gmfEzSIwdX7cSNN9h1kb74p6dkkHYuZ8a/LZ10ylUrue2ju/ByIwS4iiTuAtxh/F9IewJRcfTFerqMPTOF3neummgSI15uT/8faf9h3EekffMlulKvR33ao/nAR7R3IrHNI6T9b2/R4uXMDvq7wZWSv/Y8nXQ73F0/2fzHOyeTa9f8RufScSG1R/71JGk/EEV7re6bSHs0f9tG3xRwgzXffGMMaR/jS+dzbepB5wz99xgdrufaIMz5tOLZUGUc6TGOtHd4p+LfTmkh/ZtwNDXU8aKMXQnFvXr1QmpqKoKDy99xZ2ZmolevXtUyLOXhReetNCaGY+Yn0tVQydl0aIHzLHDVTtx4g3mL6Bb2XDUTl7fxt8fpO7SHPemk2A1DXyft61LpJohcRVLf7+kRB2+m0gnLbz+vX3HEebW6D6cr0b4bSx+7E6MeIu0vf0GLp3AiER4A3vyQ/u4c70uHBjq+SX93UqbRIwQax9Chn+1fv0XaW26gK5oW96TDbi8V6Id+nniY9vrMP0yHxPoyv7uxc+ip2LXr02Gl+BceJu2T/ka3AfihPx0S/DqGPnYbFH93u5nf3ctv0XPTxs+nf3vRaXR/rX2zKg6LZWVlIXQhudRhSFjKfuwSN2W5NX/mypUrqFOHjqG7KlQHYgCoQ3huuMGWWSW0GOS6zHLVTlxCcNrpDNLOeWYa3ELH5h9sT88fSv+I7ve58xLtWeJGCHAVScfr051O//0hfYGmKo6a3fEAufaDR2nPC1dN9N81dJk9993560v03K+0gXTi5ONvMOIlcR1p547PV/+gOyQHLqO9J/Nfo70X3OTpF/5PvxFd7jhaWEVO+oG0c54To4/NDMVjM2Uq7fXijk+HWVtI+8ktdFiNS8jesoQenNlg2T9I++x6FXvOqjLnRrCfmxI3Dz10/S7RYrFg5MiR8Pb+PdGytLQUBw8eRPfudHWFq+LFVBw1qKV/gU09eJpcy1VDcZ1MQ4fQvURe3UonBJ9N2kLaufEEU0Z1Ie1Nt9Kx+/dm0b1KmJQjPDeDrgo5Npju5Dosnr4D5EpG73pSv9nad3+hhyNuuqsnaX+P6XE0rBtdKdZ1I11R0vO9vaQ9qe8E0t7wdvrYH/huPmmvNY8ODbwXQYu7Vr50svfk9XQH5vfc6fNVhxnLdG3a/+hk8Rcm0165SY/QOSHL7qOF5UcL6fXjnqbDSsPO0Dk3/aZX3OeljNnf0ML1Vnc6n2z/zN6kfV8/en7Tig/o78b5DfRU8bbbVpL23fUqzhkqzs8F4ulkZ0ch1VL2c1Pixt//uhtT0zTUrVsXtWr9Horx8vJCt27dMHo03VLbVXH3oL8gVC+bXCahlbt4B4XSCbWlYbR7/OCXB0h7MZMY2Ph2+u7+3lvok0jya/Td/ek82rM0sAntPg8cPoG0T19FN+m7dIyu+mh0e1/SvmiIftXHycl0NdE3J2jxwlUTdV/5PmkfvIoOS+1fTQs7Ludn9TS6xX7td+gmg+/Op3OSuF4pj216l7Q/k0wnq3/yDh0a8QvXD7l+8iYdsuu4mm7++MoUukUBGzL8kQ7XzsinPZLv/uVV0u5NVIoBwIplM0h7v7107GZmIJ2PVYfxlnOVdEtCBpH2oSPo0Rt6VGkTPwlL2c1NiZulS69XVjRt2hQvvvhitQ1BVYQHU7ngWaIfuuGqoTjqM1O3uWonLiGY6+HDVTMFX6K74G7431nSzuUUdX6Kvrv+MYMWf1xFkhczvmH0ELpvU8Pt+km5cz89TK7lLmBcNdGbv9GfPWEFPduK88otnUpX/AR9ROfMzGMSgm8LoL+7w3YtJ+0DE2iv5w8f0l5DTrhufiNO13Z19GBy7bSv6MGaj3Skf1c9dtCek+6z6RYMh9bR+UadB9GVaNsn0OJoazf9YwMAE5mE4HFD6cGVdd6nm2u2nUR7JVP3v0Ha7xtHJxx/3r3iXMisnFwE3l41U8EF+7Er52batOtu/osXLyI5+foPuHXr1jckGFcnfGrT1VIe1/Qv4GeYhFduum/LIbT7dsZOerAmlxAcHkWfpJYwF/dtvej941rgvziOLmk9NpS+A37s7/QFXLPSF8AvF9P9RFoumkDan5+vnzjKXcBu27GFtHd8mS53PbfnbdL+zBQ672CmZRNpf6MjXen2GxOufetX2jPUazU9+fnJv9BJs1y5ce7XE0j7J53pSrgFH+snTE9dTs9+ChlDe5UGTKW9biET6L/9zrfoZOvjP9KelU+n09VOp87T4qbOd3RO0ZfP0qM9PrtCnxeWM1WcW+rT4mbufrpD9INvHSDt+UMrFub5WXRTVkdigXq1lD3rFyxYgLlz5yItLQ2dO3fG+++/j65d9c/TX3zxBV555RWcPn0aLVu2xOzZs9G//+8ha03TMG3aNCxevBgZGRno0aMHFi5ciJbM3D8V7BI32dnZeOaZZ7Bq1SpbZZS7uzuGDBmCBQsW2MJX1QkPL2ZCR65+0vDVIjoBrR7jFfJuS+e0JH5Pl5pzCcFNmH4PAWfpqo49zB0aN7+n2TO0d+KptXRYiRshwFUkcbOl3l5M56VQn+/uVXQDwd6L6XweLiGX+2xzW9I9it7u+k/S7udBfzef3E6LD068JH5CJ41yny8hjo7pvt2G7gPE8caBj3Vt93xHr02cQ//tlT9bQzoZnWPBL8tIO/v5HqQr5ZQ/X1c6JEr7xYD3jvybtN+znr7pSdQpFddK6epWR+JmscBNMWfmZtd/9tlniI+Px6JFixAdHY358+cjLi4OycnJFTowduzYgUcffRSzZs3Cfffdh5UrV2LQoEHYt28fOnS43oZkzpw5eO+997B8+XI0a9YMr7zyCuLi4nDkyBH4+NDeW3uxaBqTEl8BQ4YMwf79+/H+++8jJuZ6JUpiYiKee+45REZGYtUquvOjs5GVlQV/f394dBwGi3vFF6r+4+i7nP+E6nea/ccg2oXZvyXdJrz1dvoOpMuTtOfCWkL/GHcvpZM6T99De2bWHqHF1czP6O2PymbyOt5lLqCj6X4gX7c4RtqnDJhJ2mMb02I9cp9+3kjkE3QfmeJ8+uK/Yxmds5Lan/a6fbWf7n79xgq6V8p4C+0dWDmXFkdci/sNkXTIcmqfV0j7XSF0WK7bETqfquNTdNirIFP/u/2/pRPItRkP0yG9lTvoz/7GYrpQYJI/Xeq9ZCbtOYp5bDhp39idvmmZ3pP2eHJ9fO4+QQv79mPohN+cdLoJYsLSF0l70Qi6Ed8nOgNxizQrPsJZZGZmws+PLne3l7JrUlp6uvJ7ZGVlITQkpNL7Gx0djdtvvx3//Of137bVakVERASeffZZTJp0Y5L7kCFDkJubi3Xrfr8R69atGyIjI7Fo0SJomobw8HC88MILePHF63+TzMxMhISEYNmyZRg6lA6P2otdnpt169bh+++/xx133GF7Li4uDosXL0bfvnQM21WpzYSOii/pzwDiZrQEtqDFzS+Mezb3En2S5BKCGxfQYa3PT9Kl7FzehGUAHRr5aRR99+4b0pS0L3yEDptt7U4LBCblB/d8Qk8/vnuB/gX02mk652bWfLoixnMW06I/iZ68POllulHaP8PpqeP/eZH2PnQdSidMfx9F9xr5xz20eOnbis43a7KZDqu1GEz3YvENobt3X/hKfzzDqgjao0oNVAWAd8/RpdgdZtOi/GzSMtL+7ef0Z/d7dghpf/4luspy9r/psNyIQlp4/+UO+qbnry/R5435ObTHdlLjSNL+9EP0QOC4wxX/rvNysvFRNJ2r5oxk/Smc5u3tXa7iGQCKioqQlJSEyZN/byPg5uaG2NhYJCZWnOOVmJiI+Pj4cs/FxcVhzZo1AK5PN0hLS0Ns7O/FB/7+/oiOjkZiYqJh4sYuz03jxo2xfv16dOzYsdzzBw8eRP/+/XHuHD1LydlwpEoWBEEQqi836wmx9z38/f2RnpbmEM9NSOiNvcamTZuG6dOnl3vuwoULaNiwIXbs2GGLygDAyy+/jK1bt2LXrhuLM7y8vLB8+XI8+ujvN0offPABXn31VaSnp2PHjh3o0aMHLly4gLCw31tjPPLII7BYLPjsMzpx3F7s8txMmTIF8fHx+Pe//43Q/3/Q0tLS8NJLL+GVV+g7MUEQBEEQKoFmvf5Q3QaAs2fPlhNKf/baVDfsEjcLFy7EiRMn0LhxYzRu3BgAkJKSAm9vb1y6dAkffvih7bX79u1zzJ4KgiAIgmAXfn5+rBeoQYMGcHd3R3p6+TSL9PR0myPjz4SGhpKvL/tvenp6Oc9Neno6IiMjb/ZjVBq7xM2gQYMcvBuCIAiCIPwRi2aFRdFzczPrvby8EBUVhYSEBNt13mq1IiEhAePHV5wDFRMTg4SEBEyYMMH23MaNG21hrWbNmiE0NBQJCQk2MZOVlYVdu3Zh7Nixdn2myqDU50YQBEEQBINwYFiqssTHx2PEiBHo0qULunbtivnz5yM3NxejRl2vrBw+fDgaNmyIWbNmAQCee+453H333XjrrbcwYMAArFq1Cnv37sW//nW9itdisWDChAl4/fXX0bJlS1speHh4uKGOErvEDQBkZGTgyy+/xMmTJ/HSSy8hMDAQ+/btQ0hICBo2pOfdCIIgCILgfAwZMgSXLl3C1KlTkZaWhsjISGzYsAEhISEArqeguLn93vete/fuWLlyJaZMmYJ//OMfaNmyJdasWWPrcQNcT0jOzc3FmDFjkJGRgTvuuAMbNmwwrMcNYGe11MGDBxEbGwt/f3+cPn0aycnJaN68OaZMmYKUlBSsWEGX9jobUi0lCIIgVIaqrJa6eO6MQ6qlghs1MXR/nRGm7W7FxMfHY+TIkTh+/Hg55dW/f39s20bPkhEEQRAEoRKUhaVUHzUQu8TNnj178Le//e2G5xs2bIi0NLojqiAIgiAIgpHYlXPj7e19Q7dDAPj1118RFETPKRIEQRAEs7n5hAy1dfZg0TQHVEtV4Q47EXZ5bh544AG89tprKC4uBnA9GzolJQUTJ07E4MGDHbqDgiAIgvBnNE3t4RJIWMpu7BI3b731FnJychAUFIT8/HzcfffdaNGiBerWrYuZM+khhIIgCEL1R1V8VAtxooqIG7uxKyzl7++PjRs3Yvv27fj555+Rk5OD2267rdxgLEEQBMF1qTECQqiW3LS4sVqtWLZsGb766iucPn0aFovF1oFQ0zRYLMyIZUEQBMF0RLy4ACY08asu3FRYStM0PPDAA3jqqadw/vx5dOzYEe3bt8eZM2cwcuRIPPjgg0btpyAIQo1CwjoCNCtgVXzUUHFzU56bZcuWYdu2bUhISECvXr3K2TZt2oRBgwZhxYoVGD58uEN3UhAEobohAkMQjOOmPDf/+c9/8I9//OMGYQMA99xzDyZNmoRPP/3UYTsnCILgqojnRFClbHCm6qMmclPi5uDBg+jbt6+uvV+/fvj555+Vd0oQBKEqkLCP4NRItZTd3FRY6urVq7bhWRUREhKCa9euKe+UIAiCIxCRIQg1k5sSN6WlpfDw0F/i7u6OkpIS5Z0SBEEARJwINRxHuAFr6I/opsSNpmkYOXIkvL29K7QXFhY6ZKcEQagZ1NDzriBUDikFt5ubEjcjRoxgXyOVUoIglCHiRRAEM7gpcbN06VKj9kMQBCdExIkgmIcMzrQfu8YvCIJQPaih5z1BgNXOL7+96+xCwlJ2I+JGEKoxIl4EV6VKRYSzIuLGbkTcCIILI+d/wVkRcSKYiYgbQXBi5PogOCsiXqoA8dzYjYgbQTARuT4IRiHiw/VxxPiEmjp+QcSNIBiIXF8EoxDxIgj6iLgRBEXkGiPYg4gTgcVqvf5Q3UYNRMSNIDDINUioCBEnguHI+AW7EXEjCIKggwiYmoveX16+Ea6BiBuh2iPXp+qLiI/qi/xlIdVSCoi4EQTBaRHxUn2RvyyPVEvZj4gbweWR65/rIuLFdZG/nODMiLgRnB65/jkvIk6cF/nLVAMkLGU3bma++axZs3D77bejbt26CA4OxqBBg5CcnFzuNQUFBRg3bhzq168PX19fDB48GOnp6eVek5KSggEDBqB27doIDg7GSy+9hJKSkqr8KIJQLbFqGvkQzENjHkI1QNN+Fzh2P2rmt8FUcbN161aMGzcOO3fuxMaNG1FcXIw+ffogNzfX9prnn38ea9euxRdffIGtW7fiwoULeOihh2z20tJSDBgwAEVFRdixYweWL1+OZcuWYerUqWZ8JMEOyqod9R6CcYh4MQ5OfKg+BBqrZtyjytBKAaviQyutwh12Hiya5jxnsEuXLiE4OBhbt27FXXfdhczMTAQFBWHlypX4y1/+AgA4duwY2rZti8TERHTr1g3fffcd7rvvPly4cAEhISEAgEWLFmHixIm4dOkSvLy82PfNysqCv78/0tLT4efnZ+hnFG7Eeb6B1Q8RKMYhR9ZYqlRE3ARZWVloHB6KzMxMw64XZdeka0nfw8+3jtq2cnJRLyrO0P11Rkz13PyZzMxMAEBgYCAAICkpCcXFxYiNjbW9pk2bNmjcuDESExMBAImJiejYsaNN2ABAXFwcsrKy8Msvv1T4PoWFhcjKyir3EIxDPDPGIZ4XYxHPiXE4jXfEidGsVoc8aiJOk1BstVoxYcIE9OjRAx06dAAApKWlwcvLCwEBAeVeGxISgrS0NNtr/ihsyuxltoqYNWsWXn31VQd/AkFwPCJQ1JCjZz8iMJyAstCS6jZqIE7juRk3bhwOHz6MVatWGf5ekydPRmZmpu1x9uxZw9+zOiOeGfsRz4sakpeij8vklTgpmqbpPgTnxynEzfjx47Fu3Tps3rwZjRo1sj0fGhqKoqIiZGRklHt9eno6QkNDba/5c/VU2b/LXvNnvL294efnV+4hCEYg4kUNES/6iDihocRJZR5OgWoysSM8PwRXr17FsGHD4Ofnh4CAADz55JPIyckhX//ss8+idevWqFWrFho3boy///3vtpSUMiwWyw2Pm3V8mCpuNE3D+PHjsXr1amzatAnNmjUrZ4+KioKnpycSEhJszyUnJyMlJQUxMTEAgJiYGBw6dAgXL160vWbjxo3w8/NDu3btquaDVHPEM0Mj4kWfm60Aqk7ixchqneogXlTFh0uIE0W00lKHPIxi2LBh+OWXX7Bx40asW7cO27Ztw5gxY3Rff+HCBVy4cAHz5s3D4cOHsWzZMmzYsAFPPvnkDa9dunQpUlNTbY9Bgwbd1L6ZmnMzbtw4rFy5El9//TXq1q1ry5Hx9/dHrVq14O/vjyeffBLx8fEIDAyEn58fnn32WcTExKBbt24AgD59+qBdu3Z4/PHHMWfOHKSlpWHKlCkYN24cvL29zfx4QjVBRIo+NfnIVAeBoUJ1ERCCfRw9ehQbNmzAnj170KVLFwDA+++/j/79+2PevHkIDw+/YU2HDh3w3//+1/bvW265BTNnzsRjjz2GkpISeHj8LkkCAgJ0oy+VwVTPzcKFC5GZmYmePXsiLCzM9vjss89sr3nnnXdw3333YfDgwbjrrrsQGhqKr776ymZ3d3fHunXr4O7ujpiYGDz22GMYPnw4XnvtNTM+kktS0z0zNTl0VJM9K4B4TqqzZ6RaeM2sVsc8gBsqhAsLC5V2LTExEQEBATZhAwCxsbFwc3PDrl27Kr2dshL1Pwob4Lrzo0GDBujatSs+/vjjm/7Omeq5qczO+vj4YMGCBViwYIHua5o0aYJvv/3WkbsmCNUC578EGUt1ECF6uILAMJLq/Le1YbWq58z8f3ETERFR7ulp06Zh+vTpdm82LS0NwcHB5Z7z8PBAYGCgbqXyn7l8+TJmzJhxQyjrtddewz333IPatWvjhx9+wDPPPIOcnBz8/e9/r/T+OU0puCAYRXX2vlTfT1Y5qvMFrrqLl+r8t3NGzp49W654Ri9tY9KkSZg9eza5raNHjyrvT1ZWFgYMGIB27drdILJeeeUV2//feuutyM3Nxdy5c0XcCOWp5udIES8ujCtf4ER8uDau8PE0ayk0Rc9N2frKVga/8MILGDlyJPma5s2bIzQ0tFwhDwCUlJTg6tWrbK5MdnY2+vbti7p162L16tXw9PQkXx8dHY0ZM2agsLCw0rm0Im4EwURc4QSrgqtfIKuzgHH5v43ZO1AVaL/nzCht4yYICgpCUFAQ+7qYmBhkZGQgKSkJUVFRAIBNmzbBarUiOjpad11WVhbi4uLg7e2Nb775Bj4+Pux7HThwAPXq1bupIiERN4LT48qeGdfd8+u4/AXQhb87Ln/szd4BRfSOf1X+XRzpuXE0bdu2Rd++fTF69GgsWrQIxcXFGD9+PIYOHWqrlDp//jx69+6NFStWoGvXrsjKykKfPn2Ql5eHTz75pNz4o6CgILi7u2Pt2rVIT09Ht27d4OPjg40bN+KNN97Aiy++eFP7J+KmGuDC528Ari1eANc+ibv8BdSFvzsuf+zN3gEGVz++rsCnn36K8ePHo3fv3nBzc8PgwYPx3nvv2ezFxcVITk5GXl4eAGDfvn22SqoWLVqU29apU6fQtGlTeHp6YsGCBXj++eehaRpatGiBt99+G6NHj76pfXOqqeBm4epTwV39L+js4sa5947G1U/wznx6cvlja/YOKGLW8c/OysItEWFVMhX80vdL4Venttq2cvMQFDeqxk0FF8+NYDgiXozD2S+wzixOAOc+fk68a5XCmY8tYP93s0q/01YH5NzIVHBBEJyN6nqBqCqc+fg58a4BcO5jBzj/d08wFxE3LoCz/4bFM2M/cgFRw5mPnxPvGgDnPnaA+d89PX9HVfpBHDEbysjZUs6MiBvB5XHmc7TZFxCzLxAcph8fc9+exOxjw2H2d6tGBFsc2KG4piHiRmBxds+M2Zh5ETL7AsNh9gXamY+O6cdGxAmJ3uFx8p+c8P8RcSM4PWafS2qyeDH9Amzu29fsv72p785TI0SGtdQBnhsJSwkmYfaP1GzPjNnnqBp9ARPxYhqm/+1NfXdnOO9V7Tp70KxWaIphJdX1roqIG0Goxoh4Me+9q7t4cVVxItQMRNwIhmP2Ocjok6CZFzHDP5uxm2ep1n87g7df08WHUX/bqu1zI2EpexFxUwMwO+xkNGafRI1ExIsaIl6Mw9X/dvb+fao0yKM5QNxoIm4EwS6MPse6+kmUwtXFi6v/bczMRjD6ayd/Gxp7d78qzxaSc2M/Im4EwUBEvNC48gVSxIkazipehOqBiJsqwPiToLFvIBdQGiP3X449jStfIOXY0zjrebNKw/zSxM9uRNwILo+IF31U910ukPrIsTV2+6oiwqjdq1KHkCQU242IG8HpL7BGI+LFOJz9Amvo397Jj62zixMO1a27Qp8bwX5E3AhOj9m5ARxG7p2zn0hFvNiPsx87Dmf1rJShLvz1DGrbval9kMGZdiPixgVw9lJup/ceqO6fY3ajQsw+ds5+gXXmvJTq7jlxWfFR2fe38xPau86+N7Oq58xIzo1QXXFuaaSOM3s3zBYvRmO2eHHm0I/6sameYR3b+xst7hQ/oQzOdG1E3AgsZl+AnNnzAqjtn9meFxEnNCq7Z7bnxGzxYbSHw6zvbpXeTElCsd2IuBEEBhEvKu9ffcNmIl6M8YxU+v1N+vxV6bnRrKXQFMWJ6npXRcSNg6jZJ0nX9syY+flFvKi+v9p6ld+e+d87xfc3WZxwOOvxqdKcG8FuRNwITo/5FxERL7rbV9u86XkpKqud9eLrqPev7p+PfX+dDZRWYVxKxi/Yj4gbgcVsz4yzY27Sqdp6o3NeXFm8XH9/hffmxIPpF2+19UaLF7O/O6U6P2y9541As2rQFN9Qq+4nYB1E3LgAZnsujN6+0Z/PzNCJ8XfHzh0WcnZxohROruGeE2cVHw7bvs76kir0hGilVnVxU5VqzIkQcSM4Pc5+42Gk86OmixdnvsA7u+fEfM8QvQHVay67fWWPs07OjZOfj4TriLgRjA9NGLp18wUAuW2jvWaK60W8GOedMNJrBIhnhRMv1SMsJTk39iLipgowuhmXs4edVLdvtHgx8g7c+feduUCqbV75Au/MoR/z913Eh6Hb11lfLGEpl0DETQ3A1T0zHEb/dI0UdyJenFe8cNs323PiqmEd2/ZNF1f2bbdmSgXXQ8SNE2C6OHByz4zxoRduvf0HSFW8mB0WcnZxYmZox2zPQ00XH6ol2fzxrfj5whLx3LgCIm5cALMT2IzOm1DF6Lt3DurUYXSjM6PFiyquLF4A+gJotufE2cWV6u+OEy/2ipPKop9QXHUnPK20FFaZCm4XIm4Ew3H+4Y/Geo7IbSuGjZxfHNB28/fPfu+F2Tkpru5ZUd8/2s4fP/v2r7iGekJcDRE31QD24sxdYJjtm13xY3RYyczQkdlho5osTiq1nrC7uuekuooPR72/HsVV2aFYc0C1lFYzxZiIG0EZ48XDze2PMyHixXXFC7e+untOzBYnHKr7Z2X2T2/7JarxrptAcm7sR8SN4PKY7ZmhrGaLF6OpzuKFe3+zPSsczi5ezBInld2+o9cJVYuIm0qicpJWv3s3N+xktmfGmcXL9fUK722y56UmixPV96/unhNnFx/sd8Og988vqboEXfHc2I+b2TsgCGajMQ+rRj804mGFRj6otRrzvtx7O+b9NfJRagX5YNczD/PfX//Yl1o18sG/N8gH/9nNfX+rlX4ofz7ub2Pi+1fZucmq2boU2/8wbn+vXr2KYcOGwc/PDwEBAXjyySeRk5NDrunZsycsFku5x9NPP13uNSkpKRgwYABq166N4OBgvPTSSygpKbmpfRPPTQ3AfM+Ise/P3aFyP20jvR/qx4a2O7Nnoyren0vuNNJ7Ud09J1wnXrM9M9zxNcpeUFB1nhtrqRVWRc+L6nqKYcOGITU1FRs3bkRxcTFGjRqFMWPGYOXKleS60aNH47XXXrP9u3bt2rb/Ly0txYABAxAaGoodO3YgNTUVw4cPh6enJ954441K75uIm2pA1d1H2IfR4slo1Bq90dtWEU7Xty/iRWX7Khdg1ffmPpvZ4sHs9y9imuWZJW64/aopHD16FBs2bMCePXvQpUsXAMD777+P/v37Y968eQgPD9ddW7t2bYSGhlZo++GHH3DkyBH8+OOPCAkJQWRkJGbMmIGJEydi+vTp8PLyqtT+ibhxAcyuFlL1PqhiZE4M4Ig2+fZv2/QW/iJeSDvV08Rsz4rKvjti+87qWSnDKHFUlX1uHJlzk5WVVe55b29veHt7273dxMREBAQE2IQNAMTGxsLNzQ27du3Cgw8+qLv2008/xSeffILQ0FDcf//9eOWVV2zem8TERHTs2BEhISG218fFxWHs2LH45ZdfcOutt1Zq/0TcOAGGz35S9JwYvX3u4xstXlSrWqjl5osH7gSvun1zxQl3oTFS/BQzi432nLDH3mTx4ayelzK4MQp66wsLbi73QwVHipuIiIhyz0+bNg3Tp0+3e7tpaWkIDg4u95yHhwcCAwORlpamu+6vf/0rmjRpgvDwcBw8eBATJ05EcnIyvvrqK9t2/yhsANj+TW33z4i4qQKM92w49/Z5z4/aG6h6R8zcvtFhI+O379ripUTBu+HsnhOzxYXZ71+iun86x9dVOxSfPXsWfn5+tn/reW0mTZqE2bNnk9s6evSo3fsxZswY2/937NgRYWFh6N27N06ePIlbbrnF7u3+GRE3LoCqdlH2nCh6ZlTDSuwFmtkCt15VvFAnWe692aRN9gKqdgFU9z6QZmVxYnReikrSrOq2zRYHZosPez0nNrvh4q7i9UWFxeQ6R+LIDsV+fn7lxI0eL7zwAkaOHEm+pnnz5ggNDcXFixfLPV9SUoKrV6/q5tNURHR0NADgxIkTuOWWWxAaGordu3eXe016ejoA3NR2Rdw4CBUBou4ZUduA0Z4Zp39/xbwXlUZvqmEp1ZCaiBf7989sz4nZ2zfKM2KzK+8/XdVkd85NNZ8KHhQUhKCgIPZ1MTExyMjIQFJSEqKiogAAmzZtgtVqtQmWynDgwAEAQFhYmG27M2fOxMWLF21hr40bN8LPzw/t2rWr9HZF3NQA1C/+iu+vuJ6vGGLeX9GuIlCMDhtxn101NGK2eFHNKzEyL0ZVXJi93mjPShH33TNZvNgrzoqLXDMs5Wjatm2Lvn37YvTo0Vi0aBGKi4sxfvx4DB061FYpdf78efTu3RsrVqxA165dcfLkSaxcuRL9+/dH/fr1cfDgQTz//PO466670KlTJwBAnz590K5dOzz++OOYM2cO0tLSMGXKFIwbN+6mEqBF3FQBRntWjE4INjohV1UAqO6f6gWQ8g4YHTZixYPBeSFczorZ4kT1AksJCLPFidniwyjPSBnKn09xvV7zu5JC10woNoJPP/0U48ePR+/eveHm5obBgwfjvffes9mLi4uRnJyMvLw8AICXlxd+/PFHzJ8/H7m5uYiIiMDgwYMxZcoU2xp3d3esW7cOY8eORUxMDOrUqYMRI0aU64tTGUTcOAGqng0Oo8URh6rnhEO1F4yRSbtGh41EvND7Z6QAUb24mi9OuPVqx1Y5bKW4nuvMy3kN9ZrflRRXYRM/qxVWxZwb1fUUgYGBZMO+pk2blrt5joiIwNatW9ntNmnSBN9++63Svom4cQGMLhXnUA2NGF2tpJITA6iXa1N2o8NGfNIlvV6lWui63bnFi5nihhMvqjkp6p9NzfNitmellBM33E0JK4701pl7PhYqh4ibaoBqtZNqnxmzxQcfumG8DwauV81J4cQPv97YY2O2ODH67p+6gKt6VvjP5tzio4Sx2+sZqfx6Rc8Mt/863+3SgiqslnLysJQzI+Kmkqh4L/jQA2OnzQ5Yb6xnRn292XbSTDd6M1m8qHbJrcniBaA/n9GeE6PFiWpYp5Tz+tnpGans9lX3j12vs/+lxVVdLaUWBhNxI7gsTu+ZUSz5NN4zY5wA4dZyFzCjxZHZ4kRZfBgc2qEEiOH7zlX7cOtNFh/2ekYqvd4kz05pVSYUWx3Q58bAnBtnRsSNE6AqTlQx2jPDoRqWUl9vnPjhE25JM7/vBgtLZxcv6gnF9q9X3ndu37iLu8FhHV78GOzZYXNmmPOmQZ4dbp3gHIi4qQKMTj9TDTtxqHpujOwjUzk7/f6q85HIFv2scDI2H8jpwz4GixfV0I+RpeCq4oW7uJvtWeHFA2l2WnFlZb5TjkSzOiDnRjw3ggoqFU2qOTWqYSej+9QYHVYqMDi0U8iFfoj9K2ROhKpho/wievtGe17yuPd3YnFSGTsV+lG9eKp6TlTFi9Hiw6iclzKUPVclRaRds1b83bMW5ZPrHIoDEopZ13Y1RcSNC+Ds4xXMHzGgZlfvoktcAJljZ2aflsrYVRJuK/f+xooXTvypXECN9pyYHdZRF0dGJyTTf1tWvDCJunrrrcVVKG4EuxFxUwWoel5Uw05OX4rN2FWTbjnPDr99+9fnMw2/VC/ORntW+PdXEyeqn0/17r6EqXyhtm+8eHFt8WGvZ8S2vlhxver763luigvIdY7EWmplv8OV2UZNxFRxs23bNsydOxdJSUlITU3F6tWrMWjQIJtd0zRMmzYNixcvRkZGBnr06IGFCxeiZcuWttdcvXoVzz77LNauXWtr//zuu+/C19fXhE9kDpxjRnUwpJEdfK9vn7arzj/iGtWpTt6mBEJN96wYLV64brFGhn6cXbyo5/TQVUGq4kRZfDDr9cRJpbevs/9aFYobqZayH1PFTW5uLjp37ownnngCDz300A32OXPm4L333sPy5cvRrFkzvPLKK4iLi8ORI0fg4+MDABg2bBhSU1OxceNGFBcXY9SoURgzZgzZEtoeWAFBvIBzDyt7dpQ9Q6riRM1zwwkA1cnVquJIZf9NryYS8aK0fWq9s+fMmO1ZUd1+qeJ6ozxDWmnVNfET7MdUcdOvXz/069evQpumaZg/fz6mTJmCgQMHAgBWrFiBkJAQrFmzBkOHDsXRo0exYcMG7NmzB126dAEAvP/+++jfvz/mzZtnm0zq7PDiR9Xzwr0/bVeu6GHsXMKualiJS+otUAzNUHZuLRc24sJa+UX03bVq2Eo1Z0VVnJRw7694gefCUpTAMNtzwnounFx8qIadeM8OLULs3b5WUkiucyTSodh+nDbn5tSpU0hLS0NsbKztOX9/f0RHRyMxMRFDhw5FYmIiAgICbMIGAGJjY+Hm5oZdu3bhwQcfNGPXb8Dor5bRYSfVsBSfUMxc4BQ9K2xFkoHeEePDRoo5NyZ7VlTFC+95MS70o+L1uf7eauJEOWzD2J1dvHDhFuWwlU7CscaIJkeilWrQONdzJbZRE3FacZOWlgYACAkJKfd8SEiIzZaWlobg4OBydg8PDwQGBtpeUxGFhYUoLPxdfWdlZSnvr5FfH9Vtq4adDM+ZUWx0x1UzmdmF1+ywEbt9Rlyw5bZOLl5Y8aUgbowWL64uTnjPkFo1lL0Jwbb1TLWUrudGq7o+N4L9OK24MZJZs2bh1VdfrbL348cT0HZWPLDixNhqJj5spHaB5tbnsqEb2p5TQLv/VUJH3HtnM0P4uPWFjJ27uKtUC1Vu++aKE/797Q/9mC1OnF18sOLKTnFRWVTFke66UvpzORKr1QHVUpJQ7FyEhoYCANLT0xEWFmZ7Pj09HZGRkbbXXLx4sdy6kpISXL161ba+IiZPnoz4+Hjbv7OyshAREeHAvXcsRk/dNnJwJFCJPjKK682ef0TZVXNi2LCRahdbJxcvnOdHNW+ltJDuWUKKG5M9J+rVRmo5KUZ5Rhxl57Bb3Ci+7829l8aW7FdmGzURpxU3zZo1Q2hoKBISEmxiJisrC7t27cLYsWMBADExMcjIyEBSUhKioqIAAJs2bYLVakV0dLTutr29veHt7X1T+6My2VtVHLBhJcVqJc7zomrnPDN5nOfFYM9MNjMIT8X7wiYjM/um6nlRFQ9Gb5/tQ1NEJ2+a6T0xPqHXnJyS39/fNcVHdcJaCljdFJu41tDDaKq4ycnJwYkTJ2z/PnXqFA4cOIDAwEA0btwYEyZMwOuvv46WLVvaSsHDw8NtvXDatm2Lvn37YvTo0Vi0aBGKi4sxfvx4DB061KkqpdjBmOx6tfc32jOj3AdHeXKz0eXU9iflcmEj5ZwWk8ULO5/IYPFSyrTCNzL0Y7bnpNTghF0Oo7cvCCqYKm727t2LXr162f5dFioaMWIEli1bhpdffhm5ubkYM2YMMjIycMcdd2DDhg22HjcA8Omnn2L8+PHo3bu3rYnfe++9V+WfRXVEAoWq+FDNyWFzdhRLwQ0PG7EJxWrl2tT7q4aNWM+H4vwidr1q2MqFxQu3fWVxYnBOitnixtWxuLlXbNB0njcArdQKzU1Kwe3Bohl5VXYRsrKy4O/vj7T0dPj5+VX4miKuVwtxEShg1zLjA7g+MMzddzaT95HD9WJhLmA5TFiHS/jNzKMvEqphpRwmaTeb2T73/sXE+xcVquWsUNsGgJIiY3NmSoroY8flrBgtTlRyZgC10A8X9jG6FJrD1cWHrrhw0HrO7ubpVeHzWkkh8nctRGZmpu71QpWya9K2/r3g66nmg8gpLsFd3242dH+dEafNuXE22EZ75Fp621xODYfyVG9Fzww7HNLksJJqozqVvBfW88E2yVMMSzm5eCnh1ivn1BgX+jHac+Lq4oRDVXyo2t11xAu3XjO08YfgKETcVAHqgy3t9xoBlUj4ZTxDbCk25xniPC+c54TxXmTk0Rc47v0Lme0X5tvvuWE9MwVMyIvz3HDVSox4UBUvRntWOHFiZminuosTo8WFnmfEZufWezDrPTyV1rt716rweWtxAXLIlY7DWqo5IKG4ZooxETcOQiW4p9zHRjHhl2uCZ3gptmJOjGqjOi60wzZ6I+yqYSM24dhgceHq4kU16deotVWBq3pGyuDEibtXxeLDtp55fw9mvd72rcX0dh2J5NzYj4ibKkC1Gkq12olv4mdsNZPRYSVuPdsLRbFLLhWWYj0ritVEquLC7LCQ2eLF2QUKhdmeFXeDPSdGiRObXcczU4anT+0Kn7cWVZ24EexHxE0lMbTPDSN/+MGVamEnLmzFhXXYPjOKCb/c+xfk0+tVwkqq67mwEhf2KcmnHeCcOFH23BQV0HYRJ7o4u+eEDct4+dB2RhwYLz7o/fP0pi9vXpy9VsX20gI3nCZXOg6rprGe9cpsoyYi4qYKMLrDMOd5YUu1GfFktGeGXa/Yq4X3zCh2ASbsRoeN1O2Kk5NFvBhmV/WccOLE3pyTMjjx4lHLl7TreUZsdkZ88OKFPr5etWjPkreOuCnxom9YHEqpBs2iKE4k50ZQgaqm4jwz/PgB5uKqON7A+PEEatVKfIt9tYokdbu+QDA/bESLF1Z8GTwCwJnFC0ALED5nhBEPBntWOHHk6UOLD86zwooXb/r97fWclOFtpzgpw4dZX9+34v0vzq+ZOSyuhoibSsJpX5VScNUmeFzYiR1vwI4XoO9UMvPpC1wG08dGtVqpkAlLFXHrmbBYcQEdmqFCR8UFdFhJvdSa3vcSZvvqwxVdV5wAankfqjknnHjwZMSDqvjwqkWvVxUP9npGyqhbhz6+9X3pETqBzPpgP3p9vdoVr8/PrYUvyZWOw1pqhdWiODhTEooFo2ATfg2udlIeb8BWM6nZuYoi1bASP9yReX8DhysanpBbzcULJ06MDP2o5qRwnhMPzs6IF16c0HbOc+JTm15fmxUntD2wDi0+OHESyG3fh97/ejrHJ8+d/k06Es0BYSlNwlICBSdQKP2g6rlhB1cyF3/lPjOMZ4P1zHAJv8z2C3I5zwz3/nTFEZu0y3hfion1bJ8YLmG3moeFVMWJByMgVEM7lIBgxYmi58SHEQds2EXH8/D79umLOycegurSx45b34DxvDRgxFMD5vPpiZMyfL3caLtnxfbsrKrzhIi4sR8RNw6C8q6o5tywOTkmz24qMXm+EefZ4SuCmIolhS68rOdFxAtp58QL6z1RDP1Q3hPPOv7kWlXPCecZqVWXPrZcWCfYjxEndWnxERpArw9ixEd9xs6JFz8mYZgVL4zdq6Ti373FQt8sCc6BiBsHQV2+Wc+NYh8a1WonLuzEVTPxvVqMnUytPEJAebgikVBssDgR8aImXjxr0wLFq05dfZuiOOE8M7WZsEoDRc9KGCNOQhhxoypO6jPHhxMffoy9loX+bbjlXqHtedcqfN4ju6r6E0vOjQoibioJO1uKMPMJw2riQjkhmBlfkM2GndQSdtmwFRNWKs7NJO1FebSdC0txoSMqaVc158ZoVBNuVcUJXy7M2BnviVftOqSdC91Q3hFOfPgx4iAsgD52Yf70sQtn1nNhnWAmp4UTH5y48PdhSrF1PCNl6ImLMtwzaLv1WjppL7mSRtqLrqRW+HxBHn0+cCSapkFT7HNTU2dji7hxENT3hwsrcSFRoxOClT0zrGdFcQSBYlhJtVxaxftitmfFyGohwAGeFUXx4l2HXq/qHaHETX1GfIT505+9USAjbriwERd2YsQLJz78mbBPXQ/6vOSWR3tG3BnPCbIvk+biy7Q4KdURJ2UUXqS3n5tW8f5lF5p7QyJUDhE3lYRz7FF5NVzYiBtMyZVycwnDqh2AOc8MW2rNlWrn5ZJ2zjNDJfQCQEkBvX2ju/waieGN2Hxoz4cXIz5UxUltxvtRmwnN1GHsXGimUT39RnMNOXHCJMyGsWEf2nPCiY8AxrPChWXcr9F2LYP2jJReOk/aCxh7no64KCM37Sq9PpW251ykzwt5lyv+3eeWVl0TP2upxuZsVmYbNRERNw6Ccq7wgy8Zu+JUcDYhmJ2dpFZKzQ5/ZDwrKgm9lbIz72+m94WffGxsF1s2Z8Vk8eLHiBMu9NOkPt0llwr9NGQ8K5y4qVeLESeMeKkD+nvrlsmIl5xLpL2EER+ceClMoz0rOefp989Npfc/7yJ9U5OTzoiXK/R5JTuz4nB4XhWeD7RSDRp7a81vwyiuXr2KZ599FmvXroWbmxsGDx6Md999F76+Ff/uT58+jWbNmlVo+/zzz/Hwww8DACwWyw32//znPxg6dGil903ETSXhh1tSNnpxicF9ZNiwk2K1Eh+WYoY/Gh1WUhQvRoob1Rb9XM6LsueFSbg1W7w0CqTFCSdeGtWjj19DIimX87wEMGGfeozduyibtLtn054TZF4kzcVpKaSdEze5jDhhxUtaBr2eESe56XmkPe8qfV65yuQq6tkLNOfu/VSVDBs2DKmpqdi4cSOKi4sxatQojBkzBitXrqzw9REREUhNLR8u/Ne//oW5c+eiX79+5Z5funQp+vbta/t3QEDATe2biBsHQekXo/vUcIMpuYRgLqzE9Znhwk5swi9j58JKXFiKE09GihfVsJEnI05Yz4lvIGn38a9H2msx3gdfRnxw4qRJA/rzNQ+i7Y0ZzwznXQllyqnrEd4Tfw/6d+vGiA93Jmek+MJp0l6YTouT3Au0uMlOofePEx/ZF+jfXS4T9sm8Sifm2is+bO/PnVe5m0adO9YirYr73CiGpYzy3Bw9ehQbNmzAnj170KVLFwDA+++/j/79+2PevHkIDw+/YY27uztCQ0PLPbd69Wo88sgjN3h7AgICbnjtzSDippKoTPY2uk9NfhEtTpTDTtxsJ6PDRk7e64XyrrDVRsrDB9XCRly1EJezYrZ4iWCSdkOZhOH6TGiojqZ/AXZnck4s1y6Q9iJGvBSdP0PaOXHCeU6yz9E3FdmpjHhhPCeXmZsuTpxkMec9e8VJZe16VKXfxplzbhITExEQEGATNgAQGxsLNzc37Nq1Cw8++CC7jaSkJBw4cAALFiy4wTZu3Dg89dRTaN68OZ5++mmMGjWqwnCVHiJuHASZc8OGtNRyaviwE1OtpVgNpZ4zY2yjO1VUKo644YhszgsjXrzq0p6Z2oznhRMvnGdGVbw0ZcJKjZmKJK5iqAEjXmqX0Bdw90z9ihvt8jlybVHqadJecPYsaefES1YK7ZnJOkeHtVQ9K5cKndOz4ii83HQupJqFHzbohGRlZZX7t7e3N7y96d8/RVpaGoKDg8s95+HhgcDAQKQx+VZlLFmyBG3btkX37t3LPf/aa6/hnnvuQe3atfHDDz/gmWeeQU5ODv7+979Xev9E3FQSrlcA5X3hqqG4sBJr56qd2D4zTDUUU82k2keGrVYyOKzEiReVRnGedfzItT5+QaS9VgAtXvyYip26jD2CER8tQ/Sb2AFAC0bcNGVyWsK4RnW1mcnQTLmxRzrtPSm58BtpLzh3QteWdZouNc46RdszUzJIew4T9slkxAknPjh7FnPeyuc8zgaLj1ru9F28rwfTBNCD/t0H6YQk87VSgG6x4zA0qxXaTXgr9LYBXM93+SPTpk3D9OnTb3j9pEmTMHv2bHKbR48eVdonAMjPz8fKlSvxyiuv3GD743O33norcnNzMXfuXBE3RsCWghPih/PcKI8/YPvEqHUQ5hN61RJ+za5WYsWNgvfFi0vIrRtA2lUTckMYcdE8iPYMNatPixfVsFAQJ15y6dCKeyYtXopTfiXtRedocZN5Uj+pNusUfXeaeS6LtHOelSvX1Dwn17gWEly4mwtHK2oXXc/I/4cTL/aKk8raA8Iq/m3klladuHFkWOrs2bPw8/v9ZkvPa/PCCy9g5MiR5DabN2+O0NBQXLxY3ntYUlKCq1evVipX5ssvv0ReXh6GDx/OvjY6OhozZsxAYWFhpb1NIm4qCT9CQf9EoNphOIPpIMx5XvjBk7RnhPPMFOfSJ3Gqgy9gfFhJtUW/V10m6TZA/4dcJ5Bey3lWAhnPSpsw2jPULpy2t2SqibiwUEgdZngjJ07O0aGZotPHSHvu6ZOk/dqv9PYzT9GhnYwz+t/trPO0OEljPKqqnhNOnBgtPvx1BkuW4cd4TkJ96O9OCJPs7deI/m77N6HtAS1uTHgtZ28VUeHzWXkFwJgkcq2j0KwOSCj+/x40Pz+/cuJGj6CgIAQF0R5lAIiJiUFGRgaSkpIQFRUFANi0aROsViuio6PZ9UuWLMEDDzxQqfc6cOAA6tWrd1NhNBE3DoK6yVGe/WTwYEl29pKiOFFNCOZgc2IU815474v+CaMW47nwZcQDm9MSzCTkMttvyE1uZnJWuLCQ+zU18ZJ/8jhpv/YrXTGUeYruQnvttwzSfpUoR+bEy2XmpiWT8axwYR8OxvGBWu7MYElGnDTwor8bDZmp5XqeEZu9Cf27C2gRQtrr6YiTMnya3kLavZq3r/D5kuxcALPItTWBtm3bom/fvhg9ejQWLVqE4uJijB8/HkOHDrVVSp0/fx69e/fGihUr0LVrV9vaEydOYNu2bfj2229v2O7atWuRnp6Obt26wcfHBxs3bsQbb7yBF1988ab2T8RNJVGZ3M1VQ5ndhE+5msnkaiXDK5IUKo7qMKXIjRrQnhO2mogJC3H2IOYCVLsog7R7ZDBJtYri5eqx06T92nHaM0R5XgDgyiU6b+U84RXlwj6ZzEBY1ZwUTrxw4qSeJy1OOM8KNxWc85wEtqhP2jlxUrcVLU48m7Qh7VpoS9KeV7dij2xeFv2dciilVmiaWs4NmJtrFT799FOMHz8evXv3tjXxe++992z24uJiJCcnIy+vfGXdxx9/jEaNGqFPnz43bNPT0xMLFizA888/D03T0KJFC7z99tsYPXr0Te2bRaupU7X+QFZWFvz9/ZGWnq7rtkvNoUM7vxHx8V8u0u7r/WcySPvJ83RY6BrT7CrrMh0gzrvCdBrNZGa8MAnDqmEnLqzENaLz9m9A2usENSbt/owAqR+qn3TbtiG9b5ERtL1dMC28mjCemQYe9PfW4zKdc1Jy6jBpzz9Bi5crv9Dbv5JMh4U4z0oq02WW866o5KVwjhVVzwkvPmh7OOOVC2geQNobtKbDBfXaNiHtvq1ak3avFp1Ie0mD5qQ9w0IL/7NZ9Hf/yCX6vHVI57xbmJeD94Z2Q2ZmZqXCPPZQdk36PLwdajOeaY48aykeuXDE0P11RsRzU0m4myxqsjcfduI8L2phJzYhmLMb7JkxO6zEDVf0Zbwf4UTeDBc2akrMLgL4aqL6TAja4wodFio9RyfcFpyi7VePnibtRosXyrMC8OJFJWlW1XPSwIs+/XJhnWDmu9WgDV1pF9iKzjmp34kJ2zDixNKoLWnP829E2s9l0+Lk1yv0TeOhVNrDsu8MfdN3TichvJRpKio4ByJuKgmfUKz/Aq5UO5uxFzEdgAtyaXHClmozP1Yu50ZV3HCeGW9fOimX9byE0J6b+kzsv10T+v1vb6pv7xxC3yk1DWCSJgtor5kl+QhpL/x1P2m/fIAWL5cO02Gny8l0zo2qOLnMNKhUzUvhkmapippwJmwT4U8rz8CWtPgI6hBG2ht0akHafdp0Ju1uTWlxks+Ij18z6fPSIcZjvW8v7dVLOkUPvrySRm//GtNhOfcSna+l57HWSuhxMo5EK9XYNiTsNgwuyXdWRNxUEq4cj/LccIMvuWop1jPDhH3YhGGDE37ZnBhuPhKTE0Ml9AJ8o7pQpmKIbURHdNHlqonqWukur1xCbuEZut/ENSZn5eqvdCn11RP0BcbZxQtXTsyFfhoTk7kbBtPfmwat6ZySoM50WCewEx3W8W5PV6SUhtLr06z0/v/K9Nk5wHhGdp2khe/pFPqmixMvOen0byPvCtMhmhn7onde1UqNre4stw+axuZ7VmYbNRERN5WE+35Q52AuLMUNriwpUqt2MnsqNt8kTzGsxIwQ4HrBNGF6uTRhQkdUxRFXbeSerpaQm/Urffd77Vd6+1eP0675tEu0+DJbvKgmzTaroy9eACCskX4+VVA72iMYfCvtWfGPjCTtHi1vI+1FIbR4OZNJ/64PpNEX992n6e/Gvt9o8XKZKZXPSGVmWzGelQImF5Br/snhodMdXCspAv2tFpwBETeVhPPsURVPGUxYqYjrMMytVww7qSb8cp4Zb6ZPTO36DUl7QBhd8hnSmBY/MS3pi1C3pnR4oCOT2xDmri8A3H7dSa7NP0zb03fTYae0JDoZPPU47Xk5xfRASue6YzPCnIMTJyHe9CmqhS8tToJb0H/b0Cj6uxfStZ2uzadDN3KttUkkaU8tpUXzIaYD8c7Np0h74nH64p/OeE6unac9H6rigzvvcDdFXCFB3XA6Z8g/vBlpD9ZJ9i8pyMXuvR+Sax1FqaahVNHzorreVRFxU0lUmvixpdxch2DVsJTBnhk27MQ2yaMvQFxYiUroBYCmTNipCVPSGsyEltxT9b0jRWdoz8u1o/RwRNWEXFXPirOLl1DGexJ2e1PSHty1I2n36XyHrq0onF57Opv+Xe05n0Ha/3eCFgeHTtCek0vMYMzM83QDRC6sU8yEdTj0PCNl1KpH39TUDac9Yw0aBpD2VrfQ5527WwdX+Hx+bjZ2kysdR6mm3ozRoLmZTo+Im0rC5dxYCdcOVw3FDq4sohPYzA4rcTkz3PBHrlqJCytxIwS4iiRuBIAHMTwRoCuOsn+lLyBczgsnXs7mubbnpTXThTakA12OHB5Nlws36EaHdrw69CDtecH6oZ/kK/TvMvEcHdbZfJQWrr+dpL1ul7mp4Om0Zyf/Gr1e1bPi40//7XxDmpL2eg3paq6GzWmPcK+2FYuTMno0ocVNu6CKz2vZWd54nlwpOAMibioJlxScS5ScctVQ3PgE7g7J6LATW83E9JHxC6WrLoKZNuo9mH4bd99CJ252DqHDSv6Z9EWg5OA20n7xf/r3ced20OLm7FH67vwE01+JEy9coziuWiiCKUfmxEl4R/oC07A7c/fdgw79eHS8i7Rfrk1fIPen0kmzmxP0c5p2HKHFSRpTapxxlq5Uy7vMDP0soPfdaPER1Iz+Xd/CNOnr056eP3RHY1q8tPCjv7seZ+gRCblb/03az287UOHzOUX0b9KRSFjKfkTcVBLGuUKKH64aSrWDsGq1k6pnhkv4rcVcAFWrlbgRAlxFkpZKC5CcE/qToQHa+2J02EhVvHAt9FswfXa4sBAnXoLu7E7a3TrQ4iXNiw5d7DlLV/T8cIwWKPsIAZN2mu6OnHUumbSrek64m47a9ZnZSY3pDr5hzWjPxp0d6GPfuxUtnm4NpX/XARn077IoYTNpP7OZzmc7t+M0af/tRMXiNF8ztuP6H5GwlP2IuKkk3HwoqpcNmxDM2RnPDVctxaHumaHvwMIbB5D2nm3ou/vuEfT6Zl709GQc3kKaL22mT5Jnt9J5MycP6F+kfs2hL1CXGM8Ld2IKZMTJLUw1UEum10rjnnSL+tBedFjH/dZ7SfsFD/oCuD0lg7R//TOdcH3sGO0Zu3iSrjbLIoQv51Hlbhq4nBL/xvrJzAAQ1pz53XSkf5f929Hv3yWMFh9+F+ljn5e4kLSf/XEvad+7na70O5ZGe6xPMwOHuUo9vRuDIs24cQZ/xuoAz42Uggsk3BeEShq2Mj8iNqfG4D407kwHYM4zU5u5u28USHtmIrj5R0xOjPtl+iSYz5RTZ7Dl0nTuA+V9ucr1MGLOO1xOC9dIrhkT8mvYjW6AGHJnV9Lu0bkXaT/PiJdtzOiRNQfoarDkI7T35OJJOvSTk3aatFOhHy6Rvk4QPRspsBktXhq3pm8qHmAqve69hV7fwoe5Kdj/DWm+8GMCaT+TQP/ujjMhWe7GgPttcR2kud9OR50mjHnWUnxIf+0EJ0DETSXhwlKUuGGrobjxByW0Z0c17MT2mWGqmXyZaiMurMRVK3FhJW6EAFeRxHXZPZtJi08qdKQaNmKriZi5V+Fd6dBE2F1dSLvnrfeQ9jRvuovuttMZpP2/+2hheZwVL/QFlBMvXC8UqqLHN7QpuTakJd2HpmMH2vMyKJIWL3cxU7ODMuhwasHmtaT9zHe7SPupzfTv6tBV+the4DqzM78dzmvZirnpasUkqze9t+Kp4NkFRcBMOuToKErhgLCUQ/bE9RBxU0m4sFQ2EV4oLKDFSTHTp4YbTMnBx+bpk2j9MP1GZgAQ1YK+Q+zZnE4sbEVvHvh5E2lO+/Z70n5yA+0+P3gyg7SruLf9PWnPSwcmX6h1F1o8NO9/K2kP7PsQab8WRq//jhEnnycxx/bnNNJ+6QS9nhMnnFeTS5oNaksnLEe00f9t3B9Ne2YeaEuHfVp40PlA1t1fk/azC2nPyYEf6JyVA2n0eeUsEy7nLrrcVPFeQUyu3V20V/GWgTGkvdZdD5L283XoPjff6pTa5+dkAzM/Jtc6ilJNQylTqVuZbdRERNxUEq6JH+W5KS2hFxs9/oAt1WaaYXF9ZrjhkCFM3od7Bt0MLP8kfQHkyqkvMp1SuYoje2PzABBRi/7sEU0DSHvDGDrnJaAH7VkpiIgi7bvP0MJ6zc/0sT18kE6KNVq81KpH55XUa07PV2rGzG8aGqM/IqFfS1q0N8ymxUXud/8l7ce/SiTtyTvpkN3hLNrjyIV1uNEVrXzp80L71rTHt8X99N8m+H5anOS3vJO0rz9Nf7dXbaZHl+zfX3ELiNJC2pMsOAcibipJAdMPJIO4u+cShrnERO4Ez8X+fZjExcAwuuQymql6uIvpF9HISod9ihLXk/bf1tHu8SPb6BkzP2fSuQWZzNR1zvvS2V8/rNbuLvruvtUjPUm7Z8+hpP2olT72KzfRCbM/7KSF5dnDtDjJYkKC3HeXq+gJak2Ls05R9PoR3ej5TbHNAki7xz79vJNzM9aQazd+Q188k5gydM5zwoU0b2H6R/WNpM8LrQfTxz5oEP3dTA+OJO3rf6Vzbv79I92i4dQ7X5L2Kyf2kXbuvKt301eVgzNLNfWwklRLCSQqCcV8qbda3wRO3HBN9Or40TkvzZicGS7h1y2NFh8ZyfRJTCWhF+Ab1fGJh7T3pVHzAF1bwzvopFGf2+lqorNMQu6Gw3TYZ/N+2vNy4Ridl5F9gfY+qIqXkLZ0zk/U7XTI9ImYpqT9zlD6b6dtXUHaT362Ttd27Dv62Oy5RotqznPCierbmFy1dr2bkvYWj/Yl7e530eJl+1U652XJt3Reyq49dL5V2i9Mn5pL9HmFOy8GMl69Rh0qLpUvLczFgV10JZijEHFjPyJuKgnXxI9OKKaVPheW4mCrnZiE4Hr16JMkV80U6EmLh+LT9B0sV63EhZW4XjDcjzvIm+n1olM1UQZVcRTQ/W5ybVYwLX62/UoLu/X76NDEOSaZOpPxvHAJt1xYiPO8cOLlbz3ovIiY+kx37x8+Iu2/rtxI2g9s0fdscR5BLpzJVetER9CVbu0eofOlwof+lbRfiqDzjVYfpZO5P9lCewVP7afFzbXTh0k7993jqtHCO9DC+a7udE7PU9EVe/1ysrPQcza5VHACRNxUkkKmXKqAcCFz7k+uTw1X7cT1oeESgrk25V3Cac+P+7EtpP3Md3RCcPIP9ElyX4ZaWIm7iHRn2ri3f4wuhw5+dLSuLdmHHg/w4Ub67v+HbbRXK/UQPeWGmw/E5Vs1vL0/ab+tG32BmdCTbuLXrS7dqyR/3Xukfc8SukfRrp/pnCCu3JjKO6HCkQAQ1YcWZi1HDSbt2l3DSPtqppR64XfHSfvJvbTX6upvP5N2LhfQr1Er0t6h/0DS/lgs/d15lGkiWO9XplR9+UzSfmj8LxU+n6eYA3kzSEKx/Yi4qSTFzF0YlTSs2qeGnY7L9KHhEoKbBNCemXruTMkm45nhhj+m5KmFlbjEx2ZMQnN4FO19qH8HnbiYUV//JL7xIB022n6AFh9cQi4nXrhKufot6NlLbW+jw0rj7qLFGyde8r75F2n/+V+0MN5xmPYunGZmb3F9hHrU1z9+nYbQgzObPPEEab8QQXdnXrL1NGn/KoEWxucO0MKXC+twwje0g/5QUQDofged7/RCL1q8dNBor+TVxf8g7VuW/ETatzMJx2k6pepFqMomfuphKa4Yproi4qaScDk3VF6NajUUd4FiT0LMeIOmAbTdPYNpcvcrlzNDz9jhqpW4sFIDL/przFYk3U3H3tGeHgGwmwibfZNEn6AvHKO9Vlw1EZdXUK9pB9J+SyR9AXrm7ltI+x316e9u/ho6LMSJl/8dosULl3TL9UK5i2ly2GmUfugmZMQz5NoDoL1ac76hheuun+jf1cVftpN2bvYU51lpEU2Hdcb0pdc/yvTxwQ+0sD2wiC6F3/4TfV46mk2nA3A3RbE6VaB51lIspZ1mghMg4qaScNVS1IgFLizF4VWHPgFz1U7ceIN2QbR7vWgtXc104psDpH33BTpnhkus5MJKd3WmP1/UhPtIu9sDE0j70oO052nRN/ru+9N76Pk23N0zl5DbtCt99/+3B+icnpGRtNdKW8uEhUbTjeC27KM9V5w44f72f2WaFEZNoMNq7gPp+c6fHNa/ii38F+2x/G0XXc2j+rdv1YtOCH7q/rak/YlIugwe3/6TNO+f8Cppf9vgv33PTvTv/imD/vb5OdlAb9pr5ygkLGU/Im4qCdfEr4TI+1CthvKopVbtxCUEe2TQ3oVLTDXTNca9e42pFuOqldiwUlf6DpmrSEq6ROf0fJXEVHX8qh8eUHX9c9VE991J53X8pR19AbBsofMuDv7rO9L+v/3GXsB6R9EX4Ntefpi0F97zFGmfn0j/bT9Zp98BOWUPne/D3dRw1Tpt76QThl97oOIOumV0d6c/29np00n7rqV7SPv2K2oz7fQ8I2VEj6YTnsPGvkjat+TRuYgzFtKf78i2iu3WYmZshQORain7EXFTSajBmADdy6aESRjmZ9TQWf2RLehmYlxCcMn/6DvME2vpxMI91+jPx1WNcCWtMUPpu6Sm8ZNJ+6Yi+g542uf7SfvRLXTsPif9tK6tbhgd1unQmx48+fqD9GfvodF5F7/G0xUzn31Bex+4RnBcF9rhTJfZ2/5vBGm/2GUIaR//A500+8PYz+ntH6FDO9SNRdOudAPFsQ/TIcHRHekqxpxlr5P2nXePJ+0vptIeU+6ix4mPKbMfIO0Bo18h7UuP0flY/f9LV1OdfnQJaecGDjdodTtpv3dwzwqfL87PwZe05ncY18WNqufGQTvjYoi4qSRUqTcAlBLVVFxCMZcw7FOHaVPOnIS4hOCs43SvkytMF1uuWolL2mweTldzhd9Ney8yAukuvmt/pAXA2WO050rF+xLeIZJc+0RPWvxwCbnn3/2QtO9ZQ5fjcuKFK5PvzTSC48RLehQtXiavo8XXlvX0ZOnLv9J351wpe6u7e+rapj1Ce176+dLhzFOTJpL2jZ8cJO1cKTrnFevfi8636jztWdL+a2Na3P3138z+b6Q7MGem0H9735CmpD3y/gGkffbDnUh7dEHFOVFZOYWgbwcFZ0DETSXh+tyUFOl7briEYU8m7FSrLu3Z4aqd2IRgphcKV83EDbhrWlutWsm7Syxp33SOvkPlKpIymH4bHFTF0b096AvIQ21p13nef+hy1Z8/pitiuEZynPC8h9m/Lv9He4Y4z8vLTFLt1rX0BZArV+YugB3vpZPF3x0aqWtrf2EbuTbp+fmk/dutdHdovWqdMqhKLgC451n6swU9N4O0f/ALnZD8wRt0MnjKbrqHEMctPQeR9hcepcXliGZ0vDv1bTqstezDir97+VpVloJLWMpeRNxUkmzmREPF1zlx4+VLJwSHNw4g7bc3pBOOixI+Ie0nvqM9N7/mqN3d39GjEWlvPekF0v5jEb3+lU/oTqa/bv2BtHNT2VvcTbvfXxmhL24e9qerfY48Sfc6Wb2GbrLHTVbmQguxM+heI9ZHp5D2sWvpqdwbRtOhA06cBDDVXkNeoCuW5g+kE6rd/0OHfn68bZyubdFF2qvGeU4evJ/2OHaYM4u0/zeLzqfqs4z+XfwWRycEcx7l1vf0I+1rVkwj7bEa7VXcHz+dtH9912nSPoEpVOjPnDcfXVix1zErrwDPPvEaudZRSEKx/Yi4qSRcWIoLPVF4MUmlTRqojT+4xiQEn2camXHKnxtPwA1/LG4WTdrXfkvnVVAJvYB6YifXyXRga/2cp/R59MUzcQNdCs6JF66RXMwzdC8Sn8fpvIgXmWP//eodpJ0TL9yxf/DRnqT9rQGtSXvmfFo4r51DN3qjGkhyx/7+5+j+SA0mzift8d/T343PV35F2rnZStyxf2AI3V17/v0Vjycogzv2y+bSx57zOnLH/28T6bAZd/wn6hz/wpIcAFUjbgT7EXFTSTLzaQHAJa9R+AbTLeh7MYMrvU7QSZFcQrBq3sWdA+i8keCxdMLv69vosNhXn9OfjzuJh3Sg3fN/f4oWAM+3oCt+Dg3V9+x8wcwf4kJ6I5i8iOjFc0n7vzPoaqPpz9Fl/uf3fEva/RvT5cZP/t9zpH1+HP35zk4eQ9rfGU4ng18ooP92gwhhCgAj1+lfxLYF9yLX9v2QHvh6/M4JpL0WM/C2+1/oUud/f0w3ESxZSOf8fDWGnp8Uz+T8cF7DRz+k96/HXU+T9pGL6ZDsC+v1h54CgCWB9kp26l9xC4mSAtpj50g0QLllYM3024i4qTQqnhuuGqp2XbqDcDMmp6boIB0a4BKCuQtsRC3GM3MHXdFzzo2+gPy4n076zGASC7ly6uaRTUn7XzvRAuDaMvokuH2zvjjjevjczzSRu+0fI0n7/0B3CJ7zKZ2zciHpe9LO5awMGNKbtHPiJeVlulT702UHSDt3fP/anW4TELP8LdK++LK+wJg5cwO5Nu1nulQ8qA1d6vzCM/Sxfa4xnROz9yE6oXbV5tOkncvH+vvj9O++xbuLSXv8JjrhetVTH5B2biJ9szvocPL8cTGkvfeVrRW/b24e6NtNxyFhKfsRcVNJCllxo3+HyMWuazPjEYKZPi85TLUTlxDMwVUz1e1Ou3+/OZNB2rlqJa7Tamhn+g6aq0gKSaE9Q5v/RSeOUp6vtoxw7RpPX8DSbqX7uExcSDcJ/G07fQH28adP03f9pQ9pX/Qg7blJiX+ctHPiJZPpkTQ8jhZ3nT+mu+BO2kN7H5Z+pF9Kzl1cW/Z6kLQveY5uA9B5/3LS/uVgWjxsvpRH2jnPyoAldD7T3nb01PDBr9G/q5Nb1pB2Lt/q5TdeIu3TO9PibN8Tj5L2iT9WHM4v1Kpu/IJgPyJuKkk2M6OmOF//Auxdl04Ybtec7ncRXkInpe5dS4dluIRg7gJ8e3wcaT/cgB4sOWtOxXdAZXB3uFxYaR4jEAZe/pG0/3fom6Sdu0gMIgZv3rv2HXLtu1doz8KbT9EXZ65ctkN/OmH5qwl0SM7jn3TexJxAOm8ii7kpGPEXOm+j1UefkfYHltPlxj/9he6y68GMNnngiYd0bR/H0ZVg+/5Kl8F/2vzvpH0HU2U4ao7+vgFAu0F0OHjoXFq0//WV1aQ9uB2d8zLrBfq8MeQh+ne1/mG6muv7NXT37I1NaI/uvavp0d67p1Wcz5Wfmw3cy4xscRBSLWU/Im4qCdXHBqArorjZUM2D6Dsotyt0Tgo3u4n7crfwp8WNbwxdir3+GO1evnScLrXmjs8tt9FdeO9vRYvDX2etIu07r9JNCG+pQ4cVu03UrxrZ50ufBD/4Jy3suDL1xjH0aIlFo+lkbf//vkHaF8+l94/rPv3Ew7Rnp+XH/yXtcYvovIpdq+gmfdwIg/gXaM/YpDD9NgI/3P4kufZrJhzMhST7fk9fvN9MpT/b20/Swo4butp9OC3OvhtLf7dOjKLF10SmgSRXqPDmh3QbguN9ac9O29lbSPuZHRWLK63U/uKRm0XCUvYj4qaSlBTZn3PDVUM1C6Sb9BUzU7e5aiduvAHXZ4arZtqwXm1+UgCXlMoMb7RsWkrauYokLucoNrYpaacqjuL/SR+bc3voMnWuomXaU7TXrOORL0j7iol0xc3pPPq79cQAuhKu5RL6/e9ljs+ez2nPDffdmft/tOfq4Yt0wvSSHm/r2rgmeqPvo49N+8/WkPaeC5geRp/r7xvAH5tFH0wi7UMu0W14V4T9jbRzx4f77nDHJ44Zn7BrMO254o7P4o8qLpXPz8nG2J6fkmsF8xFxU0kKmaoLynPDVUN1bUSLn7P/ol3/XLUTN96g/UT6JDWTqWb6eT2X10E3gnuZGWz5yFW6GdjiR+eT9tNMSPGll+iS19wJ75P2tkTFEVdtdPuQx0j7pmciSfvuuPtJ+4SddD7T4DZ0sveoY/QFru9ndIPIn5iKIC6pdtMXdBPDZv+ZStrfj2HKmbmk2WWjdW3ezemcjbjp9AUwvw+dqD58wkjSvmU5nfPyZU86Z2bb7XTI9LYhdI+gB07TfXTem0F7/RYn0MK67Wt0OHvT//Uk7Wd30H2CFq+mz1t1fqo4JGxh+pY5EglL2Y+Im0piZXIHKLhqKK5PzbETV0g753loEkYnBKMNndj444d0F9mCTDoniKtaGNKB9hwd+xsdeuDEHef+b/TcP0h73Bd0KT1VcRQeRecdfPwEPVri9IsjSfuqXbR46RVEewV7r6PLfR/9jv7bbv83fQHnxMvqWXToImRhPGmfM3cLaefyyZ74ka6WmnBaPyfq4/F0Gb5vaFPS/uVHtOckah0dMvzHX+mJ7FxY5+0f6CZ7szV64vy8gXSvF8/a9O9uyRJ6/aDDH5H2t4LpnCVPC+2ynvsZLf7+3bjinKr8XJkK7gqIuKkkhfn2VxyFNqY9M3VT6Yvnz3tTSbu/J333ySUEL0mm3ceHNtDlwlxVw/IX6YTg/FdGkfZ/fUmH5R5sSefc9Eyi28R3mUU3ojvyHT1JhspNSHgogFy7sh0d8vs5kxZur86nwy7re0wg7fVG0+LGsw59gVq3kk76bPER3eJ+blN6PddFdvZFuiKn+7uHSPv4p+iwF+VZy/qvfvdiAPhvJF0t9UMH2iMYOYPuYxO1lx7o+rcX6bljr86jw8XL3qbPW0d7ZZD22a/RNyW++1aSdp+f15H2Df9HJ8Pv+y+9/W+O0lPXf+pZsTjKyrbgeXKl47A6wHPD3PtWW6qNuFmwYAHmzp2LtLQ0dO7cGe+//z66dqXzEW6GEiZxkir3bsTm1Bwg7dyMGe4OjUsI/u8mOrTAeWZa3U2XgkfVyiLtX31OJ83WcqfFW7eJ9EVgwQHa83V8Gx324qq1lg+P0rXtHUqHjbZfoZOZx/+1PWm/8Bc6LPPSOHr8Acc/X6dLuduupEMrs+fRFTn3McL/nl/oC/itr2wh7Se20o3cuPENHzXSHxEwuwUdzq3FJLvNTaLFxwOJ9My5H0fQx759fzpZetfztPdhfQe6CnHeZbraadq7fyHt/4mij33AYDqhOiCCzpk5tvFd0n51+CDS/uy0ihtwFim31as+zJw5E+vXr8eBAwfg5eWFjIwMdo2maZg2bRoWL16MjIwM9OjRAwsXLkTLlr/nYF29ehXPPvss1q5dCzc3NwwePBjvvvsufH3p38QfqRbi5rPPPkN8fDwWLVqE6OhozJ8/H3FxcUhOTkZwMD1/pbJw4oZq1MdVQ+WfpFvccxUpUQ1o8VTUnA4NnP4nnRfCVTM9eBddzZS7hi5n5qqV+jBhJa9h9En+g3j683GzpZ4eSbvnqYqj+UyjtIFMueotS+hqoo7MZ8u+QHdInvIG3UG4764FpP3VGXRCdL8I+m/X8xjdxbc98/nOJ9H5aFPn0BUz404uI+2TYv+ja+O+l9xna8N+tq9J+7R5dFiL+2wTQ2lxwX2+N3Jojyr33Tz/73mknftuPvPrx6T9tVB66jf3+d7Jr7g5alZWFpaG0N2jHYWzh6WKiorw8MMPIyYmBkuWVO5Gas6cOXjvvfewfPlyNGvWDK+88gri4uJw5MgR+Phczw8dNmwYUlNTsXHjRhQXF2PUqFEYM2YMVq6kvXF/pFqIm7fffhujR4/GqFHXwxuLFi3C+vXr8fHHH2PSJPoEUFmK8uiW21RF1F3N6KTNE2/RJ8F8JiPs9ufoJnZvbj1N2s/uopNGIwc+QtqpclkAmHkf7Z7mEp7vOUzf/bceT18ErjLjGdZ+Mp20N5hEJ/1O+kr/7n7qDHq44IoedE7JoB7PkvaO99F3x7+upMt5Z3Wku9iuqUWfImZfoStW2k5kehzF0hUtny2mf79NXqPzMhbE0k0Is/+P9k403aovnh6cMIdc2+Y1Ohya/DLdgHBue3o8QuBfaGFX+yT9/ut+vp20/+tn+ne34vhV0r42fxlpf3c33aPor9/RHt/PH6bnti28NYO0/9iL9qruv1zxjYFnNt1U1JGUQj0h2Mj051dfvV5RtmzZskq9XtM0zJ8/H1OmTMHAgdeH9q5YsQIhISFYs2YNhg4diqNHj2LDhg3Ys2cPunS5npP4/vvvo3///pg3bx7Cw+kWCGW4vLgpKipCUlISJk/+/STp5uaG2NhYJCZW3Hq+sLAQhYW/5zJkZl7vR5Gdna37PiV5dM8KNyJy4gs6p+UMkzDMDBeGWySdlPrtNrqTKte3YWBXuovtLwvpqoTTBfT2H/wbfZKds40ZLrmXvkO8dRAtACJP0fOVXv2anl7ct2WAri2dmao9+2+0Z6T+LXQp+Ldj6bvTD9vR1UJWC+1if2AHPVG+/UQ6pJd64H+k/T8L6Zwc37/Twvo9nS6yZbz6L1qYji+hQ7Zrx+qH/R7+Oz33anYBnfD7dDM632d4/xakPXU67RH1j6MTdpt0pW+Kzi2ifzdv304PBr3GFEpMTaVv6qLG0/2pCqfQwnbvF3TINrlXX9L+4jsVi6uyDsVaFSTqOiIEVraNrKzyn8fb2xve3nTCvaM5deoU0tLSEBv7++/O398f0dHRSExMxNChQ5GYmIiAgACbsAGA2NhYuLm5YdeuXXjwQTqXzYbm4pw/f14DoO3YsaPc8y+99JLWtWvXCtdMmzZNw/V5YvKQhzzkIQ953PTj5MmThl3X8vPztdDQUIftq6+v7w3PTZs2zWH7u3TpUs3f35993fbt2zUA2oULF8o9//DDD2uPPPKIpmmaNnPmTK1Vq1Y3rA0KCtI++OCDSu+Ty3tu7GHy5MmIj/89HJCRkYEmTZogJSUF/v7+Ju6Z65GVlYWIiAicPXsWfn50DFsojxw7NeT42Y8cO/vJzMxE48aNERhIV2mq4OPjg1OnTqGoyDHdkDVNg+VPpfF6XptJkyZh9mx6NMXRo0fRpg09OsVsXF7cNGjQAO7u7khPTy/3fHp6OkJDK+6foueO8/f3lx+6nfj5+cmxsxM5dmrI8bMfOXb240blIjgAHx8fW4JtVfLCCy9g5MiR5GuaN6fzxfQouyanp6cjLCzM9nx6ejoiIyNtr7l4sfxIn5KSEly9elX3ml4RLi9uvLy8EBUVhYSEBAwaNAgAYLVakZCQgPHjx5u7c4IgCILgQgQFBSEoiM6ztJdmzZohNDQUCQkJNjGTlZWFXbt2YezYsQCAmJgYZGRkICkpCVFR19tsbNq0CVarFdHRdF+wP2Ks9Kwi4uPjsXjxYixfvhxHjx7F2LFjkZuba6ueEgRBEATBsaSkpODAgQNISUlBaWkpDhw4gAMHDiAn5/eKsjZt2mD16tUAAIvFggkTJuD111/HN998g0OHDmH48OEIDw+3OSfatm2Lvn37YvTo0di9eze2b9+O8ePHY+jQoZWulAKqgecGAIYMGYJLly5h6tSpSEtLQ2RkJDZs2ICQSvYi8Pb2xrRp06o8c7w6IMfOfuTYqSHHz37k2NmPHLvfmTp1KpYvX2779623Xu/6vHnzZvTs2RMAkJycbKtIBoCXX34Zubm5GDNmDDIyMnDHHXdgw4YN5UJwn376KcaPH4/evXvbmvi99x7d1PHPWDSthg6eEARBEAShWlItwlKCIAiCIAhliLgRBEEQBKFaIeJGEARBEIRqhYgbQRAEQRCqFTVe3CxYsABNmzaFj48PoqOjsXv3brN3ySnZtm0b7r//foSHh8NisWDNmjXl7JqmYerUqQgLC0OtWrUQGxuL48fpaec1hVmzZuH2229H3bp1ERwcjEGDBiE5ufy8qoKCAowbNw7169eHr68vBg8efENjyprIwoUL0alTJ1uzuZiYGHz33e+DXuW4VZ4333zTVopbhhw/faZPnw6LxVLu8ceuvHLsnJsaLW4+++wzxMfHY9q0adi3bx86d+6MuLi4G7ojCkBubi46d+6MBQsqHvRYNsZ+0aJF2LVrF+rUqYO4uDgUFNBDQ2sCW7duxbhx47Bz505s3LgRxcXF6NOnD3Jzf580//zzz2Pt2rX44osvsHXrVly4cAEPPfSQiXvtHDRq1AhvvvkmkpKSsHfvXtxzzz0YOHAgfvnlFwBy3CrLnj178OGHH6JTp/KDVuX40bRv3x6pqam2x08//WSzybFzcio9haoa0rVrV23cuHG2f5eWlmrh4eHarFmzTNwr5weAtnr1atu/rVarFhoaqs2dO9f2XEZGhubt7a395z//MWEPnZuLFy9qALStW7dqmnb9WHl6empffPGF7TVHjx7VAGiJiYlm7abTUq9ePe2jjz6S41ZJsrOztZYtW2obN27U7r77bu25557TNE2+dxzTpk3TOnfuXKFNjp3zU2M9N0VFRUhKSio3et3NzQ2xsbFITEw0cc9cD26MvVCesoZWZYP3kpKSUFxcXO74tWnTBo0bN5bj9wdKS0uxatUq5ObmIiYmRo5bJRk3bhwGDBhQ7jgB8r2rDMePH0d4eDiaN2+OYcOGISUlBYAcO1egWnQotofLly+jtLT0hi7GISEhOHbsmEl75ZqkpaUBQIXHsswmXMdqtWLChAno0aMHOnToAOD68fPy8kJAQEC518rxu86hQ4cQExODgoIC+Pr6YvXq1WjXrh0OHDggx41h1apV2LdvH/bs2XODTb53NNHR0Vi2bBlat26N1NRUvPrqq7jzzjtx+PBhOXYuQI0VN4JgBuPGjcPhw4fLxe4FmtatW+PAgQPIzMzEl19+iREjRmDr1q1m75bTc/bsWTz33HPYuHGjKdOlXZ1+/frZ/r9Tp06Ijo5GkyZN8Pnnn6NWrVom7plQGWpsWKpBgwZwd3e/Ibs9PT39psaqC+XH2P8ROZblGT9+PNatW4fNmzejUaNGtudDQ0NRVFSEjIyMcq+X43cdLy8vtGjRAlFRUZg1axY6d+6Md999V44bQ1JSEi5evIjbbrsNHh4e8PDwwNatW/Hee+/Bw8MDISEhcvxugoCAALRq1QonTpyQ754LUGPFjZeXF6KiopCQkGB7zmq1IiEhATExMSbumevxxzH2ZZSNsZdjeb1Mfvz48Vi9ejU2bdqEZs2albNHRUXB09Oz3PFLTk5GSkqKHL8KsFqtKCwslOPG0Lt3bxw6dMg2qfnAgQPo0qULhg0bZvt/OX6VJycnBydPnkRYWJh891wBszOazWTVqlWat7e3tmzZMu3IkSPamDFjtICAAC0tLc3sXXM6srOztf3792v79+/XAGhvv/22tn//fu3MmTOapmnam2++qQUEBGhff/21dvDgQW3gwIFas2bNtPz8fJP33HzGjh2r+fv7a1u2bNFSU1Ntj7y8PNtrnn76aa1x48bapk2btL1792oxMTFaTEyMiXvtHEyaNEnbunWrdurUKe3gwYPapEmTNIvFov3www+apslxu1n+WC2laXL8KF544QVty5Yt2qlTp7Tt27drsbGxWoMGDbSLFy9qmibHztmp0eJG0zTt/fff1xo3bqx5eXlpXbt21Xbu3Gn2Ljklmzdv1gDc8BgxYoSmadfLwV955RUtJCRE8/b21nr37q0lJyebu9NOQkXHDYC2dOlS22vy8/O1Z555RqtXr55Wu3Zt7cEHH9RSU1PN22kn4YknntCaNGmieXl5aUFBQVrv3r1twkbT5LjdLH8WN3L89BkyZIgWFhameXl5aQ0bNtSGDBminThxwmaXY+fcWDRN08zxGQmCIAiCIDieGptzIwiCIAhC9UTEjSAIgiAI1QoRN4IgCIIgVCtE3AiCIAiCUK0QcSMIgiAIQrVCxI0gCIIgCNUKETeCIAiCIFQrRNwIgmBjy5YtsFgsN8zM+TNNmzbF/Pnzq2SfBEEQbhYRN4LggowcORIWiwUWi8U2WPK1115DSUmJ0na7d++O1NRU+Pv7AwCWLVuGgICAG163Z88ejBkzRum9BEEQjMLD7B0QBME++vbti6VLl6KwsBDffvstxo0bB09PT0yePNnubXp5eVVqqnFQUJDd7yEIgmA04rkRBBfF29sboaGhaNKkCcaOHYvY2Fh88803uHbtGoYPH4569eqhdu3a6NevH44fP25bd+bMGdx///2oV68e6tSpg/bt2+Pbb78FUD4stWXLFowaNQqZmZk2L9H06dMB3BiWSklJwcCBA+Hr6ws/Pz888sgjSE9Pt9mnT5+OyMhI/Pvf/0bTpk3h7++PoUOHIjs7u0qOlSAINQsRN4JQTahVqxaKioowcuRI7N27F9988w0SExOhaRr69++P4uJiAMC4ceNQWFiIbdu24dChQ5g9ezZ8fX1v2F737t0xf/58+Pn5ITU1FampqXjxxRdveJ3VasXAgQNx9epVbN26FRs3bsRvv/2GIUOGlHvdyZMnsWbNGqxbtw7r1q3D1q1b8eabbxpzMARBqNFIWEoQXBxN05CQkIDvv/8e/fr1w5o1a7B9+3Z0794dAPDpp58iIiICa9aswcMPP4yUlBQMHjwYHTt2BAA0b968wu16eXnB398fFouFDFUlJCTg0KFDOHXqFCIiIgAAK1asQPv27bFnzx7cfvvtAK6LoGXLlqFu3boAgMcffxwJCQmYOXOmw46FIAgCIJ4bQXBZ1q1bB19fX/j4+KBfv34YMmQIRo4cCQ8PD0RHR9teV79+fbRu3RpHjx4FAPz973/H66+/jh49emDatGk4ePCg0n4cPXoUERERNmEDAO3atUNAQIDtPYHroawyYQMAYWFhuHjxotJ7C4IgVISIG0FwUXr16oUDBw7g+PHjyM/Px/Lly2GxWNh1Tz31FH777Tc8/vjjOHToELp06YL333/f8P319PQs92+LxQKr1Wr4+wqCUPMQcSMILkqdOnXQokULNG7cGB4e1yPMbdu2RUlJCXbt2mV73ZUrV5CcnIx27drZnouIiMDTTz+Nr776Ci+88AIWL15c4Xt4eXmhtLSU3I+2bdvi7NmzOHv2rO25I0eOICMjo9x7CoIgVBUibgShGtGyZUsMHDgQo0ePxk8//YSff/4Zjz32GBo2bIiBAwcCACZMmIDvv/8ep06dwr59+7B582a0bdu2wu01bdoUOTk5SEhIwOXLl5GXl3fDa2JjY9GxY0cMGzYM+/btw+7duzF8+HDcfffd6NKli6GfVxAEoSJE3AhCNWPp0qWIiorCfffdh5iYGGiahm+//dYWFiotLcW4cePQtm1b9O3bF61atcIHH3xQ4ba6d++Op59+GkOGDEFQUBDmzJlzw2ssFgu+/vpr1KtXD3fddRdiY2PRvHlzfPbZZ4Z+TkEQBD0smqZpZu+EIAiCIAiCoxDPjSAIgiAI1QoRN4IgCIIgVCtE3AiCIAiCUK0QcSMIgiAIQrVCxI0gCIIgCNUKETeCIAiCIFQrRNwIgiAIglCtEHEjCIIgCEK1QsSNIAiCIAjVChE3giAIgiBUK0TcCIIgCIJQrRBxIwiCIAhCteL/AbKtINzqqoBFAAAAAElFTkSuQmCC\n"},"metadata":{}}],"execution_count":147},{"cell_type":"markdown","source":"Transformer","metadata":{}},{"cell_type":"code","source":"temp_mha = MultiHeadAttention(d_model=512, num_heads=8)\ny = tf.random.uniform((1, 60, 512))  # (batch_size, encoder_sequence, d_model)\nout, attn = temp_mha(y, k=y, q=y, mask=None)\nout.shape, attn.shape\ndef point_wise_feed_forward_network(d_model, dff):\n  return tf.keras.Sequential([\n      tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n      tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n  ])\n\nsample_ffn = point_wise_feed_forward_network(512, 2048)\nsample_ffn(tf.random.uniform((64, 50, 512))).shape\n     ","metadata":{"execution":{"iopub.status.busy":"2025-04-13T14:09:08.318841Z","iopub.execute_input":"2025-04-13T14:09:08.319010Z","iopub.status.idle":"2025-04-13T14:09:08.432167Z","shell.execute_reply.started":"2025-04-13T14:09:08.318997Z","shell.execute_reply":"2025-04-13T14:09:08.431642Z"},"trusted":true},"outputs":[{"execution_count":148,"output_type":"execute_result","data":{"text/plain":"TensorShape([64, 50, 512])"},"metadata":{}}],"execution_count":148},{"cell_type":"code","source":"class EncoderLayer(tf.keras.layers.Layer):\n    def __init__(self, d_model, num_heads, dff, rate=0.1):\n        super(EncoderLayer, self).__init__()\n\n        self.mha = MultiHeadAttention(d_model, num_heads)\n        self.ffn = point_wise_feed_forward_network(d_model, dff)\n\n        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n\n        self.dropout1 = tf.keras.layers.Dropout(rate)\n        self.dropout2 = tf.keras.layers.Dropout(rate)\n\n    def call(self, x, training, mask):\n        # Explicitly pass arguments as keyword arguments\n        attn_output, _ = self.mha(q=x, k=x, v=x, mask=mask)  # (batch_size, input_seq_len, d_model)\n        attn_output = self.dropout1(attn_output, training=training)\n        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n\n        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n        ffn_output = self.dropout2(ffn_output, training=training)\n        out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n\n        return out2\n\n\nsample_encoder_layer = EncoderLayer(d_model=512, num_heads=8, dff=2048)\n\n# Call the encoder layer\nsample_encoder_layer_output = sample_encoder_layer(\n    x=tf.random.uniform((64, 43, 512)), training=False, mask=None\n)\n\nprint(sample_encoder_layer_output.shape)  # Expected: (64, 43, 512)\n","metadata":{"execution":{"iopub.status.busy":"2025-04-13T14:09:08.432828Z","iopub.execute_input":"2025-04-13T14:09:08.433006Z","iopub.status.idle":"2025-04-13T14:09:08.663332Z","shell.execute_reply.started":"2025-04-13T14:09:08.432992Z","shell.execute_reply":"2025-04-13T14:09:08.662772Z"},"trusted":true},"outputs":[{"name":"stdout","text":"(64, 43, 512)\n","output_type":"stream"}],"execution_count":149},{"cell_type":"code","source":"class DecoderLayer(tf.keras.layers.Layer):\n    def __init__(self, d_model, num_heads, dff, rate=0.1):\n        super(DecoderLayer, self).__init__()\n\n        self.mha1 = MultiHeadAttention(d_model, num_heads)\n        self.mha2 = MultiHeadAttention(d_model, num_heads)\n\n        self.ffn = point_wise_feed_forward_network(d_model, dff)\n\n        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n\n        self.dropout1 = tf.keras.layers.Dropout(rate)\n        self.dropout2 = tf.keras.layers.Dropout(rate)\n        self.dropout3 = tf.keras.layers.Dropout(rate)\n\n    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n        # enc_output.shape == (batch_size, input_seq_len, d_model)\n\n        # Corrected to use keyword arguments\n        attn1, attn_weights_block1 = self.mha1(q=x, k=x, v=x, mask=look_ahead_mask)  # (batch_size, target_seq_len, d_model)\n        attn1 = self.dropout1(attn1, training=training)\n        out1 = self.layernorm1(attn1 + x)\n\n        # Corrected to use keyword arguments\n        attn2, attn_weights_block2 = self.mha2(\n            q=out1, k=enc_output, v=enc_output, mask=padding_mask\n        )  # (batch_size, target_seq_len, d_model)\n        attn2 = self.dropout2(attn2, training=training)\n        out2 = self.layernorm2(attn2 + out1)  # (batch_size, target_seq_len, d_model)\n\n        ffn_output = self.ffn(out2)  # (batch_size, target_seq_len, d_model)\n        ffn_output = self.dropout3(ffn_output, training=training)\n        out3 = self.layernorm3(ffn_output + out2)  # (batch_size, target_seq_len, d_model)\n\n        return out3, attn_weights_block1, attn_weights_block2\n","metadata":{"execution":{"iopub.status.busy":"2025-04-13T14:09:08.664078Z","iopub.execute_input":"2025-04-13T14:09:08.664349Z","iopub.status.idle":"2025-04-13T14:09:08.671488Z","shell.execute_reply.started":"2025-04-13T14:09:08.664330Z","shell.execute_reply":"2025-04-13T14:09:08.670722Z"},"trusted":true},"outputs":[],"execution_count":150},{"cell_type":"code","source":"sample_decoder_layer = DecoderLayer(d_model=512, num_heads=8, dff=2048)\n\n# Call the decoder layer\nsample_decoder_layer_output, _, _ = sample_decoder_layer(\n    x=tf.random.uniform((64, 50, 512)),  # Target sequence\n    enc_output=sample_encoder_layer_output,  # Encoder output\n    training=False,\n    look_ahead_mask=None,\n    padding_mask=None\n)\n\nprint(sample_decoder_layer_output.shape)  # Expected: (64, 50, 512)\n","metadata":{"execution":{"iopub.status.busy":"2025-04-13T14:09:08.672199Z","iopub.execute_input":"2025-04-13T14:09:08.672429Z","iopub.status.idle":"2025-04-13T14:09:09.086840Z","shell.execute_reply.started":"2025-04-13T14:09:08.672405Z","shell.execute_reply":"2025-04-13T14:09:09.086204Z"},"trusted":true},"outputs":[{"name":"stdout","text":"(64, 50, 512)\n","output_type":"stream"}],"execution_count":151},{"cell_type":"code","source":"class AlternateEncoder(tf.keras.layers.Layer):\n    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,\n                 maximum_position_encoding, rate=0.1):\n        super(AlternateEncoder, self).__init__()\n\n        # Storing parameters\n        self.d_model = d_model\n        self.num_layers = num_layers\n\n        # Layers\n        self.embedding_layer = tf.keras.layers.Embedding(input_vocab_size, d_model)\n        self.positional_encoding = positional_encoding(maximum_position_encoding, d_model)\n        self.encoder_layers = [EncoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)]\n        self.dropout_layer = tf.keras.layers.Dropout(rate)\n\n    def call(self, x, training, mask):\n        # Determine the sequence length from the input\n        seq_len = tf.shape(x)[1]\n\n        # Embedding and scaling\n        embedded_input = self.embedding_layer(x)  # (batch_size, input_seq_len, d_model)\n        embedded_input *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n\n        # Add positional encoding\n        positional_encoded_input = embedded_input + self.positional_encoding[:, :seq_len, :]\n\n        # Apply dropout\n        output = self.dropout_layer(positional_encoded_input, training=training)\n\n        # Pass through each encoder layer\n        for layer in self.encoder_layers:\n            output = layer(output, training=training, mask=mask)\n\n        return output  # (batch_size, input_seq_len, d_model)\n","metadata":{"execution":{"iopub.status.busy":"2025-04-13T14:09:09.089634Z","iopub.execute_input":"2025-04-13T14:09:09.089842Z","iopub.status.idle":"2025-04-13T14:09:09.096075Z","shell.execute_reply.started":"2025-04-13T14:09:09.089828Z","shell.execute_reply":"2025-04-13T14:09:09.095480Z"},"trusted":true},"outputs":[],"execution_count":152},{"cell_type":"code","source":"alternate_encoder = AlternateEncoder(\n    num_layers=2, \n    d_model=512, \n    num_heads=8, \n    dff=2048, \n    input_vocab_size=8500, \n    maximum_position_encoding=10000\n)\n\n# Input tensor\ntemp_input = tf.random.uniform((64, 62), dtype=tf.int64, minval=0, maxval=200)\n\n# Encoder output\nalternate_encoder_output = alternate_encoder(temp_input, training=False, mask=None)\n\nprint(alternate_encoder_output.shape)  # Expected output: (64, 62, 512)\n","metadata":{"execution":{"iopub.status.busy":"2025-04-13T14:09:09.096934Z","iopub.execute_input":"2025-04-13T14:09:09.097178Z","iopub.status.idle":"2025-04-13T14:09:09.935626Z","shell.execute_reply.started":"2025-04-13T14:09:09.097157Z","shell.execute_reply":"2025-04-13T14:09:09.934877Z"},"trusted":true},"outputs":[{"name":"stdout","text":"(64, 62, 512)\n","output_type":"stream"}],"execution_count":153},{"cell_type":"code","source":"class AlternateDecoder(tf.keras.layers.Layer):\n    def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size,\n                 maximum_position_encoding, rate=0.1):\n        super(AlternateDecoder, self).__init__()\n\n        self.d_model = d_model\n        self.num_layers = num_layers\n\n        self.embedding_layer = tf.keras.layers.Embedding(target_vocab_size, d_model)\n        self.positional_encoding = positional_encoding(maximum_position_encoding, d_model)\n        self.decoder_layers = [DecoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)]\n        self.dropout_layer = tf.keras.layers.Dropout(rate)\n\n    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n        seq_len = tf.shape(x)[1]\n\n        embedded_input = self.embedding_layer(x)  # (batch_size, target_seq_len, d_model)\n        embedded_input *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n\n        positional_encoded_input = embedded_input + self.positional_encoding[:, :seq_len, :]\n\n        output = self.dropout_layer(positional_encoded_input, training=training)\n\n        attention_weights = {}\n        for i, layer in enumerate(self.decoder_layers):\n\n            output, block1, block2 = layer(\n                x=output,  # Target sequence\n                enc_output=enc_output,  # Encoder output\n                training=training,\n                look_ahead_mask=look_ahead_mask,\n                padding_mask=padding_mask\n            )\n\n            attention_weights[f\"decoder_layer{i+1}_block1\"] = block1\n            attention_weights[f\"decoder_layer{i+1}_block2\"] = block2\n\n        return output, attention_weights  # (batch_size, target_seq_len, d_model), attention weights\n\n\nalternate_decoder = AlternateDecoder(\n    num_layers=2,\n    d_model=512,\n    num_heads=8,\n    dff=2048,\n    target_vocab_size=8000,\n    maximum_position_encoding=5000\n)\n\n\ntemp_target = tf.random.uniform((64, 50), dtype=tf.int64, minval=0, maxval=8000)\n\n\nalternate_decoder_output, attention_weights = alternate_decoder(\n    x=temp_target,\n    enc_output=alternate_encoder_output,\n    training=False,\n    look_ahead_mask=None,\n    padding_mask=None\n)\n\nprint(alternate_decoder_output.shape) \n","metadata":{"execution":{"iopub.status.busy":"2025-04-13T14:09:09.936431Z","iopub.execute_input":"2025-04-13T14:09:09.936813Z","iopub.status.idle":"2025-04-13T14:09:11.202099Z","shell.execute_reply.started":"2025-04-13T14:09:09.936782Z","shell.execute_reply":"2025-04-13T14:09:11.201398Z"},"trusted":true},"outputs":[{"name":"stdout","text":"(64, 50, 512)\n","output_type":"stream"}],"execution_count":154},{"cell_type":"code","source":"class Transformer(tf.keras.Model):\n    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,\n                 target_vocab_size, pe_input, pe_target, rate=0.1):\n        super(Transformer, self).__init__()\n\n        # Replacing with AlternateEncoder and AlternateDecoder\n        self.tokenizer = AlternateEncoder(num_layers, d_model, num_heads, dff,\n                                          input_vocab_size, pe_input, rate)\n        \n        self.decoder = AlternateDecoder(num_layers, d_model, num_heads, dff,\n                                        target_vocab_size, pe_target, rate)\n\n        # Final dense layer for output\n        self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n\n    def call(self, inp, tar, training, enc_padding_mask,\n             look_ahead_mask, dec_padding_mask):\n        # Pass input through the encoder (AlternateEncoder)\n        enc_output = self.tokenizer(inp, training=training, mask=enc_padding_mask)  # (batch_size, inp_seq_len, d_model)\n\n        # Pass target and encoder output through the decoder (AlternateDecoder)\n        dec_output, attention_weights = self.decoder(\n            x=tar, enc_output=enc_output, training=training, \n            look_ahead_mask=look_ahead_mask, padding_mask=dec_padding_mask)\n\n        # Apply the final dense layer for output prediction\n        final_output = self.final_layer(dec_output)  # (batch_size, tar_seq_len, target_vocab_size)\n\n        return final_output, attention_weights\n\n# Instantiate the Transformer model with updated encoder and decoder\nsample_transformer = Transformer(\n    num_layers=2, d_model=512, num_heads=8, dff=2048,\n    input_vocab_size=8500, target_vocab_size=8000,\n    pe_input=10000, pe_target=6000\n)\n\n# Sample input and target sequences\ntemp_input = tf.random.uniform((64, 38), dtype=tf.int64, minval=0, maxval=200)\ntemp_target = tf.random.uniform((64, 36), dtype=tf.int64, minval=0, maxval=200)\n\n# Perform the forward pass\nfn_out, _ = sample_transformer(temp_input, temp_target, training=False,\n                               enc_padding_mask=None,\n                               look_ahead_mask=None,\n                               dec_padding_mask=None)\n\nprint(fn_out.shape)  # Expected: (64, 36, 8000)\n","metadata":{"execution":{"iopub.status.busy":"2025-04-13T14:09:11.202886Z","iopub.execute_input":"2025-04-13T14:09:11.203237Z","iopub.status.idle":"2025-04-13T14:09:13.933252Z","shell.execute_reply.started":"2025-04-13T14:09:11.203215Z","shell.execute_reply":"2025-04-13T14:09:13.932650Z"},"trusted":true},"outputs":[{"name":"stdout","text":"(64, 36, 8000)\n","output_type":"stream"}],"execution_count":155},{"cell_type":"code","source":"\nfrom tensorflow.keras.preprocessing.text import Tokenizer\n\n# Urdu tokenizer using Keras Tokenizer (not BERT)\nurdu_tokenizer = Tokenizer(num_words=8000, oov_token=\"<OOV>\")\nurdu_tokenizer.fit_on_texts(data['urdu'])\ndata['urdu_tokens'] = urdu_tokenizer.texts_to_sequences(data['urdu'])\n","metadata":{"execution":{"iopub.status.busy":"2025-04-13T14:09:13.933917Z","iopub.execute_input":"2025-04-13T14:09:13.934128Z","iopub.status.idle":"2025-04-13T14:09:14.398134Z","shell.execute_reply.started":"2025-04-13T14:09:13.934108Z","shell.execute_reply":"2025-04-13T14:09:14.397627Z"},"trusted":true},"outputs":[],"execution_count":156},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\n# Define the loss function and optimizer\nloss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n\ndef loss_function(real, pred):\n    # Mask out padding tokens and calculate loss\n    mask = tf.math.logical_not(tf.equal(real, 0))  # Pad tokens have value 0\n    loss_ = loss_object(real, pred)\n    mask = tf.cast(mask, dtype=loss_.dtype)\n    loss_ *= mask\n    return tf.reduce_sum(loss_) / tf.reduce_sum(mask)  # Average loss per non-padded token\n\n# Define the optimizer\noptimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)\n","metadata":{"execution":{"iopub.status.busy":"2025-04-13T14:09:14.398851Z","iopub.execute_input":"2025-04-13T14:09:14.399105Z","iopub.status.idle":"2025-04-13T14:09:14.406251Z","shell.execute_reply.started":"2025-04-13T14:09:14.399083Z","shell.execute_reply":"2025-04-13T14:09:14.405756Z"},"trusted":true},"outputs":[],"execution_count":157},{"cell_type":"code","source":"# Define the training step function\n@tf.function\ndef train_step(inp, tar, transformer, enc_padding_mask, look_ahead_mask, dec_padding_mask):\n    with tf.GradientTape() as tape:\n        # Forward pass through the transformer model\n        predictions, _ = transformer(inp, tar, training=True,\n                                     enc_padding_mask=enc_padding_mask,\n                                     look_ahead_mask=look_ahead_mask,\n                                     dec_padding_mask=dec_padding_mask)\n        # Calculate the loss\n        loss = loss_function(tar, predictions)\n\n    # Compute gradients and apply them\n    gradients = tape.gradient(loss, transformer.trainable_variables)\n    # Apply gradient clipping\n    clipped_gradients = [tf.clip_by_norm(g, 1.0) for g in gradients]\n    optimizer.apply_gradients(zip(clipped_gradients, transformer.trainable_variables))\n\n\n    return loss\n\n","metadata":{"execution":{"iopub.status.busy":"2025-04-13T14:09:14.407044Z","iopub.execute_input":"2025-04-13T14:09:14.407565Z","iopub.status.idle":"2025-04-13T14:09:14.420215Z","shell.execute_reply.started":"2025-04-13T14:09:14.407514Z","shell.execute_reply":"2025-04-13T14:09:14.419570Z"},"trusted":true},"outputs":[],"execution_count":158},{"cell_type":"code","source":"def create_masks(inp, tar):\n    # Padding mask for the encoder (enc_padding_mask)\n    enc_padding_mask = tf.cast(tf.math.equal(inp, 0), tf.float32)  # Mask padding tokens in the input\n    # Padding mask for the decoder (dec_padding_mask)\n    dec_padding_mask = tf.cast(tf.math.equal(inp, 0), tf.float32)  # Mask padding tokens in the encoder output\n    \n    # Look-ahead mask for the decoder (look_ahead_mask)\n    look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((tf.shape(tar)[1], tf.shape(tar)[1])), -1, 0)  # Prevent peeking\n\n    # Expand the look-ahead mask to match attention head dimensions\n    look_ahead_mask = tf.expand_dims(look_ahead_mask, axis=0)  # (1, seq_len, seq_len)\n    look_ahead_mask = tf.expand_dims(look_ahead_mask, axis=1)  # (1, 1, seq_len, seq_len)\n    look_ahead_mask = tf.tile(look_ahead_mask, [tf.shape(inp)[0], 8, 1, 1])  # (batch_size, num_heads, seq_len, seq_len)\n\n    # Ensure proper shapes for the masks\n    enc_padding_mask = tf.expand_dims(enc_padding_mask, axis=1)  # (batch_size, 1, seq_len)\n    enc_padding_mask = tf.expand_dims(enc_padding_mask, axis=2)  # (batch_size, 1, 1, seq_len)\n    dec_padding_mask = tf.expand_dims(dec_padding_mask, axis=1)  # (batch_size, 1, seq_len)\n    dec_padding_mask = tf.expand_dims(dec_padding_mask, axis=2)  # (batch_size, 1, 1, seq_len)\n\n    return enc_padding_mask, look_ahead_mask, dec_padding_mask\n\n\n# Prepare the dataset\nBATCH_SIZE = 64\nMAX_LENGTH = 50\n\nen_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\ndata['english_tokens'] = data['english'].apply(lambda x: en_tokenizer.encode(x, add_special_tokens=True))\n\nen_padded = pad_sequences(data[\"english_tokens\"], maxlen=MAX_LENGTH, padding='post', truncating='post')\nur_padded = pad_sequences(data[\"urdu_tokens\"], maxlen=MAX_LENGTH, padding='post', truncating='post')\n\ntrain_examples = tf.data.Dataset.from_tensor_slices((en_padded, ur_padded))\n","metadata":{"execution":{"iopub.status.busy":"2025-04-13T14:09:14.420964Z","iopub.execute_input":"2025-04-13T14:09:14.421192Z","iopub.status.idle":"2025-04-13T14:09:19.323910Z","shell.execute_reply.started":"2025-04-13T14:09:14.421170Z","shell.execute_reply":"2025-04-13T14:09:19.323150Z"},"trusted":true},"outputs":[],"execution_count":159},{"cell_type":"code","source":"import tensorflow as tf\n\n# === 1. Define loss and optimizer BEFORE training starts ===\nloss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n\ndef loss_function(real, pred):\n    mask = tf.math.logical_not(tf.equal(real, 0))\n    loss_ = loss_object(real, pred)\n    mask = tf.cast(mask, dtype=loss_.dtype)\n    loss_ *= mask\n    return tf.reduce_sum(loss_) / tf.reduce_sum(mask)\n\noptimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)\n\n@tf.function\ndef train_step(inp, tar, transformer, enc_padding_mask, look_ahead_mask, dec_padding_mask):\n    with tf.GradientTape() as tape:\n        predictions, _ = transformer(inp, tar, training=True,\n                                     enc_padding_mask=enc_padding_mask,\n                                     look_ahead_mask=look_ahead_mask,\n                                     dec_padding_mask=dec_padding_mask)\n        loss = loss_function(tar, predictions)\n    gradients = tape.gradient(loss, transformer.trainable_variables)\n    # Apply gradient clipping\n    clipped_gradients = [tf.clip_by_norm(g, 1.0) for g in gradients]\n    optimizer.apply_gradients(zip(clipped_gradients, transformer.trainable_variables))\n\n    return loss\n\n# === 2. Dataset prep ===\ndef make_batches(ds):\n    return ds.shuffle(buffer_size=1000).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n\ntrain_batches = make_batches(train_examples)\n\nBATCH_SIZE = 8                 # Start small to avoid OOM\nMAX_LENGTH = 50\nEPOCHS = 10\n\ntransformer = Transformer(\n    num_layers=2,\n    d_model=512,\n    num_heads=8,\n    dff=2048,\n    input_vocab_size=30522,\n    target_vocab_size=8000,  # or 119547 if GPU allows\n    pe_input=MAX_LENGTH,\n    pe_target=MAX_LENGTH\n)\n\n# === 4. Training loop ===\nepochs = 10\nfor epoch in range(epochs):\n    print(f\"Epoch {epoch + 1}/{epochs}\")\n    total_loss = 0\n\n    for step, (inp, tar) in enumerate(train_batches):\n        enc_padding_mask, look_ahead_mask, dec_padding_mask = create_masks(inp, tar)\n        batch_loss = train_step(inp, tar, transformer, enc_padding_mask, look_ahead_mask, dec_padding_mask)\n        total_loss += batch_loss\n\n        if step % 10 == 0:\n            print(f\"Step {step}: Loss = {batch_loss.numpy()}\")\n\n    avg_loss = total_loss / (step + 1)\n    print(f\"Epoch {epoch + 1}: Average Loss = {avg_loss.numpy()}\")\n    transformer.save_weights(f\"/kaggle/working/transformer_epoch_{epoch + 1}.weights.h5\")\n    print(f\"Model weights saved at epoch {epoch + 1}\")\n","metadata":{"execution":{"iopub.status.busy":"2025-04-13T14:09:19.324811Z","iopub.execute_input":"2025-04-13T14:09:19.325036Z","iopub.status.idle":"2025-04-13T14:27:27.771340Z","shell.execute_reply.started":"2025-04-13T14:09:19.325018Z","shell.execute_reply":"2025-04-13T14:27:27.770577Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Epoch 1/10\nStep 0: Loss = 9.051582336425781\nStep 10: Loss = 8.051912307739258\nStep 20: Loss = 7.447792053222656\nStep 30: Loss = 6.8374552726745605\nStep 40: Loss = 6.108687400817871\nStep 50: Loss = 5.553360939025879\nStep 60: Loss = 5.107004165649414\nStep 70: Loss = 4.74232816696167\nStep 80: Loss = 4.476678848266602\nStep 90: Loss = 4.417648792266846\nStep 100: Loss = 3.7124617099761963\nStep 110: Loss = 3.4681453704833984\nStep 120: Loss = 3.2324507236480713\nStep 130: Loss = 3.404010057449341\nStep 140: Loss = 2.7731235027313232\nStep 150: Loss = 2.587160110473633\nStep 160: Loss = 2.7680695056915283\nStep 170: Loss = 2.639176607131958\nStep 180: Loss = 2.5013668537139893\nStep 190: Loss = 2.493377685546875\nStep 200: Loss = 2.481670379638672\nStep 210: Loss = 2.3139028549194336\nStep 220: Loss = 2.3963515758514404\nStep 230: Loss = 2.337888240814209\nStep 240: Loss = 1.9925860166549683\nStep 250: Loss = 2.0741701126098633\nStep 260: Loss = 1.9001023769378662\nStep 270: Loss = 1.9104459285736084\nStep 280: Loss = 1.8409045934677124\nStep 290: Loss = 1.8420833349227905\nStep 300: Loss = 1.9900250434875488\nStep 310: Loss = 1.8456920385360718\nStep 320: Loss = 1.9532684087753296\nStep 330: Loss = 1.881492257118225\nStep 340: Loss = 1.7001549005508423\nStep 350: Loss = 1.697200894355774\nStep 360: Loss = 1.7239700555801392\nStep 370: Loss = 2.6928937435150146\nStep 380: Loss = 4.408196926116943\nStep 390: Loss = 5.307235240936279\nStep 400: Loss = 4.224814414978027\nStep 410: Loss = 3.8811612129211426\nStep 420: Loss = 3.340874195098877\nStep 430: Loss = 2.9932329654693604\nStep 440: Loss = 2.7477712631225586\nStep 450: Loss = 2.4830026626586914\nEpoch 1: Average Loss = 3.3079049587249756\nModel weights saved at epoch 1\nEpoch 2/10\nStep 0: Loss = 1.9094289541244507\nStep 10: Loss = 1.5460293292999268\nStep 20: Loss = 1.4575819969177246\nStep 30: Loss = 1.2987771034240723\nStep 40: Loss = 1.432944893836975\nStep 50: Loss = 1.0896672010421753\nStep 60: Loss = 1.3066519498825073\nStep 70: Loss = 1.3585522174835205\nStep 80: Loss = 1.1682482957839966\nStep 90: Loss = 1.2100756168365479\nStep 100: Loss = 1.2138457298278809\nStep 110: Loss = 0.9278699159622192\nStep 120: Loss = 1.2875343561172485\nStep 130: Loss = 1.079934000968933\nStep 140: Loss = 1.1661709547042847\nStep 150: Loss = 1.0759397745132446\nStep 160: Loss = 1.0949376821517944\nStep 170: Loss = 0.9655590653419495\nStep 180: Loss = 1.1346487998962402\nStep 190: Loss = 0.9605711102485657\nStep 200: Loss = 0.9585679173469543\nStep 210: Loss = 1.0199594497680664\nStep 220: Loss = 0.7423892021179199\nStep 230: Loss = 1.0945686101913452\nStep 240: Loss = 0.9611010551452637\nStep 250: Loss = 0.9954593777656555\nStep 260: Loss = 0.83933025598526\nStep 270: Loss = 1.0454951524734497\nStep 280: Loss = 1.0185922384262085\nStep 290: Loss = 0.9447576999664307\nStep 300: Loss = 0.7654047608375549\nStep 310: Loss = 0.9077469706535339\nStep 320: Loss = 0.613999605178833\nStep 330: Loss = 0.6675058603286743\nStep 340: Loss = 0.6835533976554871\nStep 350: Loss = 0.8035678863525391\nStep 360: Loss = 0.8293284177780151\nStep 370: Loss = 1.2630293369293213\nStep 380: Loss = 1.3831743001937866\nStep 390: Loss = 1.826954960823059\nStep 400: Loss = 1.824449896812439\nStep 410: Loss = 1.9393398761749268\nStep 420: Loss = 1.6077544689178467\nStep 430: Loss = 1.4424350261688232\nStep 440: Loss = 1.1991676092147827\nStep 450: Loss = 1.219551920890808\nEpoch 2: Average Loss = 1.1655634641647339\nModel weights saved at epoch 2\nEpoch 3/10\nStep 0: Loss = 0.8564674258232117\nStep 10: Loss = 0.7435275912284851\nStep 20: Loss = 0.9524842500686646\nStep 30: Loss = 0.5335482358932495\nStep 40: Loss = 0.5825789570808411\nStep 50: Loss = 0.8411376476287842\nStep 60: Loss = 0.5869505405426025\nStep 70: Loss = 0.9151272177696228\nStep 80: Loss = 0.6695774793624878\nStep 90: Loss = 0.6010798811912537\nStep 100: Loss = 0.6487627029418945\nStep 110: Loss = 0.6355358362197876\nStep 120: Loss = 0.5489137768745422\nStep 130: Loss = 0.5578815340995789\nStep 140: Loss = 0.6431444883346558\nStep 150: Loss = 0.5135912299156189\nStep 160: Loss = 0.5735642910003662\nStep 170: Loss = 0.6045374870300293\nStep 180: Loss = 0.6277700662612915\nStep 190: Loss = 0.39890745282173157\nStep 200: Loss = 0.6908662915229797\nStep 210: Loss = 0.5739419460296631\nStep 220: Loss = 0.6051170229911804\nStep 230: Loss = 0.5219296813011169\nStep 240: Loss = 0.5431844592094421\nStep 250: Loss = 0.44153913855552673\nStep 260: Loss = 0.42845067381858826\nStep 270: Loss = 0.6213980913162231\nStep 280: Loss = 0.36793097853660583\nStep 290: Loss = 0.47685563564300537\nStep 300: Loss = 0.3950749635696411\nStep 310: Loss = 0.44930747151374817\nStep 320: Loss = 0.4490448534488678\nStep 330: Loss = 0.40362006425857544\nStep 340: Loss = 0.5419781804084778\nStep 350: Loss = 0.44026049971580505\nStep 360: Loss = 0.41708555817604065\nStep 370: Loss = 0.7387783527374268\nStep 380: Loss = 1.017716407775879\nStep 390: Loss = 1.1986061334609985\nStep 400: Loss = 1.1090863943099976\nStep 410: Loss = 0.9897323846817017\nStep 420: Loss = 1.1358546018600464\nStep 430: Loss = 1.1159347295761108\nStep 440: Loss = 0.5362727642059326\nStep 450: Loss = 0.7150622010231018\nEpoch 3: Average Loss = 0.6638608574867249\nModel weights saved at epoch 3\nEpoch 4/10\nStep 0: Loss = 0.4367697834968567\nStep 10: Loss = 0.5050511360168457\nStep 20: Loss = 0.5041399598121643\nStep 30: Loss = 0.4230552613735199\nStep 40: Loss = 0.4717831611633301\nStep 50: Loss = 0.4008960425853729\nStep 60: Loss = 0.4910379946231842\nStep 70: Loss = 0.42529407143592834\nStep 80: Loss = 0.462717741727829\nStep 90: Loss = 0.6100901961326599\nStep 100: Loss = 0.4565414488315582\nStep 110: Loss = 0.38049226999282837\nStep 120: Loss = 0.41263943910598755\nStep 130: Loss = 0.32085344195365906\nStep 140: Loss = 0.3018084168434143\nStep 150: Loss = 0.2875576317310333\nStep 160: Loss = 0.3416847884654999\nStep 170: Loss = 0.3275826871395111\nStep 180: Loss = 0.2510603666305542\nStep 190: Loss = 0.491711288690567\nStep 200: Loss = 0.46960699558258057\nStep 210: Loss = 0.30222055315971375\nStep 220: Loss = 0.2709229290485382\nStep 230: Loss = 0.3305959105491638\nStep 240: Loss = 0.4133652448654175\nStep 250: Loss = 0.4350358545780182\nStep 260: Loss = 0.38379549980163574\nStep 270: Loss = 0.5176617503166199\nStep 280: Loss = 0.3863045275211334\nStep 290: Loss = 0.22013512253761292\nStep 300: Loss = 0.2583276331424713\nStep 310: Loss = 0.33378735184669495\nStep 320: Loss = 0.3271278142929077\nStep 330: Loss = 0.21179845929145813\nStep 340: Loss = 0.2858903110027313\nStep 350: Loss = 0.21131648123264313\nStep 360: Loss = 0.36281561851501465\nStep 370: Loss = 0.4021504521369934\nStep 380: Loss = 0.6922841668128967\nStep 390: Loss = 0.5930315852165222\nStep 400: Loss = 0.7866783142089844\nStep 410: Loss = 0.9030821323394775\nStep 420: Loss = 0.6356331706047058\nStep 430: Loss = 0.6308724880218506\nStep 440: Loss = 0.4253498613834381\nStep 450: Loss = 0.46820583939552307\nEpoch 4: Average Loss = 0.42569026350975037\nModel weights saved at epoch 4\nEpoch 5/10\nStep 0: Loss = 0.26891177892684937\nStep 10: Loss = 0.186800017952919\nStep 20: Loss = 0.21168361604213715\nStep 30: Loss = 0.36028072237968445\nStep 40: Loss = 0.15873344242572784\nStep 50: Loss = 0.2817189693450928\nStep 60: Loss = 0.47670605778694153\nStep 70: Loss = 0.2959504723548889\nStep 80: Loss = 0.4192017912864685\nStep 90: Loss = 0.3642529547214508\nStep 100: Loss = 0.3455772399902344\nStep 110: Loss = 0.22475802898406982\nStep 120: Loss = 0.2791007161140442\nStep 130: Loss = 0.28748857975006104\nStep 140: Loss = 0.29796725511550903\nStep 150: Loss = 0.15868143737316132\nStep 160: Loss = 0.1672665923833847\nStep 170: Loss = 0.21479271352291107\nStep 180: Loss = 0.20865432918071747\nStep 190: Loss = 0.2152683585882187\nStep 200: Loss = 0.3614249527454376\nStep 210: Loss = 0.2080523818731308\nStep 220: Loss = 0.20651832222938538\nStep 230: Loss = 0.1880970001220703\nStep 240: Loss = 0.23539677262306213\nStep 250: Loss = 0.35034677386283875\nStep 260: Loss = 0.22160643339157104\nStep 270: Loss = 0.37040871381759644\nStep 280: Loss = 0.2946164011955261\nStep 290: Loss = 0.23813755810260773\nStep 300: Loss = 0.12707611918449402\nStep 310: Loss = 0.17395907640457153\nStep 320: Loss = 0.1001385971903801\nStep 330: Loss = 0.19862769544124603\nStep 340: Loss = 0.18464793264865875\nStep 350: Loss = 0.22445248067378998\nStep 360: Loss = 0.24354858696460724\nStep 370: Loss = 0.24870778620243073\nStep 380: Loss = 0.19045230746269226\nStep 390: Loss = 0.6364860534667969\nStep 400: Loss = 0.6095948815345764\nStep 410: Loss = 0.5669081807136536\nStep 420: Loss = 0.37216758728027344\nStep 430: Loss = 0.37462103366851807\nStep 440: Loss = 0.3060861825942993\nStep 450: Loss = 0.21139293909072876\nEpoch 5: Average Loss = 0.2880508303642273\nModel weights saved at epoch 5\nEpoch 6/10\nStep 0: Loss = 0.20127113163471222\nStep 10: Loss = 0.20562317967414856\nStep 20: Loss = 0.17500561475753784\nStep 30: Loss = 0.11452652513980865\nStep 40: Loss = 0.2271481454372406\nStep 50: Loss = 0.20437707006931305\nStep 60: Loss = 0.17008717358112335\nStep 70: Loss = 0.28160080313682556\nStep 80: Loss = 0.17725397646427155\nStep 90: Loss = 0.29829099774360657\nStep 100: Loss = 0.2541799247264862\nStep 110: Loss = 0.2279178351163864\nStep 120: Loss = 0.18662258982658386\nStep 130: Loss = 0.14050236344337463\nStep 140: Loss = 0.1647796481847763\nStep 150: Loss = 0.12155324965715408\nStep 160: Loss = 0.15808561444282532\nStep 170: Loss = 0.14952872693538666\nStep 180: Loss = 0.17347128689289093\nStep 190: Loss = 0.11792204529047012\nStep 200: Loss = 0.17072953283786774\nStep 210: Loss = 0.17034092545509338\nStep 220: Loss = 0.19175824522972107\nStep 230: Loss = 0.10772719979286194\nStep 240: Loss = 0.17710761725902557\nStep 250: Loss = 0.21759958565235138\nStep 260: Loss = 0.15263625979423523\nStep 270: Loss = 0.20074954628944397\nStep 280: Loss = 0.1884232610464096\nStep 290: Loss = 0.14122414588928223\nStep 300: Loss = 0.10805267095565796\nStep 310: Loss = 0.1371767520904541\nStep 320: Loss = 0.1870897263288498\nStep 330: Loss = 0.1617226004600525\nStep 340: Loss = 0.21188507974147797\nStep 350: Loss = 0.1635194569826126\nStep 360: Loss = 0.12278658151626587\nStep 370: Loss = 0.26897627115249634\nStep 380: Loss = 0.4526841938495636\nStep 390: Loss = 0.37011611461639404\nStep 400: Loss = 0.4437064528465271\nStep 410: Loss = 0.45842698216438293\nStep 420: Loss = 0.24827511608600616\nStep 430: Loss = 0.36949434876441956\nStep 440: Loss = 0.1372629702091217\nStep 450: Loss = 0.11945273727178574\nEpoch 6: Average Loss = 0.19950585067272186\nModel weights saved at epoch 6\nEpoch 7/10\nStep 0: Loss = 0.11335374414920807\nStep 10: Loss = 0.16345731914043427\nStep 20: Loss = 0.07410028576850891\nStep 30: Loss = 0.17107008397579193\nStep 40: Loss = 0.19169840216636658\nStep 50: Loss = 0.15844735503196716\nStep 60: Loss = 0.1680835783481598\nStep 70: Loss = 0.12735730409622192\nStep 80: Loss = 0.1966131627559662\nStep 90: Loss = 0.1285875141620636\nStep 100: Loss = 0.11616194993257523\nStep 110: Loss = 0.11979500204324722\nStep 120: Loss = 0.1169457957148552\nStep 130: Loss = 0.12211216986179352\nStep 140: Loss = 0.1250324547290802\nStep 150: Loss = 0.10757745057344437\nStep 160: Loss = 0.17202433943748474\nStep 170: Loss = 0.18922026455402374\nStep 180: Loss = 0.1170915961265564\nStep 190: Loss = 0.18571844696998596\nStep 200: Loss = 0.0757407546043396\nStep 210: Loss = 0.13855145871639252\nStep 220: Loss = 0.07155697047710419\nStep 230: Loss = 0.10965991020202637\nStep 240: Loss = 0.12730711698532104\nStep 250: Loss = 0.08180035650730133\nStep 260: Loss = 0.15978781878948212\nStep 270: Loss = 0.12124036252498627\nStep 280: Loss = 0.1171141043305397\nStep 290: Loss = 0.12852241098880768\nStep 300: Loss = 0.12659227848052979\nStep 310: Loss = 0.10318015515804291\nStep 320: Loss = 0.0952521413564682\nStep 330: Loss = 0.20490877330303192\nStep 340: Loss = 0.07260720431804657\nStep 350: Loss = 0.14640313386917114\nStep 360: Loss = 0.1416071653366089\nStep 370: Loss = 0.13315801322460175\nStep 380: Loss = 0.15214009582996368\nStep 390: Loss = 0.2555232048034668\nStep 400: Loss = 0.2771276533603668\nStep 410: Loss = 0.25151610374450684\nStep 420: Loss = 0.33049556612968445\nStep 430: Loss = 0.10433024913072586\nStep 440: Loss = 0.1026335135102272\nStep 450: Loss = 0.09572002291679382\nEpoch 7: Average Loss = 0.13900473713874817\nModel weights saved at epoch 7\nEpoch 8/10\nStep 0: Loss = 0.14950132369995117\nStep 10: Loss = 0.08128608763217926\nStep 20: Loss = 0.060420598834753036\nStep 30: Loss = 0.06207401677966118\nStep 40: Loss = 0.10974115133285522\nStep 50: Loss = 0.062480390071868896\nStep 60: Loss = 0.1041940227150917\nStep 70: Loss = 0.0713934451341629\nStep 80: Loss = 0.08909815549850464\nStep 90: Loss = 0.07517827302217484\nStep 100: Loss = 0.08609560877084732\nStep 110: Loss = 0.06678729504346848\nStep 120: Loss = 0.13108545541763306\nStep 130: Loss = 0.08211857080459595\nStep 140: Loss = 0.08726247400045395\nStep 150: Loss = 0.07770952582359314\nStep 160: Loss = 0.10088922828435898\nStep 170: Loss = 0.09042885154485703\nStep 180: Loss = 0.09144849330186844\nStep 190: Loss = 0.10805783420801163\nStep 200: Loss = 0.06965077668428421\nStep 210: Loss = 0.0394488163292408\nStep 220: Loss = 0.11635366827249527\nStep 230: Loss = 0.12147140502929688\nStep 240: Loss = 0.0754457637667656\nStep 250: Loss = 0.04532691836357117\nStep 260: Loss = 0.09112926572561264\nStep 270: Loss = 0.08421722799539566\nStep 280: Loss = 0.06189987435936928\nStep 290: Loss = 0.052534837275743484\nStep 300: Loss = 0.1425018608570099\nStep 310: Loss = 0.13887426257133484\nStep 320: Loss = 0.09260206669569016\nStep 330: Loss = 0.04920244589447975\nStep 340: Loss = 0.04825714975595474\nStep 350: Loss = 0.04277941957116127\nStep 360: Loss = 0.07615327835083008\nStep 370: Loss = 0.10256591439247131\nStep 380: Loss = 0.12304207682609558\nStep 390: Loss = 0.23804359138011932\nStep 400: Loss = 0.187142014503479\nStep 410: Loss = 0.21635068953037262\nStep 420: Loss = 0.15023228526115417\nStep 430: Loss = 0.08787111192941666\nStep 440: Loss = 0.11741722375154495\nStep 450: Loss = 0.0331147275865078\nEpoch 8: Average Loss = 0.09646856784820557\nModel weights saved at epoch 8\nEpoch 9/10\nStep 0: Loss = 0.07621043920516968\nStep 10: Loss = 0.06371181458234787\nStep 20: Loss = 0.054610658437013626\nStep 30: Loss = 0.032320428639650345\nStep 40: Loss = 0.04214181751012802\nStep 50: Loss = 0.05165431648492813\nStep 60: Loss = 0.025497857481241226\nStep 70: Loss = 0.10612156242132187\nStep 80: Loss = 0.06269228458404541\nStep 90: Loss = 0.05082753300666809\nStep 100: Loss = 0.045948002487421036\nStep 110: Loss = 0.04675362631678581\nStep 120: Loss = 0.05342472717165947\nStep 130: Loss = 0.036932673305273056\nStep 140: Loss = 0.044418808072805405\nStep 150: Loss = 0.041683174669742584\nStep 160: Loss = 0.04831695929169655\nStep 170: Loss = 0.06379175931215286\nStep 180: Loss = 0.06590565294027328\nStep 190: Loss = 0.04469053074717522\nStep 200: Loss = 0.11395981162786484\nStep 210: Loss = 0.05220462381839752\nStep 220: Loss = 0.040085215121507645\nStep 230: Loss = 0.06378289312124252\nStep 240: Loss = 0.06487449258565903\nStep 250: Loss = 0.02841515652835369\nStep 260: Loss = 0.07877044379711151\nStep 270: Loss = 0.08376260101795197\nStep 280: Loss = 0.06161888688802719\nStep 290: Loss = 0.06260597705841064\nStep 300: Loss = 0.05234237015247345\nStep 310: Loss = 0.07229994237422943\nStep 320: Loss = 0.08991985768079758\nStep 330: Loss = 0.08253926783800125\nStep 340: Loss = 0.06996647268533707\nStep 350: Loss = 0.06108885630965233\nStep 360: Loss = 0.08637717366218567\nStep 370: Loss = 0.047615598887205124\nStep 380: Loss = 0.07659837603569031\nStep 390: Loss = 0.13944794237613678\nStep 400: Loss = 0.1313638985157013\nStep 410: Loss = 0.1151965782046318\nStep 420: Loss = 0.06363466382026672\nStep 430: Loss = 0.04882892221212387\nStep 440: Loss = 0.04681038111448288\nStep 450: Loss = 0.015668727457523346\nEpoch 9: Average Loss = 0.06529127061367035\nModel weights saved at epoch 9\nEpoch 10/10\nStep 0: Loss = 0.023120667785406113\nStep 10: Loss = 0.04703927040100098\nStep 20: Loss = 0.06521935015916824\nStep 30: Loss = 0.039549633860588074\nStep 40: Loss = 0.029552631080150604\nStep 50: Loss = 0.0418018214404583\nStep 60: Loss = 0.02819920890033245\nStep 70: Loss = 0.07774604856967926\nStep 80: Loss = 0.030438892543315887\nStep 90: Loss = 0.10030738264322281\nStep 100: Loss = 0.04794413223862648\nStep 110: Loss = 0.0574648454785347\nStep 120: Loss = 0.03619460016489029\nStep 130: Loss = 0.039421334862709045\nStep 140: Loss = 0.045492324978113174\nStep 150: Loss = 0.0438150018453598\nStep 160: Loss = 0.03392941877245903\nStep 170: Loss = 0.033917222172021866\nStep 180: Loss = 0.033191073685884476\nStep 190: Loss = 0.054445624351501465\nStep 200: Loss = 0.02515891194343567\nStep 210: Loss = 0.019416412338614464\nStep 220: Loss = 0.03449918329715729\nStep 230: Loss = 0.06445374339818954\nStep 240: Loss = 0.034974850714206696\nStep 250: Loss = 0.0471489243209362\nStep 260: Loss = 0.048383474349975586\nStep 270: Loss = 0.030426494777202606\nStep 280: Loss = 0.055784717202186584\nStep 290: Loss = 0.019778091460466385\nStep 300: Loss = 0.030860045924782753\nStep 310: Loss = 0.05655955523252487\nStep 320: Loss = 0.03231985867023468\nStep 330: Loss = 0.03499812260270119\nStep 340: Loss = 0.022152896970510483\nStep 350: Loss = 0.03666846454143524\nStep 360: Loss = 0.05134360492229462\nStep 370: Loss = 0.03812950477004051\nStep 380: Loss = 0.03841843083500862\nStep 390: Loss = 0.09336042404174805\nStep 400: Loss = 0.04966525360941887\nStep 410: Loss = 0.08777902275323868\nStep 420: Loss = 0.043575551360845566\nStep 430: Loss = 0.03307093307375908\nStep 440: Loss = 0.02300335094332695\nStep 450: Loss = 0.01501431968063116\nEpoch 10: Average Loss = 0.04178871214389801\nModel weights saved at epoch 10\n","output_type":"stream"}],"execution_count":160},{"cell_type":"code","source":"def loss_function(real, pred):\n    # Mask out padding tokens and calculate loss\n    mask = tf.math.logical_not(tf.equal(real, 0))  # Pad tokens have value 0\n    loss_ = loss_object(real, pred)\n    mask = tf.cast(mask, dtype=loss_.dtype)\n    loss_ *= mask\n    return tf.reduce_sum(loss_) / tf.reduce_sum(mask)  # Average loss per non-padded token\n\n# Define the optimizer\noptimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)\n\n","metadata":{"execution":{"iopub.status.busy":"2025-04-13T14:27:27.771985Z","iopub.execute_input":"2025-04-13T14:27:27.772183Z","iopub.status.idle":"2025-04-13T14:27:27.779595Z","shell.execute_reply.started":"2025-04-13T14:27:27.772168Z","shell.execute_reply":"2025-04-13T14:27:27.779040Z"},"trusted":true},"outputs":[],"execution_count":161},{"cell_type":"code","source":"@tf.function\ndef train_step(inp, tar, transformer, enc_padding_mask, look_ahead_mask, dec_padding_mask):\n    with tf.GradientTape() as tape:\n        # Forward pass through the transformer model\n        predictions, _ = transformer(inp, tar, training=True,\n                                     enc_padding_mask=enc_padding_mask,\n                                     look_ahead_mask=look_ahead_mask,\n                                     dec_padding_mask=dec_padding_mask)\n        # Calculate the loss\n        loss = loss_function(tar, predictions)\n\n    # Compute gradients and apply them\n    gradients = tape.gradient(loss, transformer.trainable_variables)\n    # Apply gradient clipping\n    clipped_gradients = [tf.clip_by_norm(g, 1.0) for g in gradients]\n    optimizer.apply_gradients(zip(clipped_gradients, transformer.trainable_variables))\n\n\n    return loss\n","metadata":{"execution":{"iopub.status.busy":"2025-04-13T14:27:27.780229Z","iopub.execute_input":"2025-04-13T14:27:27.780446Z","iopub.status.idle":"2025-04-13T14:27:27.872052Z","shell.execute_reply.started":"2025-04-13T14:27:27.780431Z","shell.execute_reply":"2025-04-13T14:27:27.871490Z"},"trusted":true},"outputs":[],"execution_count":162},{"cell_type":"code","source":"print(type(data['english_tokens'].iloc[0]))\nprint(data['english_tokens'].iloc[0])\n","metadata":{"execution":{"iopub.status.busy":"2025-04-13T14:27:27.873622Z","iopub.execute_input":"2025-04-13T14:27:27.873821Z","iopub.status.idle":"2025-04-13T14:27:27.879013Z","shell.execute_reply.started":"2025-04-13T14:27:27.873807Z","shell.execute_reply":"2025-04-13T14:27:27.878382Z"},"trusted":true},"outputs":[{"name":"stdout","text":"<class 'list'>\n[101, 2003, 23564, 2378, 2115, 7833, 102]\n","output_type":"stream"}],"execution_count":163},{"cell_type":"code","source":"def create_masks(inp, tar):\n    # Padding mask for the encoder (enc_padding_mask)\n    enc_padding_mask = tf.cast(tf.math.equal(inp, 0), tf.float32)  # Mask padding tokens in the input\n    # Padding mask for the decoder (dec_padding_mask)\n    dec_padding_mask = tf.cast(tf.math.equal(inp, 0), tf.float32)  # Mask padding tokens in the encoder output\n    \n    look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((tf.shape(tar)[1], tf.shape(tar)[1])), -1, 0)  # Prevent peeking\n\n    look_ahead_mask = tf.expand_dims(look_ahead_mask, axis=0)  # (1, seq_len, seq_len)\n    look_ahead_mask = tf.expand_dims(look_ahead_mask, axis=1)  # (1, 1, seq_len, seq_len)\n    look_ahead_mask = tf.tile(look_ahead_mask, [tf.shape(inp)[0], 8, 1, 1])  # (batch_size, num_heads, seq_len, seq_len)\n\n    # Ensure proper shapes for the masks\n    enc_padding_mask = tf.expand_dims(enc_padding_mask, axis=1)  # (batch_size, 1, seq_len)\n    enc_padding_mask = tf.expand_dims(enc_padding_mask, axis=2)  # (batch_size, 1, 1, seq_len)\n    dec_padding_mask = tf.expand_dims(dec_padding_mask, axis=1)  # (batch_size, 1, seq_len)\n    dec_padding_mask = tf.expand_dims(dec_padding_mask, axis=2)  # (batch_size, 1, 1, seq_len)\n\n    return enc_padding_mask, look_ahead_mask, dec_padding_mask\n\n\nBATCH_SIZE = 64\nMAX_LENGTH = 50\n\n\nen_padded = pad_sequences(data[\"english_tokens\"], maxlen=MAX_LENGTH, padding='post', truncating='post')\nur_padded = pad_sequences(data[\"urdu_tokens\"], maxlen=MAX_LENGTH, padding='post', truncating='post')\n\n# Create TensorFlow dataset\ntrain_examples = tf.data.Dataset.from_tensor_slices((en_padded, ur_padded))\n","metadata":{"execution":{"iopub.status.busy":"2025-04-13T14:27:27.879623Z","iopub.execute_input":"2025-04-13T14:27:27.879848Z","iopub.status.idle":"2025-04-13T14:27:28.024074Z","shell.execute_reply.started":"2025-04-13T14:27:27.879832Z","shell.execute_reply":"2025-04-13T14:27:28.023564Z"},"trusted":true},"outputs":[],"execution_count":164},{"cell_type":"code","source":"\ndef make_batches(ds):\n    return ds.shuffle(buffer_size=1000).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n\ntrain_batches = make_batches(train_examples)\n\ntransformer = Transformer(\n    num_layers=2, d_model=512, num_heads=8, dff=2048,\n    input_vocab_size=8500, target_vocab_size=8000,\n    pe_input=MAX_LENGTH, pe_target=MAX_LENGTH\n)\n\nepochs = 10\n\nfor epoch in range(epochs):\n    print(f\"Epoch {epoch + 1}/{epochs}\")\n    total_loss = 0\n\n    for step, (inp, tar) in enumerate(train_batches):\n        enc_padding_mask, look_ahead_mask, dec_padding_mask = create_masks(inp, tar)\n\n        batch_loss = train_step(inp, tar, transformer, enc_padding_mask, look_ahead_mask, dec_padding_mask)\n        total_loss += batch_loss\n\n        if step % 10 == 0:  # Log every 10 steps\n            print(f\"Step {step}: Loss = {batch_loss.numpy()}\")\n\n    avg_loss = total_loss / (step + 1)\n    print(f\"Epoch {epoch + 1}: Average Loss = {avg_loss.numpy()}\")\n\n    transformer.save_weights(f\"/kaggle/working/transformer_epoch_{epoch + 1}.weights.h5\")\n    print(f\"Model weights saved at epoch {epoch + 1}\")\n","metadata":{"execution":{"iopub.status.busy":"2025-04-13T14:27:28.025011Z","iopub.execute_input":"2025-04-13T14:27:28.025188Z","iopub.status.idle":"2025-04-13T14:45:11.294757Z","shell.execute_reply.started":"2025-04-13T14:27:28.025174Z","shell.execute_reply":"2025-04-13T14:45:11.294006Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Epoch 1/10\nStep 0: Loss = 9.031786918640137\nStep 10: Loss = 8.016155242919922\nStep 20: Loss = 7.416788101196289\nStep 30: Loss = 6.784709453582764\nStep 40: Loss = 6.0679755210876465\nStep 50: Loss = 5.559366226196289\nStep 60: Loss = 5.239116191864014\nStep 70: Loss = 4.481668472290039\nStep 80: Loss = 4.470423698425293\nStep 90: Loss = 3.763221025466919\nStep 100: Loss = 3.9041051864624023\nStep 110: Loss = 3.4101152420043945\nStep 120: Loss = 3.3777194023132324\nStep 130: Loss = 3.221353769302368\nStep 140: Loss = 2.8206980228424072\nStep 150: Loss = 2.73730731010437\nStep 160: Loss = 2.605804681777954\nStep 170: Loss = 2.6027188301086426\nStep 180: Loss = 2.0028886795043945\nStep 190: Loss = 2.733675003051758\nStep 200: Loss = 2.627987861633301\nStep 210: Loss = 2.233100175857544\nStep 220: Loss = 2.2568118572235107\nStep 230: Loss = 1.8790100812911987\nStep 240: Loss = 2.156864881515503\nStep 250: Loss = 1.984606385231018\nStep 260: Loss = 2.049262523651123\nStep 270: Loss = 2.152287006378174\nStep 280: Loss = 2.05100417137146\nStep 290: Loss = 1.612857460975647\nStep 300: Loss = 1.9502915143966675\nStep 310: Loss = 1.8624663352966309\nStep 320: Loss = 1.8112999200820923\nStep 330: Loss = 1.6379600763320923\nStep 340: Loss = 1.5165789127349854\nStep 350: Loss = 1.648918628692627\nStep 360: Loss = 1.6388379335403442\nStep 370: Loss = 3.259587049484253\nStep 380: Loss = 4.627309322357178\nStep 390: Loss = 4.405664443969727\nStep 400: Loss = 4.429011344909668\nStep 410: Loss = 3.833575963973999\nStep 420: Loss = 3.2714929580688477\nStep 430: Loss = 3.215791702270508\nStep 440: Loss = 2.7842330932617188\nStep 450: Loss = 2.2397220134735107\nEpoch 1: Average Loss = 3.2932186126708984\nModel weights saved at epoch 1\nEpoch 2/10\nStep 0: Loss = 1.9054956436157227\nStep 10: Loss = 1.7659647464752197\nStep 20: Loss = 1.352811336517334\nStep 30: Loss = 1.534293532371521\nStep 40: Loss = 1.1699681282043457\nStep 50: Loss = 1.0254229307174683\nStep 60: Loss = 1.0782078504562378\nStep 70: Loss = 1.503783941268921\nStep 80: Loss = 1.2255353927612305\nStep 90: Loss = 1.1305029392242432\nStep 100: Loss = 1.1930897235870361\nStep 110: Loss = 0.8220378756523132\nStep 120: Loss = 1.2132902145385742\nStep 130: Loss = 1.0715981721878052\nStep 140: Loss = 0.8043189644813538\nStep 150: Loss = 1.271672010421753\nStep 160: Loss = 1.0680028200149536\nStep 170: Loss = 1.110922932624817\nStep 180: Loss = 0.9944868087768555\nStep 190: Loss = 0.7400897145271301\nStep 200: Loss = 0.9406550526618958\nStep 210: Loss = 0.7230409383773804\nStep 220: Loss = 1.0164450407028198\nStep 230: Loss = 1.140715479850769\nStep 240: Loss = 0.9493010640144348\nStep 250: Loss = 0.7977743744850159\nStep 260: Loss = 0.8866846561431885\nStep 270: Loss = 0.8314870595932007\nStep 280: Loss = 0.8114996552467346\nStep 290: Loss = 0.8892462253570557\nStep 300: Loss = 0.874323844909668\nStep 310: Loss = 0.905064582824707\nStep 320: Loss = 0.8274490237236023\nStep 330: Loss = 0.7648290991783142\nStep 340: Loss = 0.7314068078994751\nStep 350: Loss = 0.891645073890686\nStep 360: Loss = 1.0189489126205444\nStep 370: Loss = 1.1998745203018188\nStep 380: Loss = 1.5159680843353271\nStep 390: Loss = 1.9465887546539307\nStep 400: Loss = 1.6564135551452637\nStep 410: Loss = 1.6189651489257812\nStep 420: Loss = 1.7835859060287476\nStep 430: Loss = 1.3503375053405762\nStep 440: Loss = 1.1352163553237915\nStep 450: Loss = 1.0836106538772583\nEpoch 2: Average Loss = 1.1656240224838257\nModel weights saved at epoch 2\nEpoch 3/10\nStep 0: Loss = 0.7375625967979431\nStep 10: Loss = 0.8792341947555542\nStep 20: Loss = 0.7973232269287109\nStep 30: Loss = 0.725680410861969\nStep 40: Loss = 0.5921860933303833\nStep 50: Loss = 0.6209593415260315\nStep 60: Loss = 0.7956768870353699\nStep 70: Loss = 0.6378999352455139\nStep 80: Loss = 0.6705772876739502\nStep 90: Loss = 0.637144923210144\nStep 100: Loss = 0.60377436876297\nStep 110: Loss = 0.6966440677642822\nStep 120: Loss = 0.7119366526603699\nStep 130: Loss = 0.5823376178741455\nStep 140: Loss = 0.5961734056472778\nStep 150: Loss = 0.578011155128479\nStep 160: Loss = 0.5814282894134521\nStep 170: Loss = 0.7802576422691345\nStep 180: Loss = 0.47931066155433655\nStep 190: Loss = 0.6202887296676636\nStep 200: Loss = 0.5075021386146545\nStep 210: Loss = 0.5104008913040161\nStep 220: Loss = 0.6116914749145508\nStep 230: Loss = 0.634660542011261\nStep 240: Loss = 0.5237988829612732\nStep 250: Loss = 0.5114662647247314\nStep 260: Loss = 0.5129295587539673\nStep 270: Loss = 0.5503255128860474\nStep 280: Loss = 0.5730944275856018\nStep 290: Loss = 0.38169777393341064\nStep 300: Loss = 0.7391727566719055\nStep 310: Loss = 0.6250647902488708\nStep 320: Loss = 0.4410804212093353\nStep 330: Loss = 0.41692954301834106\nStep 340: Loss = 0.5762328505516052\nStep 350: Loss = 0.5593809485435486\nStep 360: Loss = 0.5923938155174255\nStep 370: Loss = 0.8803859949111938\nStep 380: Loss = 1.1623507738113403\nStep 390: Loss = 1.0183156728744507\nStep 400: Loss = 1.198395013809204\nStep 410: Loss = 1.1213628053665161\nStep 420: Loss = 1.0321019887924194\nStep 430: Loss = 0.8890778422355652\nStep 440: Loss = 0.7504802346229553\nStep 450: Loss = 0.7492408156394958\nEpoch 3: Average Loss = 0.6637915372848511\nModel weights saved at epoch 3\nEpoch 4/10\nStep 0: Loss = 0.4421113133430481\nStep 10: Loss = 0.4327118396759033\nStep 20: Loss = 0.5629561543464661\nStep 30: Loss = 0.445863276720047\nStep 40: Loss = 0.3277798295021057\nStep 50: Loss = 0.44410374760627747\nStep 60: Loss = 0.4581170082092285\nStep 70: Loss = 0.4306909441947937\nStep 80: Loss = 0.48400038480758667\nStep 90: Loss = 0.43157699704170227\nStep 100: Loss = 0.5653662085533142\nStep 110: Loss = 0.3781256079673767\nStep 120: Loss = 0.29912030696868896\nStep 130: Loss = 0.36885184049606323\nStep 140: Loss = 0.4279499351978302\nStep 150: Loss = 0.34902140498161316\nStep 160: Loss = 0.24106015264987946\nStep 170: Loss = 0.3107932507991791\nStep 180: Loss = 0.4509781301021576\nStep 190: Loss = 0.4419585168361664\nStep 200: Loss = 0.33432650566101074\nStep 210: Loss = 0.3318243622779846\nStep 220: Loss = 0.39568883180618286\nStep 230: Loss = 0.3676367402076721\nStep 240: Loss = 0.2775192856788635\nStep 250: Loss = 0.3323376476764679\nStep 260: Loss = 0.41937151551246643\nStep 270: Loss = 0.25311529636383057\nStep 280: Loss = 0.28618866205215454\nStep 290: Loss = 0.35548636317253113\nStep 300: Loss = 0.2631036639213562\nStep 310: Loss = 0.28211885690689087\nStep 320: Loss = 0.3460126519203186\nStep 330: Loss = 0.37737151980400085\nStep 340: Loss = 0.25758594274520874\nStep 350: Loss = 0.2944074273109436\nStep 360: Loss = 0.2591915428638458\nStep 370: Loss = 0.3928382396697998\nStep 380: Loss = 0.5456025004386902\nStep 390: Loss = 0.5869469046592712\nStep 400: Loss = 0.8692108392715454\nStep 410: Loss = 0.7813201546669006\nStep 420: Loss = 0.7155628800392151\nStep 430: Loss = 0.4117625057697296\nStep 440: Loss = 0.33324384689331055\nStep 450: Loss = 0.4685087203979492\nEpoch 4: Average Loss = 0.42731359601020813\nModel weights saved at epoch 4\nEpoch 5/10\nStep 0: Loss = 0.24289155006408691\nStep 10: Loss = 0.3829178810119629\nStep 20: Loss = 0.2928518056869507\nStep 30: Loss = 0.20436997711658478\nStep 40: Loss = 0.20868803560733795\nStep 50: Loss = 0.2282244861125946\nStep 60: Loss = 0.2763820290565491\nStep 70: Loss = 0.3206373155117035\nStep 80: Loss = 0.22434653341770172\nStep 90: Loss = 0.15416234731674194\nStep 100: Loss = 0.3099954426288605\nStep 110: Loss = 0.25689297914505005\nStep 120: Loss = 0.37077900767326355\nStep 130: Loss = 0.19506821036338806\nStep 140: Loss = 0.15300951898097992\nStep 150: Loss = 0.21772989630699158\nStep 160: Loss = 0.38334524631500244\nStep 170: Loss = 0.31354984641075134\nStep 180: Loss = 0.2109484225511551\nStep 190: Loss = 0.3228748142719269\nStep 200: Loss = 0.4475719928741455\nStep 210: Loss = 0.26737603545188904\nStep 220: Loss = 0.2675492763519287\nStep 230: Loss = 0.2995586395263672\nStep 240: Loss = 0.19542288780212402\nStep 250: Loss = 0.23571960628032684\nStep 260: Loss = 0.2359251081943512\nStep 270: Loss = 0.3410886824131012\nStep 280: Loss = 0.23493830859661102\nStep 290: Loss = 0.2362576425075531\nStep 300: Loss = 0.20468206703662872\nStep 310: Loss = 0.3949481248855591\nStep 320: Loss = 0.33047303557395935\nStep 330: Loss = 0.3034829795360565\nStep 340: Loss = 0.1927475482225418\nStep 350: Loss = 0.2335270494222641\nStep 360: Loss = 0.21412689983844757\nStep 370: Loss = 0.3976145386695862\nStep 380: Loss = 0.4549270570278168\nStep 390: Loss = 0.4852796196937561\nStep 400: Loss = 0.5433462262153625\nStep 410: Loss = 0.4431722164154053\nStep 420: Loss = 0.578234851360321\nStep 430: Loss = 0.29170212149620056\nStep 440: Loss = 0.2880859375\nStep 450: Loss = 0.19131405651569366\nEpoch 5: Average Loss = 0.28910475969314575\nModel weights saved at epoch 5\nEpoch 6/10\nStep 0: Loss = 0.13720810413360596\nStep 10: Loss = 0.25281819701194763\nStep 20: Loss = 0.2177709937095642\nStep 30: Loss = 0.15058095753192902\nStep 40: Loss = 0.18098650872707367\nStep 50: Loss = 0.157894566655159\nStep 60: Loss = 0.24734607338905334\nStep 70: Loss = 0.15719622373580933\nStep 80: Loss = 0.20797431468963623\nStep 90: Loss = 0.22718320786952972\nStep 100: Loss = 0.2608930766582489\nStep 110: Loss = 0.08080343157052994\nStep 120: Loss = 0.20589880645275116\nStep 130: Loss = 0.1740126609802246\nStep 140: Loss = 0.17461328208446503\nStep 150: Loss = 0.14746727049350739\nStep 160: Loss = 0.2694453299045563\nStep 170: Loss = 0.17973066866397858\nStep 180: Loss = 0.09532122313976288\nStep 190: Loss = 0.26260194182395935\nStep 200: Loss = 0.11976439505815506\nStep 210: Loss = 0.25157874822616577\nStep 220: Loss = 0.18976424634456635\nStep 230: Loss = 0.21068568527698517\nStep 240: Loss = 0.24029527604579926\nStep 250: Loss = 0.1733430027961731\nStep 260: Loss = 0.158830463886261\nStep 270: Loss = 0.18890953063964844\nStep 280: Loss = 0.15783287584781647\nStep 290: Loss = 0.17638637125492096\nStep 300: Loss = 0.2493947297334671\nStep 310: Loss = 0.12487000226974487\nStep 320: Loss = 0.11757075041532516\nStep 330: Loss = 0.2179579734802246\nStep 340: Loss = 0.10948962718248367\nStep 350: Loss = 0.09894191473722458\nStep 360: Loss = 0.15063829720020294\nStep 370: Loss = 0.16093087196350098\nStep 380: Loss = 0.23876884579658508\nStep 390: Loss = 0.3889024257659912\nStep 400: Loss = 0.4951957166194916\nStep 410: Loss = 0.27342504262924194\nStep 420: Loss = 0.24216389656066895\nStep 430: Loss = 0.1826910674571991\nStep 440: Loss = 0.2607653737068176\nStep 450: Loss = 0.11106898635625839\nEpoch 6: Average Loss = 0.2008223831653595\nModel weights saved at epoch 6\nEpoch 7/10\nStep 0: Loss = 0.14666683971881866\nStep 10: Loss = 0.129671111702919\nStep 20: Loss = 0.19937409460544586\nStep 30: Loss = 0.13973960280418396\nStep 40: Loss = 0.08114500343799591\nStep 50: Loss = 0.1160781979560852\nStep 60: Loss = 0.1286054104566574\nStep 70: Loss = 0.12496570497751236\nStep 80: Loss = 0.11690985411405563\nStep 90: Loss = 0.17714688181877136\nStep 100: Loss = 0.10761107504367828\nStep 110: Loss = 0.15396030247211456\nStep 120: Loss = 0.15839296579360962\nStep 130: Loss = 0.14447148144245148\nStep 140: Loss = 0.10239683836698532\nStep 150: Loss = 0.1683657318353653\nStep 160: Loss = 0.12285321950912476\nStep 170: Loss = 0.14442317187786102\nStep 180: Loss = 0.0500582791864872\nStep 190: Loss = 0.11592350900173187\nStep 200: Loss = 0.1625763326883316\nStep 210: Loss = 0.11030976474285126\nStep 220: Loss = 0.18586263060569763\nStep 230: Loss = 0.13906651735305786\nStep 240: Loss = 0.06551199406385422\nStep 250: Loss = 0.06319407373666763\nStep 260: Loss = 0.05992445722222328\nStep 270: Loss = 0.1390218734741211\nStep 280: Loss = 0.04671645537018776\nStep 290: Loss = 0.09573426097631454\nStep 300: Loss = 0.06660830229520798\nStep 310: Loss = 0.14664877951145172\nStep 320: Loss = 0.10596903413534164\nStep 330: Loss = 0.15349741280078888\nStep 340: Loss = 0.11989382654428482\nStep 350: Loss = 0.05568617209792137\nStep 360: Loss = 0.15202222764492035\nStep 370: Loss = 0.18001501262187958\nStep 380: Loss = 0.1991870403289795\nStep 390: Loss = 0.22116631269454956\nStep 400: Loss = 0.22918158769607544\nStep 410: Loss = 0.2211693823337555\nStep 420: Loss = 0.22219452261924744\nStep 430: Loss = 0.13299526274204254\nStep 440: Loss = 0.10444231331348419\nStep 450: Loss = 0.05943797528743744\nEpoch 7: Average Loss = 0.1401764452457428\nModel weights saved at epoch 7\nEpoch 8/10\nStep 0: Loss = 0.12506839632987976\nStep 10: Loss = 0.1064198687672615\nStep 20: Loss = 0.1328078955411911\nStep 30: Loss = 0.10578729212284088\nStep 40: Loss = 0.06017309054732323\nStep 50: Loss = 0.1860506683588028\nStep 60: Loss = 0.07655342668294907\nStep 70: Loss = 0.09728401899337769\nStep 80: Loss = 0.07070635259151459\nStep 90: Loss = 0.05029458552598953\nStep 100: Loss = 0.09259265661239624\nStep 110: Loss = 0.10094394534826279\nStep 120: Loss = 0.08004333823919296\nStep 130: Loss = 0.055254869163036346\nStep 140: Loss = 0.03222813084721565\nStep 150: Loss = 0.06928574293851852\nStep 160: Loss = 0.07841846346855164\nStep 170: Loss = 0.044560275971889496\nStep 180: Loss = 0.059526968747377396\nStep 190: Loss = 0.06468070298433304\nStep 200: Loss = 0.061794716864824295\nStep 210: Loss = 0.09172586351633072\nStep 220: Loss = 0.09285365790128708\nStep 230: Loss = 0.0932551771402359\nStep 240: Loss = 0.13512857258319855\nStep 250: Loss = 0.030709484592080116\nStep 260: Loss = 0.09490261226892471\nStep 270: Loss = 0.10681340843439102\nStep 280: Loss = 0.06603432446718216\nStep 290: Loss = 0.10844457149505615\nStep 300: Loss = 0.12448225915431976\nStep 310: Loss = 0.08034853637218475\nStep 320: Loss = 0.11388327181339264\nStep 330: Loss = 0.08095628023147583\nStep 340: Loss = 0.06617740541696548\nStep 350: Loss = 0.07748345285654068\nStep 360: Loss = 0.07484892755746841\nStep 370: Loss = 0.06287287920713425\nStep 380: Loss = 0.159582257270813\nStep 390: Loss = 0.17050772905349731\nStep 400: Loss = 0.16927771270275116\nStep 410: Loss = 0.22940930724143982\nStep 420: Loss = 0.11123469471931458\nStep 430: Loss = 0.05731881037354469\nStep 440: Loss = 0.059750787913799286\nStep 450: Loss = 0.08262443542480469\nEpoch 8: Average Loss = 0.09721051156520844\nModel weights saved at epoch 8\nEpoch 9/10\nStep 0: Loss = 0.04410142824053764\nStep 10: Loss = 0.07937491685152054\nStep 20: Loss = 0.10184377431869507\nStep 30: Loss = 0.08057481795549393\nStep 40: Loss = 0.04248877614736557\nStep 50: Loss = 0.061348650604486465\nStep 60: Loss = 0.032530371099710464\nStep 70: Loss = 0.04631023481488228\nStep 80: Loss = 0.055927567183971405\nStep 90: Loss = 0.0928042083978653\nStep 100: Loss = 0.08762701600790024\nStep 110: Loss = 0.04420822113752365\nStep 120: Loss = 0.0631474107503891\nStep 130: Loss = 0.0270084235817194\nStep 140: Loss = 0.030579887330532074\nStep 150: Loss = 0.07355552911758423\nStep 160: Loss = 0.06636521220207214\nStep 170: Loss = 0.05924217402935028\nStep 180: Loss = 0.033887024968862534\nStep 190: Loss = 0.09147606045007706\nStep 200: Loss = 0.03772475942969322\nStep 210: Loss = 0.022423747926950455\nStep 220: Loss = 0.03744973987340927\nStep 230: Loss = 0.07922957837581635\nStep 240: Loss = 0.05892156437039375\nStep 250: Loss = 0.046324048191308975\nStep 260: Loss = 0.07069879025220871\nStep 270: Loss = 0.0806899145245552\nStep 280: Loss = 0.03166966885328293\nStep 290: Loss = 0.08821161091327667\nStep 300: Loss = 0.05847274884581566\nStep 310: Loss = 0.04533308371901512\nStep 320: Loss = 0.07249277830123901\nStep 330: Loss = 0.0678751990199089\nStep 340: Loss = 0.07116402685642242\nStep 350: Loss = 0.07409439980983734\nStep 360: Loss = 0.0648578330874443\nStep 370: Loss = 0.09610670804977417\nStep 380: Loss = 0.08494371175765991\nStep 390: Loss = 0.13577327132225037\nStep 400: Loss = 0.11706039309501648\nStep 410: Loss = 0.140115424990654\nStep 420: Loss = 0.08067387342453003\nStep 430: Loss = 0.05626364424824715\nStep 440: Loss = 0.026605965569615364\nStep 450: Loss = 0.025594359263777733\nEpoch 9: Average Loss = 0.06591922044754028\nModel weights saved at epoch 9\nEpoch 10/10\nStep 0: Loss = 0.05737045034766197\nStep 10: Loss = 0.03842238336801529\nStep 20: Loss = 0.03643210232257843\nStep 30: Loss = 0.041508108377456665\nStep 40: Loss = 0.05416037514805794\nStep 50: Loss = 0.039589278399944305\nStep 60: Loss = 0.042250774800777435\nStep 70: Loss = 0.05195508897304535\nStep 80: Loss = 0.031050661578774452\nStep 90: Loss = 0.04277141019701958\nStep 100: Loss = 0.03275347128510475\nStep 110: Loss = 0.05749683827161789\nStep 120: Loss = 0.03880751505494118\nStep 130: Loss = 0.025419555604457855\nStep 140: Loss = 0.08975385874509811\nStep 150: Loss = 0.04120869189500809\nStep 160: Loss = 0.07284296303987503\nStep 170: Loss = 0.024396972730755806\nStep 180: Loss = 0.04255755618214607\nStep 190: Loss = 0.02946854569017887\nStep 200: Loss = 0.013859621249139309\nStep 210: Loss = 0.0353839173913002\nStep 220: Loss = 0.04148336127400398\nStep 230: Loss = 0.028883282095193863\nStep 240: Loss = 0.0268187765032053\nStep 250: Loss = 0.027936197817325592\nStep 260: Loss = 0.03667297586798668\nStep 270: Loss = 0.04750790819525719\nStep 280: Loss = 0.05441669747233391\nStep 290: Loss = 0.0354740247130394\nStep 300: Loss = 0.02656581625342369\nStep 310: Loss = 0.021476151421666145\nStep 320: Loss = 0.04529550299048424\nStep 330: Loss = 0.0581357441842556\nStep 340: Loss = 0.05868184566497803\nStep 350: Loss = 0.06434997916221619\nStep 360: Loss = 0.01849522814154625\nStep 370: Loss = 0.024739626795053482\nStep 380: Loss = 0.06481406837701797\nStep 390: Loss = 0.07604056596755981\nStep 400: Loss = 0.06838784366846085\nStep 410: Loss = 0.07114923745393753\nStep 420: Loss = 0.04523121565580368\nStep 430: Loss = 0.036679137498140335\nStep 440: Loss = 0.023465748876333237\nStep 450: Loss = 0.021024275571107864\nEpoch 10: Average Loss = 0.042230360209941864\nModel weights saved at epoch 10\n","output_type":"stream"}],"execution_count":165},{"cell_type":"code","source":"\nfrom tensorflow.keras.preprocessing.text import Tokenizer\n\n# Urdu tokenizer using Keras Tokenizer (not BERT)\nurdu_tokenizer = Tokenizer(num_words=8000, oov_token=\"<OOV>\")\nurdu_tokenizer.fit_on_texts(data['urdu'])\ndata['urdu_tokens'] = urdu_tokenizer.texts_to_sequences(data['urdu'])\n","metadata":{"execution":{"iopub.status.busy":"2025-04-13T14:45:15.780046Z","iopub.execute_input":"2025-04-13T14:45:15.780346Z","iopub.status.idle":"2025-04-13T14:45:16.256744Z","shell.execute_reply.started":"2025-04-13T14:45:15.780314Z","shell.execute_reply":"2025-04-13T14:45:16.255880Z"},"trusted":true},"outputs":[],"execution_count":167},{"cell_type":"code","source":"from transformers import MBartForConditionalGeneration, MBart50TokenizerFast\n\n# Load the tokenizer and model\nmbart_tokenizer = MBart50TokenizerFast.from_pretrained(\"facebook/mbart-large-50-many-to-many-mmt\")\nmbart_model = MBartForConditionalGeneration.from_pretrained(\"facebook/mbart-large-50-many-to-many-mmt\")\n\n# Set the source language (e.g., English)\nmbart_tokenizer.src_lang = \"en_XX\"\n\n# Encode input and translate to Urdu\ninputs = mbart_tokenizer(\"How are you?\", return_tensors=\"pt\", truncation=True, padding=True)\ntranslated_ids = mbart_model.generate(**inputs, forced_bos_token_id=mbart_tokenizer.lang_code_to_id[\"ur_PK\"])\n\n# Decode and print\nprint(\"Predicted Urdu:\", mbart_tokenizer.decode(translated_ids[0], skip_special_tokens=True))\n","metadata":{"execution":{"iopub.status.busy":"2025-04-13T14:46:20.718225Z","iopub.execute_input":"2025-04-13T14:46:20.718525Z","iopub.status.idle":"2025-04-13T14:46:52.857155Z","shell.execute_reply.started":"2025-04-13T14:46:20.718505Z","shell.execute_reply":"2025-04-13T14:46:52.856365Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/529 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bdc2ff61aa9a414cb687ce9698888753"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8b17138e5ebd4ff2bc997c6cf49f4c9a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/649 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e56ec38711e14b2bab7c91b314421278"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.43k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b690c8baf1944471989d4617ccfd2d9b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/2.44G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ed01068f176547c3821c2c9c68510774"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/261 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c90475945cc0439bbe7beca93f6ac801"}},"metadata":{}},{"name":"stderr","text":"Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n","output_type":"stream"},{"name":"stdout","text":"Predicted Urdu: تم کیسے ہو؟\n","output_type":"stream"}],"execution_count":169}]}